/*
 * Intel IOMMU acceleration with nested translation
 *
 * Copyright (C) 2026 Intel Corporation.
 *
 * Authors: Zhenzhong Duan <zhenzhong.duan@intel.com>
 *
 * SPDX-License-Identifier: GPL-2.0-or-later
 */

#include "qemu/osdep.h"
#include "system/iommufd.h"
#include "intel_iommu_internal.h"
#include "intel_iommu_accel.h"
#include "hw/core/iommu.h"
#include "hw/pci/pci_bus.h"
#include "trace.h"

bool vtd_check_hiod_accel(IntelIOMMUState *s, VTDHostIOMMUDevice *vtd_hiod,
                          Error **errp)
{
    HostIOMMUDevice *hiod = vtd_hiod->hiod;
    struct HostIOMMUDeviceCaps *caps = &hiod->caps;
    struct iommu_hw_info_vtd *vtd = &caps->vendor_caps.vtd;
    PCIBus *bus = vtd_hiod->bus;
    PCIDevice *pdev = bus->devices[vtd_hiod->devfn];

    if (!object_dynamic_cast(OBJECT(hiod), TYPE_HOST_IOMMU_DEVICE_IOMMUFD)) {
        error_setg(errp, "Need IOMMUFD backend when x-flts=on");
        return false;
    }

    if (caps->type != IOMMU_HW_INFO_TYPE_INTEL_VTD) {
        error_setg(errp, "Incompatible host platform IOMMU type %d",
                   caps->type);
        return false;
    }

    if (s->fs1gp && !(vtd->cap_reg & VTD_CAP_FS1GP)) {
        error_setg(errp,
                   "First stage 1GB large page is unsupported by host IOMMU");
        return false;
    }

    if (pci_device_get_iommu_bus_devfn(pdev, &bus, NULL, NULL)) {
        error_setg(errp, "Host device downstream to a PCI bridge is "
                   "unsupported when x-flts=on");
        return false;
    }

    return true;
}

VTDHostIOMMUDevice *vtd_find_hiod_iommufd(VTDAddressSpace *as)
{
    IntelIOMMUState *s = as->iommu_state;
    struct vtd_as_key key = {
        .bus = as->bus,
        .devfn = as->devfn,
    };
    VTDHostIOMMUDevice *vtd_hiod = g_hash_table_lookup(s->vtd_host_iommu_dev,
                                                       &key);

    if (vtd_hiod && vtd_hiod->hiod &&
        object_dynamic_cast(OBJECT(vtd_hiod->hiod),
                            TYPE_HOST_IOMMU_DEVICE_IOMMUFD)) {
        return vtd_hiod;
    }
    return NULL;
}

static bool vtd_create_fs_hwpt(VTDHostIOMMUDevice *vtd_hiod,
                               VTDPASIDEntry *pe, uint32_t *fs_hwpt_id,
                               Error **errp)
{
    HostIOMMUDeviceIOMMUFD *idev = HOST_IOMMU_DEVICE_IOMMUFD(vtd_hiod->hiod);
    struct iommu_hwpt_vtd_s1 vtd = {};
    uint32_t flags = vtd_hiod->iommu_state->pasid ? IOMMU_HWPT_ALLOC_PASID : 0;

    vtd.flags = (VTD_SM_PASID_ENTRY_SRE(pe) ? IOMMU_VTD_S1_SRE : 0) |
                (VTD_SM_PASID_ENTRY_WPE(pe) ? IOMMU_VTD_S1_WPE : 0) |
                (VTD_SM_PASID_ENTRY_EAFE(pe) ? IOMMU_VTD_S1_EAFE : 0);
    vtd.addr_width = vtd_pe_get_fs_aw(pe);
    vtd.pgtbl_addr = (uint64_t)vtd_pe_get_fspt_base(pe);

    return iommufd_backend_alloc_hwpt(idev->iommufd, idev->devid, idev->hwpt_id,
                                      flags, IOMMU_HWPT_DATA_VTD_S1,
                                      sizeof(vtd), &vtd, fs_hwpt_id, errp);
}

static void vtd_destroy_old_fs_hwpt(VTDHostIOMMUDevice *vtd_hiod,
                                    VTDAddressSpace *vtd_as)
{
    HostIOMMUDeviceIOMMUFD *idev = HOST_IOMMU_DEVICE_IOMMUFD(vtd_hiod->hiod);

    if (!vtd_as->fs_hwpt_id) {
        return;
    }
    iommufd_backend_free_id(idev->iommufd, vtd_as->fs_hwpt_id);
    vtd_as->fs_hwpt_id = 0;
}

static bool vtd_device_attach_iommufd(VTDHostIOMMUDevice *vtd_hiod,
                                      VTDAddressSpace *vtd_as, Error **errp)
{
    HostIOMMUDeviceIOMMUFD *idev = HOST_IOMMU_DEVICE_IOMMUFD(vtd_hiod->hiod);
    VTDPASIDEntry *pe = &vtd_as->pasid_cache_entry.pasid_entry;
    uint32_t hwpt_id = idev->hwpt_id;
    bool ret;

    /*
     * We can get here only if flts=on, the supported PGTT is FST or PT.
     * Catch invalid PGTT when processing invalidation request to avoid
     * attaching to wrong hwpt.
     */
    if (!vtd_pe_pgtt_is_fst(pe) && !vtd_pe_pgtt_is_pt(pe)) {
        error_setg(errp, "Invalid PGTT type %d",
                   (uint8_t)VTD_SM_PASID_ENTRY_PGTT(pe));
        return false;
    }

    if (vtd_pe_pgtt_is_fst(pe)) {
        if (!vtd_create_fs_hwpt(vtd_hiod, pe, &hwpt_id, errp)) {
            return false;
        }
    }

    ret = host_iommu_device_iommufd_attach_hwpt(idev, IOMMU_NO_PASID, hwpt_id,
                                                errp);
    trace_vtd_device_attach_hwpt(idev->devid, vtd_as->pasid, hwpt_id, ret);
    if (ret) {
        /* Destroy old fs_hwpt if it's a replacement */
        vtd_destroy_old_fs_hwpt(vtd_hiod, vtd_as);
        if (vtd_pe_pgtt_is_fst(pe)) {
            vtd_as->fs_hwpt_id = hwpt_id;
        }
    } else if (vtd_pe_pgtt_is_fst(pe)) {
        iommufd_backend_free_id(idev->iommufd, hwpt_id);
    }

    return ret;
}

static bool vtd_device_detach_iommufd(VTDHostIOMMUDevice *vtd_hiod,
                                      VTDAddressSpace *vtd_as, Error **errp)
{
    HostIOMMUDeviceIOMMUFD *idev = HOST_IOMMU_DEVICE_IOMMUFD(vtd_hiod->hiod);
    IntelIOMMUState *s = vtd_as->iommu_state;
    uint32_t pasid = vtd_as->pasid;
    bool ret;

    if (s->dmar_enabled && s->root_scalable) {
        ret = host_iommu_device_iommufd_detach_hwpt(idev, IOMMU_NO_PASID, errp);
        trace_vtd_device_detach_hwpt(idev->devid, pasid, ret);
    } else {
        /*
         * If DMAR remapping is disabled or guest switches to legacy mode,
         * we fallback to the default HWPT which contains shadow page table.
         * So guest DMA could still work.
         */
        ret = host_iommu_device_iommufd_attach_hwpt(idev, IOMMU_NO_PASID,
                                                    idev->hwpt_id, errp);
        trace_vtd_device_reattach_def_hwpt(idev->devid, pasid, idev->hwpt_id,
                                           ret);
    }

    if (ret) {
        vtd_destroy_old_fs_hwpt(vtd_hiod, vtd_as);
    }

    return ret;
}

bool vtd_propagate_guest_pasid(VTDAddressSpace *vtd_as, Error **errp)
{
    VTDPASIDCacheEntry *pc_entry = &vtd_as->pasid_cache_entry;
    VTDHostIOMMUDevice *vtd_hiod = vtd_find_hiod_iommufd(vtd_as);

    /* Ignore emulated device or legacy VFIO backed device */
    if (!vtd_as->iommu_state->fsts || !vtd_hiod) {
        return true;
    }

    if (pc_entry->valid) {
        return vtd_device_attach_iommufd(vtd_hiod, vtd_as, errp);
    }

    return vtd_device_detach_iommufd(vtd_hiod, vtd_as, errp);
}

/*
 * This function is a loop function for the s->vtd_address_spaces
 * list with VTDPIOTLBInvInfo as execution filter. It propagates
 * the piotlb invalidation to host.
 */
static void vtd_flush_host_piotlb_locked(gpointer key, gpointer value,
                                         gpointer user_data)
{
    VTDPIOTLBInvInfo *piotlb_info = user_data;
    VTDAddressSpace *vtd_as = value;
    VTDHostIOMMUDevice *vtd_hiod = vtd_find_hiod_iommufd(vtd_as);
    VTDPASIDCacheEntry *pc_entry = &vtd_as->pasid_cache_entry;
    uint16_t did;

    if (!vtd_hiod) {
        return;
    }

    assert(vtd_as->pasid == PCI_NO_PASID);

    /* Nothing to do if there is no first stage HWPT attached */
    if (!pc_entry->valid ||
        !vtd_pe_pgtt_is_fst(&pc_entry->pasid_entry)) {
        return;
    }

    did = VTD_SM_PASID_ENTRY_DID(&pc_entry->pasid_entry);

    if (piotlb_info->domain_id == did && piotlb_info->pasid == PASID_0) {
        HostIOMMUDeviceIOMMUFD *idev =
            HOST_IOMMU_DEVICE_IOMMUFD(vtd_hiod->hiod);
        uint32_t entry_num = 1; /* Only implement one request for simplicity */
        Error *local_err = NULL;
        struct iommu_hwpt_vtd_s1_invalidate *cache = piotlb_info->inv_data;

        if (!iommufd_backend_invalidate_cache(idev->iommufd, vtd_as->fs_hwpt_id,
                                              IOMMU_HWPT_INVALIDATE_DATA_VTD_S1,
                                              sizeof(*cache), &entry_num, cache,
                                              &local_err)) {
            /* Something wrong in kernel, but trying to continue */
            error_report_err(local_err);
        }
    }
}

void vtd_flush_host_piotlb_all_locked(IntelIOMMUState *s, uint16_t domain_id,
                                      uint32_t pasid, hwaddr addr,
                                      uint64_t npages, bool ih)
{
    struct iommu_hwpt_vtd_s1_invalidate cache_info = { 0 };
    VTDPIOTLBInvInfo piotlb_info;

    cache_info.addr = addr;
    cache_info.npages = npages;
    cache_info.flags = ih ? IOMMU_VTD_INV_FLAGS_LEAF : 0;

    piotlb_info.domain_id = domain_id;
    piotlb_info.pasid = pasid;
    piotlb_info.inv_data = &cache_info;

    /*
     * Go through each vtd_as instance in s->vtd_address_spaces, find out
     * affected host devices which need host piotlb invalidation. Piotlb
     * invalidation should check pasid cache per architecture point of view.
     */
    g_hash_table_foreach(s->vtd_address_spaces,
                         vtd_flush_host_piotlb_locked, &piotlb_info);
}

static void vtd_find_add_pc(VTDHostIOMMUDevice *vtd_hiod, uint32_t pasid,
                            VTDPASIDEntry *pe)
{
    VTDACCELPASIDCacheEntry *vtd_pce;

    QLIST_FOREACH(vtd_pce, &vtd_hiod->pasid_cache_list, next) {
        if (vtd_pce->pasid == pasid) {
            if (vtd_pasid_entry_compare(pe, &vtd_pce->pe)) {
                vtd_pce->pe = *pe;
            }
            return;
        }
    }

    vtd_pce = g_malloc0(sizeof(VTDACCELPASIDCacheEntry));
    vtd_pce->vtd_hiod = vtd_hiod;
    vtd_pce->pasid = pasid;
    vtd_pce->pe = *pe;
    QLIST_INSERT_HEAD(&vtd_hiod->pasid_cache_list, vtd_pce, next);
}

/*
 * This function walks over PASID range within [start, end) in a single
 * PASID table for entries matching @info type/did, then create
 * VTDACCELPASIDCacheEntry if not exist yet.
 */
static void vtd_sm_pasid_table_walk_one(VTDHostIOMMUDevice *vtd_hiod,
                                        dma_addr_t pt_base,
                                        int start,
                                        int end,
                                        VTDPASIDCacheInfo *info)
{
    IntelIOMMUState *s = vtd_hiod->iommu_state;
    VTDPASIDEntry pe;
    int pasid;

    for (pasid = start; pasid < end; pasid++) {
        if (vtd_get_pe_in_pasid_leaf_table(s, pasid, pt_base, &pe) ||
            !vtd_pe_present(&pe)) {
            continue;
        }

        if ((info->type == VTD_INV_DESC_PASIDC_G_DSI ||
             info->type == VTD_INV_DESC_PASIDC_G_PASID_SI) &&
            (info->did != VTD_SM_PASID_ENTRY_DID(&pe))) {
            /*
             * VTD_PASID_CACHE_DOMSI and VTD_PASID_CACHE_PASIDSI
             * requires domain id check. If domain id check fail,
             * go to next pasid.
             */
            continue;
        }

        vtd_find_add_pc(vtd_hiod, pasid, &pe);
    }
}

/*
 * In VT-d scalable mode translation, PASID dir + PASID table is used.
 * This function aims at looping over a range of PASIDs in the given
 * two level table to identify the pasid config in guest.
 */
static void vtd_sm_pasid_table_walk(VTDHostIOMMUDevice *vtd_hiod,
                                    dma_addr_t pdt_base,
                                    int start, int end,
                                    VTDPASIDCacheInfo *info)
{
    VTDPASIDDirEntry pdire;
    int pasid = start;
    int pasid_next;
    dma_addr_t pt_base;

    while (pasid < end) {
        pasid_next = (pasid + VTD_PASID_TABLE_ENTRY_NUM) &
                     ~(VTD_PASID_TABLE_ENTRY_NUM - 1);
        pasid_next = pasid_next < end ? pasid_next : end;

        if (!vtd_get_pdire_from_pdir_table(pdt_base, pasid, &pdire)
            && vtd_pdire_present(&pdire)) {
            pt_base = pdire.val & VTD_PASID_TABLE_BASE_ADDR_MASK;
            vtd_sm_pasid_table_walk_one(vtd_hiod, pt_base, pasid, pasid_next,
                                        info);
        }
        pasid = pasid_next;
    }
}

static void vtd_replay_pasid_bind_for_dev(VTDHostIOMMUDevice *vtd_hiod,
                                          int start, int end,
                                          VTDPASIDCacheInfo *pc_info)
{
    IntelIOMMUState *s = vtd_hiod->iommu_state;
    VTDContextEntry ce;
    int dev_max_pasid = 1 << vtd_hiod->hiod->caps.max_pasid_log2;

    if (!vtd_dev_to_context_entry(s, pci_bus_num(vtd_hiod->bus),
                                  vtd_hiod->devfn, &ce)) {
        VTDPASIDCacheInfo walk_info = *pc_info;
        uint32_t ce_max_pasid = vtd_sm_ce_get_pdt_entry_num(&ce) *
                                VTD_PASID_TABLE_ENTRY_NUM;

        end = MIN(end, MIN(dev_max_pasid, ce_max_pasid));

        vtd_sm_pasid_table_walk(vtd_hiod, VTD_CE_GET_PASID_DIR_TABLE(&ce),
                                start, end, &walk_info);
    }
}

/*
 * This function replays the guest pasid bindings by walking the two level
 * guest PASID table. For each valid pasid entry, it creates an entry
 * VTDACCELPASIDCacheEntry dynamically if not exist yet. This entry holds
 * info specific to a pasid
 */
void vtd_pasid_cache_sync_accel(IntelIOMMUState *s, VTDPASIDCacheInfo *pc_info)
{
    int start = PASID_0, end = 1 << s->pasid;
    VTDHostIOMMUDevice *vtd_hiod;
    GHashTableIter as_it;

    if (!s->fsts) {
        return;
    }

    /*
     * VTDPASIDCacheInfo honors PCI pasid but VTDACCELPASIDCacheEntry honors
     * iommu pasid
     */
    if (pc_info->pasid == PCI_NO_PASID) {
        pc_info->pasid = PASID_0;
    }

    switch (pc_info->type) {
    case VTD_INV_DESC_PASIDC_G_PASID_SI:
        start = pc_info->pasid;
        end = pc_info->pasid + 1;
        /* fall through */
    case VTD_INV_DESC_PASIDC_G_DSI:
        /*
         * loop all assigned devices, do domain id check in
         * vtd_sm_pasid_table_walk_one() after get pasid entry.
         */
        break;
    case VTD_INV_DESC_PASIDC_G_GLOBAL:
        /* loop all assigned devices */
        break;
    default:
        g_assert_not_reached();
    }

    /*
     * In this replay, one only needs to care about the devices which are
     * backed by host IOMMU. Those devices have a corresponding vtd_hiod
     * in s->vtd_host_iommu_dev. For devices not backed by host IOMMU, it
     * is not necessary to replay the bindings since their cache should be
     * created in the future DMA address translation.
     *
     * VTD translation callback never accesses vtd_hiod and its corresponding
     * cached pasid entry, so no iommu lock needed here.
     */
    g_hash_table_iter_init(&as_it, s->vtd_host_iommu_dev);
    while (g_hash_table_iter_next(&as_it, NULL, (void **)&vtd_hiod)) {
        if (!object_dynamic_cast(OBJECT(vtd_hiod->hiod),
                                 TYPE_HOST_IOMMU_DEVICE_IOMMUFD)) {
            continue;
        }
        vtd_replay_pasid_bind_for_dev(vtd_hiod, start, end, pc_info);
    }
}

static uint64_t vtd_get_host_iommu_quirks(uint32_t type,
                                          void *caps, uint32_t size)
{
    struct iommu_hw_info_vtd *vtd = caps;
    uint64_t quirks = 0;

    if (type == IOMMU_HW_INFO_TYPE_INTEL_VTD &&
        sizeof(struct iommu_hw_info_vtd) <= size &&
        vtd->flags & IOMMU_HW_INFO_VTD_ERRATA_772415_SPR17) {
        quirks |= HOST_IOMMU_QUIRK_NESTING_PARENT_BYPASS_RO;
    }

    return quirks;
}

void vtd_iommu_ops_update_accel(PCIIOMMUOps *ops)
{
    ops->get_host_iommu_quirks = vtd_get_host_iommu_quirks;
}
