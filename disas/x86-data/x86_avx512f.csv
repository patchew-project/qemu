"Instruction","Opcode","Valid 64-bit","Valid 32-bit","Valid 16-bit","Feature Flags","Operand 1","Operand 2","Operand 3","Operand 4","Tuple Type","Description"
"KANDNW k1, k2, k3","VEX.L1.0F.W0 42 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bfarmitwise AND NOT 16 bits masks k2 and k3 and place result in k1."
"KANDW k1, k2, k3","VEX.L1.0F.W0 41 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND 16 bits masks k2 and k3 and place result in k1."
"KMOVW k1, k2/m16","VEX.L0.0F.W0 90 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","","Move 16 bits mask from k2/m16 and store the result in k1."
"KMOVW m16, k1","VEX.L0.0F.W0 91 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w, ModRM:[7:6] must not be 11b)","ModRM:reg (r)","","","","Move 16 bits mask from k1 and store the result in m16."
"KMOVW k1, rw","VEX.L0.0F.W0 92 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 16 bits mask from r to k1."
"KMOVW rw, k1","VEX.L0.0F.W0 93 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 16 bits mask from k1 to r."
"KNOTW k1, k2","VEX.L0.0F.W0 44 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise NOT of 16 bits mask k2."
"KORTESTW k1, k2","VEX.L0.0F.W0 98 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise OR 16 bits masks k1 and k2 and update ZF and CF accordingly."
"KORW k1, k2, k3","VEX.L1.0F.W0 45 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise OR 16 bits masks k2 and k3 and place result in k1."
"KSHIFTLW k1, k2, ib","VEX.L0.66.0F3A.W1 32 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","ib","","","Shift left 16 bits in k2 by immediate and write result in k1."
"KSHIFTRW k1, k2, ib","VEX.L0.66.0F3A.W1 30 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","ib","","","Shift right 16 bits in k2 by immediate and write result in k1."
"KUNPCKBW k1, k2, k3","VEX.L1.66.0F.W0 4B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Unpack and interleave 8 bits masks in k2 and k3 and write word result in k1."
"KXNORW k1, k2, k3","VEX.L1.0F.W0 46 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XNOR 16 bits masks k2 and k3 and place result in k1."
"KXORW k1, k2, k3","VEX.L1.0F.W0 47 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XOR 16 bits masks k2 and k3 and place result in k1."
"VADDPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed double-precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1."
"VADDPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed double-precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1."
"VADDPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F.W1 58 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed double-precision floating-point values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1."
"VADDPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.0F.W0 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed single-precision floating-point values from xmm3/m128/m32bcst to xmm2 and store result in xmm1 with writemask k1."
"VADDPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.0F.W0 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed single-precision floating-point values from ymm3/m256/m32bcst to ymm2 and store result in ymm1 with writemask k1."
"VADDPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst {er}","EVEX.512.0F.W0 58 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed single-precision floating-point values from zmm3/m512/m32bcst to zmm2 and store result in zmm1 with writemask k1."
"VADDSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.F2.0F.W1 58 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Add the low double-precision floating-point value from xmm3/m64 to xmm2 and store the result in xmm1 with writemask k1."
"VADDSS xmm1{k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.F3.0F.W0 58 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Add the low single-precision floating-point value from xmm3/m32 to xmm2 and store the result in xmm1with writemask k1."
"VALIGND xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst, ib","EVEX.128.66.0F3A.W0 03 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shift right and merge vectors xmm2 and xmm3/m128/m32bcst with double-word granularity using ib as number of elements to shift, and store the final result in xmm1, under writemask."
"VALIGND ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 03 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shift right and merge vectors ymm2 and ymm3/m256/m32bcst with double-word granularity using ib as number of elements to shift, and store the final result in ymm1, under writemask."
"VALIGND zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst, ib","EVEX.512.66.0F3A.W0 03 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shift right and merge vectors zmm2 and zmm3/m512/m32bcst with double-word granularity using ib as number of elements to shift, and store the final result in zmm1, under writemask."
"VALIGNQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst, ib","EVEX.128.66.0F3A.W1 03 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shift right and merge vectors xmm2 and xmm3/m128/m64bcst with quad-word granularity using ib as number of elements to shift, and store the final result in xmm1, under writemask."
"VALIGNQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 03 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shift right and merge vectors ymm2 and ymm3/m256/m64bcst with quad-word granularity using ib as number of elements to shift, and store the final result in ymm1, under writemask."
"VALIGNQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst, ib","EVEX.512.66.0F3A.W1 03 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shift right and merge vectors zmm2 and zmm3/m512/m64bcst with quad-word granularity using ib as number of elements to shift, and store the final result in zmm1, under writemask."
"VBLENDMPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 65 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend double-precision vector xmm2 and double-precision vector xmm3/m128/m64bcst and store the result in xmm1, under control mask."
"VBLENDMPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 65 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend double-precision vector ymm2 and double-precision vector ymm3/m256/m64bcst and store the result in ymm1, under control mask."
"VBLENDMPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 65 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend double-precision vector zmm2 and double-precision vector zmm3/m512/m64bcst and store the result in zmm1, under control mask."
"VBLENDMPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 65 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend single-precision vector xmm2 and single-precision vector xmm3/m128/m32bcst and store the result in xmm1, under control mask."
"VBLENDMPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 65 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend single-precision vector ymm2 and single-precision vector ymm3/m256/m32bcst and store the result in ymm1, under control mask."
"VBLENDMPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 65 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend single-precision vector zmm2 and single-precision vector zmm3/m512/m32bcst using k1 as select control and store the result in zmm1."
"VBROADCASTF32X4 ymm1 {k1}{z}, m128","EVEX.256.66.0F38.W0 1A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple4","Broadcast 128 bits of 4 single-precision floating-point data in mem to locations in ymm1 using writemask k1."
"VBROADCASTF32X4 zmm1 {k1}{z}, m128","EVEX.512.66.0F38.W0 1A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple4","Broadcast 128 bits of 4 single-precision floating-point data in mem to locations in zmm1 using writemask k1."
"VBROADCASTF64X4 zmm1 {k1}{z}, m256","EVEX.512.66.0F38.W1 1B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple4","Broadcast 256 bits of 4 double precision floating-point data in mem to locations in zmm1 using writemask k1."
"VBROADCASTI32X4 ymm1 {k1}{z}, m128","EVEX.256.66.0F38.W0 5A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple4","Broadcast 128 bits of 4 doubleword integer data in mem to locations in ymm1 using writemask k1."
"VBROADCASTI32X4 zmm1 {k1}{z}, m128","EVEX.512.66.0F38.W0 5A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple4","Broadcast 128 bits of 4 doubleword integer data in mem to locations in zmm1 using writemask k1."
"VBROADCASTI64X4 zmm1 {k1}{z}, m256","EVEX.512.66.0F38.W1 5B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple4","Broadcast 256 bits of 4 quadword integer data in mem to locations in zmm1 using writemask k1."
"VBROADCASTSD ymm1 {k1}{z}, xmm2/m64","EVEX.256.66.0F38.W1 19 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast low double-precision floating-point element in xmm2/m64 to four locations in ymm1 using writemask k1."
"VBROADCASTSD zmm1 {k1}{z}, xmm2/m64","EVEX.512.66.0F38.W1 19 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast low double-precision floating-point element in xmm2/m64 to eight locations in zmm1 using writemask k1."
"VBROADCASTSS xmm1 {k1}{z}, xmm2/m32","EVEX.128.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast low single-precision floating-point element in xmm2/m32 to all locations in xmm1 using writemask k1."
"VBROADCASTSS ymm1 {k1}{z}, xmm2/m32","EVEX.256.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast low single-precision floating-point element in xmm2/m32 to all locations in ymm1 using writemask k1."
"VBROADCASTSS zmm1 {k1}{z}, xmm2/m32","EVEX.512.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast low single-precision floating-point element in xmm2/m32 to all locations in zmm1 using writemask k1."
"VCMPPD k1 {k2}, xmm2, xmm3/m128/m64bcst, ib","EVEX.128.66.0F.W1 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed double-precision floating-point values in xmm3/m128/m64bcst and xmm2 using bits 4:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPD k1 {k2}, ymm2, ymm3/m256/m64bcst, ib","EVEX.256.66.0F.W1 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed double-precision floating-point values in ymm3/m256/m64bcst and ymm2 using bits 4:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPD k1 {k2}, zmm2, zmm3/m512/m64bcst{sae}, ib","EVEX.512.66.0F.W1 C2 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed double-precision floating-point values in zmm3/m512/m64bcst and zmm2 using bits 4:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPS k1 {k2}, xmm2, xmm3/m128/m32bcst, ib","EVEX.128.0F.W0 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed single-precision floating-point values in xmm3/m128/m32bcst and xmm2 using bits 4:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPS k1 {k2}, ymm2, ymm3/m256/m32bcst, ib","EVEX.256.0F.W0 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed single-precision floating-point values in ymm3/m256/m32bcst and ymm2 using bits 4:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPS k1 {k2}, zmm2, zmm3/m512/m32bcst{sae}, ib","EVEX.512.0F.W0 C2 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed single-precision floating-point values in zmm3/m512/m32bcst and zmm2 using bits 4:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPSD k1 {k2}, xmm2, xmm3/m64{sae}, ib","EVEX.LIG.F2.0F.W1 C2 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Compare low double-precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of ib as comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPSS k1 {k2}, xmm2, xmm3/m32{sae}, ib","EVEX.LIG.F3.0F.W0 C2 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Compare low single-precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of ib as comparison predicate with writemask k2 and leave the result in mask register k1."
"VCOMISD xmm1, xmm2/m64{sae}","EVEX.LIG.66.0F.W1 2F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r)","ModRM:r/m (r)","","","Tuple1 Scalar","Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VCOMISS xmm1, xmm2/m32{sae}","EVEX.LIG.0F.W0 2F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r)","ModRM:r/m (r)","","","Tuple1 Scalar","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VCOMPRESSPD xmm1/m128/f64x2 {k1}{z}, xmm2","EVEX.128.66.0F38.W1 8A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed double-precision floating-point values from xmm2 to xmm1/m128 using writemask k1."
"VCOMPRESSPD ymm1/m256/f64x4 {k1}{z}, ymm2","EVEX.256.66.0F38.W1 8A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed double-precision floating-point values from ymm2 to ymm1/m256 using writemask k1."
"VCOMPRESSPD zmm1/m512/f64x8 {k1}{z}, zmm2","EVEX.512.66.0F38.W1 8A /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed double-precision floating-point values from zmm2 using control mask k1 to zmm1/m512."
"VCOMPRESSPS xmm1/m128/f32x4 {k1}{z}, xmm2","EVEX.128.66.0F38.W0 8A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed single-precision floating-point values from xmm2 to xmm1/m128 using writemask k1."
"VCOMPRESSPS ymm1/m256/f32x8 {k1}{z}, ymm2","EVEX.256.66.0F38.W0 8A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed single-precision floating-point values from ymm2 to ymm1/m256 using writemask k1."
"VCOMPRESSPS zmm1/m512/f32x16 {k1}{z}, zmm2","EVEX.512.66.0F38.W0 8A /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed single-precision floating-point values from zmm2 using control mask k1 to zmm1/m512."
"VCVTDQ2PD xmm1 {k1}{z}, xmm2/m64/m32bcst","EVEX.128.F3.0F.W0 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert 2 packed signed doubleword integers from xmm2/m128/m32bcst to eight packed double-precision floating-point values in xmm1 with writemask k1."
"VCVTDQ2PD ymm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.256.F3.0F.W0 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert 4 packed signed doubleword integers from xmm2/m128/m32bcst to 4 packed double-precision floating-point values in ymm1 with writemask k1."
"VCVTDQ2PD zmm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.512.F3.0F.W0 E6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTDQ2PS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed signed doubleword integers from xmm2/m128/m32bcst to four packed single-precision floating-point values in xmm1with writemask k1."
"VCVTDQ2PS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed single-precision floating-point values in ymm1with writemask k1."
"VCVTDQ2PS zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.0F.W0 5B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed signed doubleword integers from zmm2/m512/m32bcst to sixteen packed single-precision floating-point values in zmm1with writemask k1."
"VCVTPD2DQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.F2.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two signed doubleword integers in xmm1 subject to writemask k1."
"VCVTPD2DQ xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.F2.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four signed doubleword integers in xmm1 subject to writemask k1."
"VCVTPD2DQ ymm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.F2.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight signed doubleword integers in ymm1 subject to writemask k1."
"VCVTPD2PS xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F.W1 5A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two single-precision floating-point values in xmm1with writemask k1."
"VCVTPD2PS xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F.W1 5A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four single-precision floating-point values in xmm1with writemask k1."
"VCVTPD2PS ymm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.66.0F.W1 5A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight single-precision floating-point values in ymm1with writemask k1."
"VCVTPD2UDQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.0F.W1 79 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two unsigned doubleword integers in xmm1 subject to writemask k1."
"VCVTPD2UDQ xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.0F.W1 79 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four unsigned doubleword integers in xmm1 subject to writemask k1."
"VCVTPD2UDQ ymm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.0F.W1 79 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight unsigned doubleword integers in ymm1 subject to writemask k1."
"VCVTPH2PS xmm1 {k1}{z}, xmm2/m64","EVEX.128.66.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Convert four packed half precision (16-bit) floating-point values in xmm2/m64 to packed single-precision floating-point values in xmm1."
"VCVTPH2PS ymm1 {k1}{z}, xmm2/m128","EVEX.256.66.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Convert eight packed half precision (16-bit) floating-point values in xmm2/m128 to packed single-precision floating-point values in ymm1."
"VCVTPH2PS zmm1 {k1}{z}, ymm2/m256 {sae}","EVEX.512.66.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Convert sixteen packed half precision (16-bit) floating-point values in ymm2/m256 to packed single-precision floating-point values in zmm1."
"VCVTPS2DQ xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 subject to writemask k1."
"VCVTPS2DQ ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 subject to writemask k1."
"VCVTPS2DQ zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.66.0F.W0 5B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 subject to writemask k1."
"VCVTPS2PD xmm1 {k1}{z}, xmm2/m64/m32bcst","EVEX.128.0F.W0 5A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert two packed single-precision floating-point values in xmm2/m64/m32bcst to packed double-precision floating-point values in xmm1 with writemask k1."
"VCVTPS2PD zmm1 {k1}{z}, ymm2/m256/m32bcst{sae}","EVEX.512.0F.W0 5A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert eight packed single-precision floating-point values in ymm2/m256/b32bcst to eight packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTPS2PH xmm1/m128 {k1}{z}, ymm2, ib","EVEX.256.66.0F3A.W0 1D /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Half Vector Mem","Convert eight packed single-precision floating-point values in ymm2 to packed half-precision (16-bit) floating-point values in xmm1/m128. ib provides rounding controls."
"VCVTPS2PH xmm1/m64 {k1}{z}, xmm2, ib","EVEX.128.66.0F3A.W0 1D /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Half Vector Mem","Convert four packed single-precision floating-point values in xmm2 to packed half-precision (16-bit) floating-point values in xmm1/m64. ib provides rounding controls."
"VCVTPS2PH ymm1/m256 {k1}{z}, zmm2{sae}, ib","EVEX.512.66.0F3A.W0 1D /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Half Vector Mem","Convert sixteen packed single-precision floating-point values in zmm2 to packed half-precision (16-bit) floating-point values in ymm1/m256. ib provides rounding controls."
"VCVTPS2UDQ xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.0F.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned doubleword values in xmm1 subject to writemask k1."
"VCVTPS2UDQ ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.0F.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned doubleword values in ymm1 subject to writemask k1."
"VCVTPS2UDQ zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.0F.W0 79 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed unsigned doubleword values in zmm1 subject to writemask k1."
"VCVTSD2SI rw, xmm1/m64{er}","EVEX.LIG.F2.0F.W0 2D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer r."
"VCVTSD2SI rw, xmm1/m64{er}","EVEX.LIG.F2.0F.W1 2D /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer signextended into r."
"VCVTSD2SS xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.F2.0F.W1 5A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one double-precision floating-point value in xmm3/m64 to one single-precision floating-point value and merge with high bits in xmm2 under writemask k1."
"VCVTSD2USI rw, xmm1/m64{er}","EVEX.LIG.F2.0F.W0 79 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one double-precision floating-point value from xmm1/m64 to one unsigned doubleword integer r."
"VCVTSD2USI rw, xmm1/m64{er}","EVEX.LIG.F2.0F.W1 79 /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one double-precision floating-point value from xmm1/m64 to one unsigned quadword integer zeroextended into r."
"VCVTSI2SD xmm1, xmm2, rw/mw","EVEX.LIG.F2.0F.W0 2A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one signed doubleword integer from r/m to one double-precision floating-point value in xmm1."
"VCVTSI2SD xmm1, xmm2, rw/mw{er}","EVEX.LIG.F2.0F.W1 2A /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one signed quadword integer from r/m to one double-precision floating-point value in xmm1."
"VCVTSI2SS xmm1, xmm2, rw/mw{er}","EVEX.LIG.F3.0F.W0 2A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one signed doubleword integer from r/m to one single-precision floating-point value in xmm1."
"VCVTSI2SS xmm1, xmm2, rw/mw{er}","EVEX.LIG.F3.0F.W1 2A /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one signed quadword integer from r/m to one single-precision floating-point value in xmm1."
"VCVTSS2SD xmm1{k1}{z}, xmm2, xmm3/m32{sae}","EVEX.LIG.F3.0F.W0 5A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one single-precision floating-point value in xmm3/m32 to one double-precision floating-point value and merge with high bits of xmm2 under writemask k1."
"VCVTSS2SI rw, xmm1/m32{er}","EVEX.LIG.F3.0F.W0 2D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r."
"VCVTSS2SI rw, xmm1/m32{er}","EVEX.LIG.F3.0F.W1 2D /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r."
"VCVTSS2USI rw, xmm1/m32{er}","EVEX.LIG.F3.0F.W0 79 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one single-precision floating-point value from xmm1/m32 to one unsigned doubleword integer in r."
"VCVTSS2USI rw, xmm1/m32{er}","EVEX.LIG.F3.0F.W1 79 /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one single-precision floating-point value from xmm1/m32 to one unsigned quadword integer in r."
"VCVTTPD2DQ xmm1{k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two signed doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2DQ xmm1{k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four signed doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2DQ ymm1{k1}{z}, zmm2/m512/m64bcst{sae}","EVEX.512.66.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight signed doubleword integers in ymm1 using truncation subject to writemask k1."
"VCVTTPD2UDQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.0F.W1 78 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two unsigned doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2UDQ xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.0F.W1 78 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four unsigned doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2UDQ ymm1 {k1}{z}, zmm2/m512/m64bcst{sae}","EVEX.512.0F.W1 78 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight unsigned doubleword integers in ymm1 using truncation subject to writemask k1."
"VCVTTPS2DQ xmm1{k1}{z}, xmm2/m128/m32bcst","EVEX.128.F3.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 using truncation subject to writemask k1."
"VCVTTPS2DQ ymm1{k1}{z}, ymm2/m256/m32bcst","EVEX.256.F3.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 using truncation subject to writemask k1."
"VCVTTPS2DQ zmm1{k1}{z}, zmm2/m512/m32bcst {sae}","EVEX.512.F3.0F.W0 5B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 using truncation subject to writemask k1."
"VCVTTPS2UDQ xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.0F.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned doubleword values in xmm1 using truncation subject to writemask k1."
"VCVTTPS2UDQ ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.0F.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned doubleword values in ymm1 using truncation subject to writemask k1."
"VCVTTPS2UDQ zmm1 {k1}{z}, zmm2/m512/m32bcst{sae}","EVEX.512.0F.W0 78 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed unsigned doubleword values in zmm1 using truncation subject to writemask k1."
"VCVTTSD2SI rw, xmm1/m64{sae}","EVEX.LIG.F2.0F.W0 2C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r using truncation."
"VCVTTSD2SI rw, xmm1/m64{sae}","EVEX.LIG.F2.0F.W1 2C /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer in r using truncation."
"VCVTTSD2USI rw, xmm1/m64{sae}","EVEX.LIG.F2.0F.W0 78 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one double-precision floating-point value from xmm1/m64 to one unsigned doubleword integer r using truncation."
"VCVTTSD2USI rw, xmm1/m64{sae}","EVEX.LIG.F2.0F.W1 78 /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one double-precision floating-point value from xmm1/m64 to one unsigned quadword integer zeroextended into r using truncation."
"VCVTTSS2SI rw, xmm1/m32{sae}","EVEX.LIG.F3.0F.W0 2C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r using truncation."
"VCVTTSS2SI rw, xmm1/m32{sae}","EVEX.LIG.F3.0F.W1 2C /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r using truncation."
"VCVTTSS2USI rw, xmm1/m32{sae}","EVEX.LIG.F3.0F.W0 78 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one single-precision floating-point value from xmm1/m32 to one unsigned doubleword integer in r using truncation."
"VCVTTSS2USI rw, xmm1/m32{sae}","EVEX.LIG.F3.0F.W1 78 /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one single-precision floating-point value from xmm1/m32 to one unsigned quadword integer in r using truncation."
"VCVTUDQ2PD xmm1 {k1}{z}, xmm2/m64/m32bcst","EVEX.128.F3.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert two packed unsigned doubleword integers from ymm2/m64/m32bcst to packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTUDQ2PD ymm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.256.F3.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert four packed unsigned doubleword integers from xmm2/m128/m32bcst to packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTUDQ2PD zmm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.512.F3.0F.W0 7A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert eight packed unsigned doubleword integers from ymm2/m256/m32bcst to eight packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTUDQ2PS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.F2.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed unsigned doubleword integers from xmm2/m128/m32bcst to packed single-precision floating-point values in xmm1 with writemask k1."
"VCVTUDQ2PS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.F2.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed unsigned doubleword integers from ymm2/m256/m32bcst to packed single-precision floating-point values in zmm1 with writemask k1."
"VCVTUDQ2PS zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.F2.0F.W0 7A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed unsigned doubleword integers from zmm2/m512/m32bcst to sixteen packed single-precision floating-point values in zmm1 with writemask k1."
"VCVTUSI2SD xmm1, xmm2, rw/mw","EVEX.LIG.F2.0F.W0 7B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one unsigned doubleword integer from r/m to one double-precision floating-point value in xmm1."
"VCVTUSI2SD xmm1, xmm2, rw/mw{er}","EVEX.LIG.F2.0F.W1 7B /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one unsigned quadword integer from r/m to one double-precision floating-point value in xmm1."
"VCVTUSI2SS xmm1, xmm2, rw/mw{er}","EVEX.LIG.F3.0F.W0 7B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one signed doubleword integer from r/m to one single-precision floating-point value in xmm1."
"VCVTUSI2SS xmm1, xmm2, rw/mw{er}","EVEX.LIG.F3.0F.W1 7B /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert one signed quadword integer from r/m to one single-precision floating-point value in xmm1."
"VDIVPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 5E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Divide packed double-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/m128/m64bcst and write results to xmm1 subject to writemask k1."
"VDIVPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 5E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Divide packed double-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/m256/m64bcst and write results to ymm1 subject to writemask k1."
"VDIVPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F.W1 5E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Divide packed double-precision floating-point values in zmm2 by packed double-precision FP values in zmm3/m512/m64bcst and write results to zmm1 subject to writemask k1."
"VDIVPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.0F.W0 5E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Divide packed single-precision floating-point values in xmm2 by packed single-precision floating-point values in xmm3/m128/m32bcst and write results to xmm1 subject to writemask k1."
"VDIVPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.0F.W0 5E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Divide packed single-precision floating-point values in ymm2 by packed single-precision floating-point values in ymm3/m256/m32bcst and write results to ymm1 subject to writemask k1."
"VDIVPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.0F.W0 5E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Divide packed single-precision floating-point values in zmm2 by packed single-precision floating-point values in zmm3/m512/m32bcst and write results to zmm1 subject to writemask k1."
"VDIVSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.F2.0F.W1 5E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Divide low double-precision floating-point value in xmm2 by low double-precision floating-point value in xmm3/m64."
"VDIVSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.F3.0F.W0 5E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Divide low single-precision floating-point value in xmm2 by low single-precision floating-point value in xmm3/m32."
"VEXPANDPD xmm1 {k1}{z}, xmm2/m128/f64x2","EVEX.128.66.0F38.W1 88 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed double-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VEXPANDPD ymm1 {k1}{z}, ymm2/m256/f64x4","EVEX.256.66.0F38.W1 88 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed double-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VEXPANDPD zmm1 {k1}{z}, zmm2/m512/f64x8","EVEX.512.66.0F38.W1 88 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed double-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VEXPANDPS xmm1 {k1}{z}, xmm2/m128/f32x4","EVEX.128.66.0F38.W0 88 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed single-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VEXPANDPS ymm1 {k1}{z}, ymm2/m256/f32x8","EVEX.256.66.0F38.W0 88 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed single-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VEXPANDPS zmm1 {k1}{z}, zmm2/m512/f32x16","EVEX.512.66.0F38.W0 88 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed single-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VEXTRACTF32X4 xmm/m128{k}{z},ymm,ib","EVEX.256.66.0F3A.W0 19 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Tuple4","Extract 128 bits of packed single precision floating-point values from ymm2 and store results in xmm1/m128 subject to writemask k1."
"VEXTRACTF32X4 xmm/m128{k}{z},zmm,ib","EVEX.512.66.0F3A.W0 19 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Tuple4","Extract 128 bits of packed single precision floating-point values from zmm2 and store results in xmm1/m128 subject to writemask k1."
"VEXTRACTF64X4 ymm/m256{k}{z},zmm,ib","EVEX.512.66.0F3A.W1 1b /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Tuple4","Extract 256 bits of packed double precision floating-point values from zmm2 and store results in ymm1/m256 subject to writemask k1."
"VEXTRACTI32X4 xmm/m128{k}{z},ymm,ib","EVEX.256.66.0F3A.W0 39 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Tuple4","Extract 128 bits of double-word integer values from ymm2 and store results in xmm1/m128 subject to writemask k1."
"VEXTRACTI32X4 xmm/m128{k}{z},zmm,ib","EVEX.512.66.0F3A.W0 39 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Tuple4","Extract 128 bits of double-word integer values from zmm2 and store results in xmm1/m128 subject to writemask k1."
"VEXTRACTI64X4 ymm/m256{k}{z},zmm,ib","EVEX.512.66.0F3A.W1 3b /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Tuple4","Extract 256 bits of quad-word integer values from zmm2 and store results in ymm1/m256 subject to writemask k1."
"VEXTRACTPS r32/m32, xmm1, ib","EVEX.128.66.0F3A.WIG 17 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","ib","","Tuple1 Scalar","Extract one single-precision floating-point value from xmm1 at the offset specified by ib and store the result in reg or m32. Zero extend the results in 64-bit register if applicable."
"VFIXUPIMMPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst, ib","EVEX.128.66.0F3A.W1 54 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Fix up special numbers in float64 vector xmm1, float64 vector xmm2 and int64 vector xmm3/m128/m64bcst and store the result in xmm1, under writemask."
"VFIXUPIMMPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 54 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Fix up special numbers in float64 vector ymm1, float64 vector ymm2 and int64 vector ymm3/m256/m64bcst and store the result in ymm1, under writemask."
"VFIXUPIMMPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{sae}, ib","EVEX.512.66.0F3A.W1 54 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Fix up elements of float64 vector in zmm2 using int64 vector table in zmm3/m512/m64bcst, combine with preserved elements from zmm1, and store the result in zmm1."
"VFIXUPIMMPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst, ib","EVEX.128.66.0F3A.W0 54 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Fix up special numbers in float32 vector xmm1, float32 vector xmm2 and int32 vector xmm3/m128/m32bcst and store the result in xmm1, under writemask."
"VFIXUPIMMPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 54 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Fix up special numbers in float32 vector ymm1, float32 vector ymm2 and int32 vector ymm3/m256/m32bcst and store the result in ymm1, under writemask."
"VFIXUPIMMPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{sae}, ib","EVEX.512.66.0F3A.W0 54 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Fix up elements of float32 vector in zmm2 using int32 vector table in zmm3/m512/m32bcst, combine with preserved elements from zmm1, and store the result in zmm1."
"VFIXUPIMMSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}, ib","EVEX.LIG.66.0F3A.W1 55 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Fix up a float64 number in the low quadword element of xmm2 using scalar int32 table in xmm3/m64 and store the result in xmm1."
"VFIXUPIMMSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}, ib","EVEX.LIG.66.0F3A.W0 55 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Fix up a float32 number in the low doubleword element in xmm2 using scalar int32 table in xmm3/m32 and store the result in xmm1."
"VFMADD132PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 98 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, add to xmm2 and put result in xmm1."
"VFMADD132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 98 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, add to ymm2 and put result in ymm1."
"VFMADD132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 98 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, add to zmm2 and put result in zmm1."
"VFMADD132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 98 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, add to xmm2 and put result in xmm1."
"VFMADD132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 98 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, add to ymm2 and put result in ymm1."
"VFMADD132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 98 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, add to zmm2 and put result in zmm1."
"VFMADD132SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 99 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, add to xmm2 and put result in xmm1."
"VFMADD132SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 99 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, add to xmm2 and put result in xmm1."
"VFMADD213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 A8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2, add to xmm3/m128/m64bcst and put result in xmm1."
"VFMADD213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 A8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2, add to ymm3/m256/m64bcst and put result in ymm1."
"VFMADD213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 A8 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm2, add to zmm3/m512/m64bcst and put result in zmm1."
"VFMADD213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 A8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, add to xmm3/m128/m32bcst and put result in xmm1."
"VFMADD213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 A8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, add to ymm3/m256/m32bcst and put result in ymm1."
"VFMADD213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 A8 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, add to zmm3/m512/m32bcst and put result in zmm1."
"VFMADD213SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 A9 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm2, add to xmm3/m64 and put result in xmm1."
"VFMADD213SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 A9 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm2, add to xmm3/m32 and put result in xmm1."
"VFMADD231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 B8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, add to xmm1 and put result in xmm1."
"VFMADD231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 B8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, add to ymm1 and put result in ymm1."
"VFMADD231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 B8 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, add to zmm1 and put result in zmm1."
"VFMADD231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 B8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, add to xmm1 and put result in xmm1."
"VFMADD231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 B8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, add to ymm1 and put result in ymm1."
"VFMADD231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 B8 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, add to zmm1 and put result in zmm1."
"VFMADD231SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 B9 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, add to xmm1 and put result in xmm1."
"VFMADD231SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 B9 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, add to xmm1 and put result in xmm1."
"VFMADDSUB132PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 96 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst,add/subtract elements in xmm2 and put result in xmm1 subject to writemask k1."
"VFMADDSUB132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 96 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst,add/subtract elements in ymm2 and put result in ymm1 subject to writemask k1."
"VFMADDSUB132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 96 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst,add/subtract elements in zmm2 and put result in zmm1 subject to writemask k1."
"VFMADDSUB132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 96 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, add/subtract elements in zmm2 and put result in xmm1 subject to writemask k1."
"VFMADDSUB132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 96 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, add/subtract elements in ymm2 and put result in ymm1 subject to writemask k1."
"VFMADDSUB132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 96 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, add/subtract elements in zmm2 and put result in zmm1 subject to writemask k1."
"VFMADDSUB213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 A6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2,add/subtract elements in xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1."
"VFMADDSUB213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 A6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2,add/subtract elements in ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1."
"VFMADDSUB213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 A6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1and zmm2,add/subtract elements in zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1."
"VFMADDSUB213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 A6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, add/subtract elements in xmm3/m128/m32bcst and put result in xmm1 subject to writemask k1."
"VFMADDSUB213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 A6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, add/subtract elements in ymm3/m256/m32bcst and put result in ymm1 subject to writemask k1."
"VFMADDSUB213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 A6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, add/subtract elements in zmm3/m512/m32bcst and put result in zmm1 subject to writemask k1."
"VFMADDSUB231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 B6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst,add/subtract elements in xmm1 and put result in xmm1 subject to writemask k1."
"VFMADDSUB231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 B6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst,add/subtract elements in ymm1 and put result in ymm1 subject to writemask k1."
"VFMADDSUB231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 B6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst,add/subtract elements in zmm1 and put result in zmm1 subject to writemask k1."
"VFMADDSUB231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 B6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, add/subtract elements in xmm1 and put result in xmm1 subject to writemask k1."
"VFMADDSUB231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 B6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, add/subtract elements in ymm1 and put result in ymm1 subject to writemask k1."
"VFMADDSUB231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 B6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, add/subtract elements in zmm1 and put result in zmm1 subject to writemask k1."
"VFMSUB132PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 9A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, subtract xmm2 and put result in xmm1 subject to writemask k1."
"VFMSUB132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 9A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, subtract ymm2 and put result in ymm1 subject to writemask k1."
"VFMSUB132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 9A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, subtract zmm2 and put result in zmm1 subject to writemask k1."
"VFMSUB132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 9A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, subtract xmm2 and put result in xmm1."
"VFMSUB132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 9A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, subtract ymm2 and put result in ymm1."
"VFMSUB132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 9A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, subtract zmm2 and put result in zmm1."
"VFMSUB132SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 9B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, subtract xmm2 and put result in xmm1."
"VFMSUB132SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 9B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, subtract xmm2 and put result in xmm1."
"VFMSUB213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 AA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2, subtract xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1."
"VFMSUB213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 AA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2, subtract ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1."
"VFMSUB213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 AA /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm2, subtract zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1."
"VFMSUB213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 AA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract xmm3/m128/m32bcst and put result in xmm1."
"VFMSUB213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 AA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract ymm3/m256/m32bcst and put result in ymm1."
"VFMSUB213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 AA /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, subtract zmm3/m512/m32bcst and put result in zmm1."
"VFMSUB213SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 AB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm2, subtract xmm3/m64 and put result in xmm1."
"VFMSUB213SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 AB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm2, subtract xmm3/m32 and put result in xmm1."
"VFMSUB231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 BA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, subtract xmm1 and put result in xmm1 subject to writemask k1."
"VFMSUB231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 BA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, subtract ymm1 and put result in ymm1 subject to writemask k1."
"VFMSUB231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 BA /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, subtract zmm1 and put result in zmm1 subject to writemask k1."
"VFMSUB231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 BA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, subtract xmm1 and put result in xmm1."
"VFMSUB231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 BA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, subtract ymm1 and put result in ymm1."
"VFMSUB231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 BA /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, subtract zmm1 and put result in zmm1."
"VFMSUB231SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 BB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, subtract xmm1 and put result in xmm1."
"VFMSUB231SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 BB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, subtract xmm1 and put result in xmm1."
"VFMSUBADD132PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 97 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst,subtract/add elements in xmm2 and put result in xmm1 subject to writemask k1."
"VFMSUBADD132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 97 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst,subtract/add elements in ymm2 and put result in ymm1 subject to writemask k1."
"VFMSUBADD132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 97 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst,subtract/add elements in zmm2 and put result in zmm1 subject to writemask k1."
"VFMSUBADD132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 97 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, subtract/add elements in xmm2 and put result in xmm1 subject to writemask k1."
"VFMSUBADD132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 97 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, subtract/add elements in ymm2 and put result in ymm1 subject to writemask k1."
"VFMSUBADD132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 97 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, subtract/add elements in zmm2 and put result in zmm1 subject to writemask k1."
"VFMSUBADD213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 A7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2,subtract/add elements in xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1."
"VFMSUBADD213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 A7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2,subtract/add elements in ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1."
"VFMSUBADD213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 A7 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm2,subtract/add elements in zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1."
"VFMSUBADD213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 A7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract/add elements in xmm3/m128/m32bcst and put result in xmm1 subject to writemask k1."
"VFMSUBADD213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 A7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract/add elements in ymm3/m256/m32bcst and put result in ymm1 subject to writemask k1."
"VFMSUBADD213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 A7 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, subtract/add elements in zmm3/m512/m32bcst and put result in zmm1 subject to writemask k1."
"VFMSUBADD231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 B7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst,subtract/add elements in xmm1 and put result in xmm1 subject to writemask k1."
"VFMSUBADD231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 B7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst,subtract/add elements in ymm1 and put result in ymm1 subject to writemask k1."
"VFMSUBADD231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 B7 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst,subtract/add elements in zmm1 and put result in zmm1 subject to writemask k1."
"VFMSUBADD231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 B7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, subtract/add elements in xmm1 and put result in xmm1 subject to writemask k1."
"VFMSUBADD231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 B7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, subtract/add elements in ymm1 and put result in ymm1 subject to writemask k1."
"VFMSUBADD231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 B7 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, subtract/add elements in zmm1 and put result in zmm1 subject to writemask k1."
"VFNMADD132PD xmm0 {k1}{z}, xmm1, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, negate the multiplication result and add to ymm2 and put result in ymm1."
"VFNMADD132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 9C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, negate the multiplication result and add to zmm2 and put result in zmm1."
"VFNMADD132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, negate the multiplication result and add to ymm2 and put result in ymm1."
"VFNMADD132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, negate the multiplication result and add to zmm2 and put result in zmm1."
"VFNMADD132SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 9D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD132SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 9D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 AC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/m128/m64bcst and put result in xmm1."
"VFNMADD213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 AC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/m256/m64bcst and put result in ymm1."
"VFNMADD213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 AC /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm2, negate the multiplication result and add to zmm3/m512/m64bcst and put result in zmm1."
"VFNMADD213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 AC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/m128/m32bcst and put result in xmm1."
"VFNMADD213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 AC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/m256/m32bcst and put result in ymm1."
"VFNMADD213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 AC /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, negate the multiplication result and add to zmm3/m512/m32bcst and put result in zmm1."
"VFNMADD213SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 AD /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m64 and put result in xmm1."
"VFNMADD213SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 AD /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m32 and put result in xmm1."
"VFNMADD231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 BC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 BC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, negate the multiplication result and add to ymm1 and put result in ymm1."
"VFNMADD231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 BC /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, negate the multiplication result and add to zmm1 and put result in zmm1."
"VFNMADD231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 BC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 BC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, negate the multiplication result and add to ymm1 and put result in ymm1."
"VFNMADD231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 BC /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, negate the multiplication result and add to zmm1 and put result in zmm1."
"VFNMADD231SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 BD /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD231SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 BD /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMSUB132PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 9E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 9E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, negate the multiplication result and subtract ymm2 and put result in ymm1."
"VFNMSUB132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 9E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, negate the multiplication result and subtract zmm2 and put result in zmm1."
"VFNMSUB132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 9E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 9E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, negate the multiplication result and subtract ymm2 and put result in ymm1."
"VFNMSUB132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 9E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, negate the multiplication result and subtract zmm2 and put result in zmm1."
"VFNMSUB132SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 9F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB132SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 9F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 AE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m128/m64bcst and put result in xmm1."
"VFNMSUB213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 AE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/m256/m64bcst and put result in ymm1."
"VFNMSUB213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 AE /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm2, negate the multiplication result and subtract zmm3/m512/m64bcst and put result in zmm1."
"VFNMSUB213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 AE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m128/m32bcst and put result in xmm1."
"VFNMSUB213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 AE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/m256/m32bcst and put result in ymm1."
"VFNMSUB213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 AE /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, negate the multiplication result and subtract zmm3/m512/m32bcst and put result in zmm1."
"VFNMSUB213SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 AF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m64 and put result in xmm1."
"VFNMSUB213SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 AF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m32 and put result in xmm1."
"VFNMSUB231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 BE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VFNMSUB231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 BE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, negate the multiplication result and subtract ymm1 and put result in ymm1."
"VFNMSUB231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 BE /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, negate the multiplication result and subtract zmm1 and put result in zmm1."
"VFNMSUB231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 BE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, negate the multiplication result subtract add to xmm1 and put result in xmm1."
"VFNMSUB231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 BE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, negate the multiplication result subtract add to ymm1 and put result in ymm1."
"VFNMSUB231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 BE /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, negate the multiplication result subtract add to zmm1 and put result in zmm1."
"VFNMSUB231SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 BF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VFNMSUB231SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 BF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VGATHERDPD xmm1 {k1}, vm32x/f64x2","EVEX.128.66.0F38.W1 92 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather float64 vector into float64 vector xmm1 using k1 as completion mask."
"VGATHERDPD ymm1 {k1}, vm32x/f64x4","EVEX.256.66.0F38.W1 92 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather float64 vector into float64 vector ymm1 using k1 as completion mask."
"VGATHERDPD zmm1 {k1}, vm32y/f64x8","EVEX.512.66.0F38.W1 92 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather float64 vector into float64 vector zmm1 using k1 as completion mask."
"VGATHERDPS xmm1 {k1}, vm32x/f32x4","EVEX.128.66.0F38.W0 92 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERDPS ymm1 {k1}, vm32y/f32x8","EVEX.256.66.0F38.W0 92 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERDPS zmm1 {k1}, vm32z/f32x16","EVEX.512.66.0F38.W0 92 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERQPD xmm1 {k1}, vm64x/f64x2","EVEX.128.66.0F38.W1 93 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather float64 vector into float64 vector xmm1 using k1 as completion mask."
"VGATHERQPD ymm1 {k1}, vm64y/f64x4","EVEX.256.66.0F38.W1 93 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather float64 vector into float64 vector ymm1 using k1 as completion mask."
"VGATHERQPD zmm1 {k1}, vm64z/f64x8","EVEX.512.66.0F38.W1 93 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather float64 vector into float64 vector zmm1 using k1 as completion mask."
"VGATHERQPS xmm1 {k1}, vm64x/f32x2","EVEX.128.66.0F38.W0 93 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERQPS xmm1 {k1}, vm64y/f32x4","EVEX.256.66.0F38.W0 93 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERQPS ymm1 {k1}, vm64z/f32x8","EVEX.512.66.0F38.W0 93 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGETEXPPD xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 42 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert the exponent of packed double-precision floating-point values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPPD ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F38.W1 42 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert the exponent of packed double-precision floating-point values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPPD zmm1 {k1}{z}, zmm2/m512/m64bcst{sae}","EVEX.512.66.0F38.W1 42 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert the exponent of packed double-precision floating-point values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination under writemask k1."
"VGETEXPPS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F38.W0 42 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert the exponent of packed single-precision floating-point values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPPS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F38.W0 42 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert the exponent of packed single-precision floating-point values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPPS zmm1 {k1}{z}, zmm2/m512/m32bcst{sae}","EVEX.512.66.0F38.W0 42 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert the exponent of packed single-precision floating-point values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}","EVEX.LIG.66.0F38.W1 43 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert the biased exponent (bits 62:52) of the low double-precision floating-point value in xmm3/m64 to a DP FP value representing unbiased integer exponent. Stores the result to the low 64-bit of xmm1 under the writemask k1 and merge with the other elements of xmm2."
"VGETEXPSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","EVEX.LIG.66.0F38.W0 43 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Convert the biased exponent (bits 30:23) of the low single-precision floating-point value in xmm3/m32 to a SP FP value representing unbiased integer exponent. Stores the result to xmm1 under the writemask k1 and merge with the other elements of xmm2."
"VGETMANTPD xmm1 {k1}{z}, xmm2/m128/m64bcst, ib","EVEX.128.66.0F3A.W1 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Get Normalized Mantissa from float64 vector xmm2/m128/m64bcst and store the result in xmm1, using ib for sign control and mantissa interval normalization, under writemask."
"VGETMANTPD ymm1 {k1}{z}, ymm2/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Get Normalized Mantissa from float64 vector ymm2/m256/m64bcst and store the result in ymm1, using ib for sign control and mantissa interval normalization, under writemask."
"VGETMANTPD zmm1 {k1}{z}, zmm2/m512/m64bcst{sae}, ib","EVEX.512.66.0F3A.W1 26 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Get Normalized Mantissa from float64 vector zmm2/m512/m64bcst and store the result in zmm1, using ib for sign control and mantissa interval normalization, under writemask."
"VGETMANTPS xmm1 {k1}{z}, xmm2/m128/m32bcst, ib","EVEX.128.66.0F3A.W0 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Get normalized mantissa from float32 vector xmm2/m128/m32bcst and store the result in xmm1, using ib for sign control and mantissa interval normalization, under writemask."
"VGETMANTPS ymm1 {k1}{z}, ymm2/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Get normalized mantissa from float32 vector ymm2/m256/m32bcst and store the result in ymm1, using ib for sign control and mantissa interval normalization, under writemask."
"VGETMANTPS zmm1 {k1}{z}, zmm2/m512/m32bcst{sae}, ib","EVEX.512.66.0F3A.W0 26 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Get normalized mantissa from float32 vector zmm2/m512/m32bcst and store the result in zmm1, using ib for sign control and mantissa interval normalization, under writemask."
"VGETMANTSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}, ib","EVEX.LIG.66.0F3A.W1 27 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Extract the normalized mantissa of the low float64 element in xmm3/m64 using ib for sign control and mantissa interval normalization. Store the mantissa to xmm1 under the writemask k1 and merge with the other elements of xmm2."
"VGETMANTSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}, ib","EVEX.LIG.66.0F3A.W0 27 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Extract the normalized mantissa from the low float32 element of xmm3/m32 using ib for sign control and mantissa interval normalization, store the mantissa to xmm1 under the writemask k1 and merge with the other elements of xmm2."
"VINSERTF32X4 ymm{k}{z},ymm,xmm/m128,ib","EVEX.256.66.0F3A.W0 18 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple4","Insert 128 bits of packed single-precision floating-point values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1."
"VINSERTF32X4 zmm{k}{z},zmm,xmm/m128,ib","EVEX.512.66.0F3A.W0 18 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple4","Insert 128 bits of packed single-precision floating-point values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1."
"VINSERTF64X4 zmm{k}{z},zmm,ymm/m256,ib","EVEX.512.66.0F3A.W1 1a /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple4","Insert 256 bits of packed double precision floating-point values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1."
"VINSERTI32X4 ymm{k}{z},ymm,xmm/m128,ib","EVEX.256.66.0F3A.W0 38 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple4","Insert 128 bits of packed doubleword integer values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1."
"VINSERTI32X4 zmm{k}{z},zmm,xmm/m128,ib","EVEX.512.66.0F3A.W0 38 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple4","Insert 128 bits of packed doubleword integer values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1."
"VINSERTI64X4 zmm{k}{z},zmm,ymm/m256,ib","EVEX.512.66.0F3A.W1 3a /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple4","Insert 256 bits of packed quadword integer values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1."
"VINSERTPS xmm1, xmm2, xmm3/m32, ib","EVEX.128.66.0F3A.W0 21 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Insert a single-precision floating-point value selected by ib from xmm3/m32 and merge with values in xmm2 at the specified destination element specified by ib and write out the result and zero out destination elements in xmm1 as indicated in ib."
"VMAXPD xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 5F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the maximum packed double-precision floating-point values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1."
"VMAXPD ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 5F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the maximum packed double-precision floating-point values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1."
"VMAXPD zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst{sae}","EVEX.512.66.0F.W1 5F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the maximum packed double-precision floating-point values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1."
"VMAXPS xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.0F.W0 5F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the maximum packed single-precision floating-point values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1."
"VMAXPS ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.0F.W0 5F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the maximum packed single-precision floating-point values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1."
"VMAXPS zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst{sae}","EVEX.512.0F.W0 5F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the maximum packed single-precision floating-point values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1."
"VMAXSD xmm1{k1}{z}, xmm2, xmm3/m64{sae}","EVEX.LIG.F2.0F.W1 5F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Return the maximum scalar double-precision floating-point value between xmm3/m64 and xmm2."
"VMAXSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","EVEX.LIG.F3.0F.W0 5F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Return the maximum scalar single-precision floating-point value between xmm3/m32 and xmm2."
"VMINPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 5D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the minimum packed double-precision floating-point values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1."
"VMINPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 5D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the minimum packed double-precision floating-point values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1."
"VMINPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{sae}","EVEX.512.66.0F.W1 5D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the minimum packed double-precision floating-point values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1."
"VMINPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.0F.W0 5D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the minimum packed single-precision floating-point values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1."
"VMINPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.0F.W0 5D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the minimum packed single-precision floating-point values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1."
"VMINPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{sae}","EVEX.512.0F.W0 5D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the minimum packed single-precision floating-point values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1."
"VMINSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}","EVEX.LIG.F2.0F.W1 5D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Return the minimum scalar double-precision floating-point value between xmm3/m64 and xmm2."
"VMINSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","EVEX.LIG.F3.0F.W0 5D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Return the minimum scalar single-precision floating-point value between xmm3/m32 and xmm2."
"VMOVAPD xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed double-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VMOVAPD ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed double-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VMOVAPD zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F.W1 28 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed double-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVAPD xmm2/m128 {k1}{z}, xmm1","EVEX.128.66.0F.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed double-precision floating-point values from xmm1 to xmm2/m128 using writemask k1."
"VMOVAPD ymm2/m256 {k1}{z}, ymm1","EVEX.256.66.0F.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed double-precision floating-point values from ymm1 to ymm2/m256 using writemask k1."
"VMOVAPD zmm2/m512 {k1}{z}, zmm1","EVEX.512.66.0F.W1 29 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed double-precision floating-point values from zmm1 to zmm2/m512 using writemask k1."
"VMOVAPS xmm1 {k1}{z}, xmm2/m128","EVEX.128.0F.W0 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed single-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VMOVAPS ymm1 {k1}{z}, ymm2/m256","EVEX.256.0F.W0 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed single-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VMOVAPS zmm1 {k1}{z}, zmm2/m512","EVEX.512.0F.W0 28 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed single-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVAPS xmm2/m128 {k1}{z}, xmm1","EVEX.128.0F.W0 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed single-precision floating-point values from xmm1 to xmm2/m128 using writemask k1."
"VMOVAPS ymm2/m256 {k1}{z}, ymm1","EVEX.256.0F.W0 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed single-precision floating-point values from ymm1 to ymm2/m256 using writemask k1."
"VMOVAPS zmm2/m512 {k1}{z}, zmm1","EVEX.512.0F.W0 29 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed single-precision floating-point values from zmm1 to zmm2/m512 using writemask k1."
"VMOVD xmm1, rw/mw","EVEX.128.66.0F.W0 6E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Move doubleword from r/m to xmm1."
"VMOVD rw/mw, xmm1","EVEX.128.66.0F.W0 7E /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Move doubleword from xmm1 register to r/m."
"VMOVDDUP xmm1 {k1}{z}, xmm2/m64","EVEX.128.F2.0F.W1 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","MOVDDUP","Move double-precision floating-point value from xmm2/m64 and duplicate each element into xmm1 subject to writemask k1."
"VMOVDDUP ymm1 {k1}{z}, ymm2/m256","EVEX.256.F2.0F.W1 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","MOVDDUP","Move even index double-precision floating-point values from ymm2/m256 and duplicate each element into ymm1 subject to writemask k1."
"VMOVDDUP zmm1 {k1}{z}, zmm2/m512","EVEX.512.F2.0F.W1 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","MOVDDUP","Move even index double-precision floating-point values from zmm2/m512 and duplicate each element into zmm1 subject to writemask k1."
"VMOVDQA32 xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F.W0 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed doubleword integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQA32 ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F.W0 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed doubleword integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQA32 zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F.W0 6F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed doubleword integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQA32 xmm2/m128 {k1}{z}, xmm1","EVEX.128.66.0F.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed doubleword integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQA32 ymm2/m256 {k1}{z}, ymm1","EVEX.256.66.0F.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed doubleword integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQA32 zmm2/m512 {k1}{z}, zmm1","EVEX.512.66.0F.W0 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed doubleword integer values from zmm1 to zmm2/m512 using writemask k1."
"VMOVDQA64 xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F.W1 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned quadword integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQA64 ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F.W1 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned quadword integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQA64 zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F.W1 6F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move aligned packed quadword integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQA64 xmm2/m128 {k1}{z}, xmm1","EVEX.128.66.0F.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed quadword integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQA64 ymm2/m256 {k1}{z}, ymm1","EVEX.256.66.0F.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed quadword integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQA64 zmm2/m512 {k1}{z}, zmm1","EVEX.512.66.0F.W1 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move aligned packed quadword integer values from zmm1 to zmm2/m512 using writemask k1."
"VMOVDQU32 xmm1 {k1}{z}, xmm2/m128","EVEX.128.F3.0F.W0 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed doubleword integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQU32 ymm1 {k1}{z}, ymm2/m256","EVEX.256.F3.0F.W0 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed doubleword integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQU32 zmm1 {k1}{z}, zmm2/m512","EVEX.512.F3.0F.W0 6F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed doubleword integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQU32 xmm2/m128 {k1}{z}, xmm1","EVEX.128.F3.0F.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed doubleword integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQU32 ymm2/m256 {k1}{z}, ymm1","EVEX.256.F3.0F.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed doubleword integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQU32 zmm2/m512 {k1}{z}, zmm1","EVEX.512.F3.0F.W0 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed doubleword integer values from zmm1 to zmm2/m512 using writemask k1."
"VMOVDQU64 xmm1 {k1}{z}, xmm2/m128","EVEX.128.F3.0F.W1 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed quadword integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQU64 ymm1 {k1}{z}, ymm2/m256","EVEX.256.F3.0F.W1 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed quadword integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQU64 zmm1 {k1}{z}, zmm2/m512","EVEX.512.F3.0F.W1 6F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed quadword integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQU64 xmm2/m128 {k1}{z}, xmm1","EVEX.128.F3.0F.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed quadword integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQU64 ymm2/m256 {k1}{z}, ymm1","EVEX.256.F3.0F.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed quadword integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQU64 zmm2/m512 {k1}{z}, zmm1","EVEX.512.F3.0F.W1 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed quadword integer values from zmm1 to zmm2/m512 using writemask k1."
"VMOVHPD xmm2, xmm1, m64","EVEX.128.66.0F.W1 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Merge double-precision floating-point value from m64 and the low quadword of xmm1."
"VMOVHPD m64, xmm1","EVEX.128.66.0F.W1 17 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Move double-precision floating-point value from high quadword of xmm1 to m64."
"VMOVHPS xmm2, xmm1, m64","EVEX.128.0F.W0 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple2","Merge two packed single-precision floating-point values from m64 and the low quadword of xmm1."
"VMOVHPS m64, xmm1","EVEX.128.0F.W0 17 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple2","Move two packed single-precision floating-point values from high quadword of xmm1 to m64."
"VMOVLPD xmm2, xmm1, m64","EVEX.128.66.0F.W1 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Merge double-precision floating-point value from m64 and the high quadword of xmm1."
"VMOVLPD m64, xmm1","EVEX.128.66.0F.W1 13 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Move double -precision floating-point value from low quadword of xmm1 to m64."
"VMOVLPS xmm2, xmm1, m64","EVEX.128.0F.W0 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple2","Merge two packed single-precision floating-point values from m64 and the high quadword of xmm1."
"VMOVLPS m64, xmm1","EVEX.128.0F.W0 13 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple2","Move two packed single-precision floating-point values from low quadword of xmm1 to m64."
"VMOVNTDQ m128, xmm1","EVEX.128.66.0F.W0 E7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move packed integer values in xmm1 to m128 using non-temporal hint."
"VMOVNTDQ m256, ymm1","EVEX.256.66.0F.W0 E7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move packed integer values in zmm1 to m256 using non-temporal hint."
"VMOVNTDQ m512, zmm1","EVEX.512.66.0F.W0 E7 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move packed integer values in zmm1 to m512 using non-temporal hint."
"VMOVNTDQA xmm1, m128","EVEX.128.66.0F38.W0 2A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move 128-bit data from m128 to xmm using non-temporal hint if WC memory type."
"VMOVNTDQA ymm1, m256","EVEX.256.66.0F38.W0 2A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type."
"VMOVNTDQA zmm1, m512","EVEX.512.66.0F38.W0 2A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move 512-bit data from m512 to zmm using non-temporal hint if WC memory type."
"VMOVNTPD m128, xmm1","EVEX.128.66.0F.W1 2B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move packed double-precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPD m256, ymm1","EVEX.256.66.0F.W1 2B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move packed double-precision values in ymm1 to m256 using non-temporal hint."
"VMOVNTPD m512, zmm1","EVEX.512.66.0F.W1 2B /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move packed double-precision values in zmm1 to m512 using non-temporal hint."
"VMOVNTPS m128, xmm1","EVEX.128.0F.W0 2B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move packed single-precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPS m256, ymm1","EVEX.256.0F.W0 2B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move packed single-precision values in ymm1 to m256 using non-temporal hint."
"VMOVNTPS m512, zmm1","EVEX.512.0F.W0 2B /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move packed single-precision values in zmm1 to m512 using non-temporal hint."
"VMOVQ xmm1, rw/mw","EVEX.128.66.0F.W1 6E /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Move quadword from r/m to xmm1."
"VMOVQ rw/mw, xmm1","EVEX.128.66.0F.W1 7E /r","Valid","Invalid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Move quadword from xmm1 register to r/m."
"VMOVQ xmm1, xmm2/m64","EVEX.128.F3.0F.W1 7E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Move quadword from xmm2/m64 to xmm1."
"VMOVQ xmm1/m64, xmm2","EVEX.128.66.0F.W1 D6 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Move quadword from xmm2 register to xmm1/m64."
"VMOVSD xmm1 {k1}{z}, m64","EVEX.LIG.F2.0F.W1 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","ModRM:r/m (r)","","","Tuple1 Scalar","Load scalar double-precision floating-point value from m64 to xmm1 register under writemask k1."
"VMOVSD xmm1 {k1}{z}, xmm2, xmm3","EVEX.LIG.F2.0F.W1 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Merge scalar double-precision floating-point value from xmm2 and xmm3 registers to xmm1 under writemask k1."
"VMOVSD m64 {k1}, xmm1","EVEX.LIG.F2.0F.W1 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Store scalar double-precision floating-point value from xmm1 register to m64 under writemask k1."
"VMOVSD xmm1 {k1}{z}, xmm2, xmm3","EVEX.LIG.F2.0F.W1 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","EVEX.vvvv (r)","ModRM:reg (r)","","","Merge scalar double-precision floating-point value from xmm2 and xmm3 registers to xmm1 under writemask k1."
"VMOVSHDUP xmm1 {k1}{z}, xmm2/m128","EVEX.128.F3.0F.W0 16 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move odd index single-precision floating-point values from xmm2/m128 and duplicate each element into xmm1 under writemask."
"VMOVSHDUP ymm1 {k1}{z}, ymm2/m256","EVEX.256.F3.0F.W0 16 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move odd index single-precision floating-point values from ymm2/m256 and duplicate each element into ymm1 under writemask."
"VMOVSHDUP zmm1 {k1}{z}, zmm2/m512","EVEX.512.F3.0F.W0 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move odd index single-precision floating-point values from zmm2/m512 and duplicate each element into zmm1 under writemask."
"VMOVSLDUP xmm1 {k1}{z}, xmm2/m128","EVEX.128.F3.0F.W0 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move even index single-precision floating-point values from xmm2/m128 and duplicate each element into xmm1 under writemask."
"VMOVSLDUP ymm1 {k1}{z}, ymm2/m256","EVEX.256.F3.0F.W0 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move even index single-precision floating-point values from ymm2/m256 and duplicate each element into ymm1 under writemask."
"VMOVSLDUP zmm1 {k1}{z}, zmm2/m512","EVEX.512.F3.0F.W0 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move even index single-precision floating-point values from zmm2/m512 and duplicate each element into zmm1 under writemask."
"VMOVSS xmm1 {k1}{z}, m32","EVEX.LIG.F3.0F.W0 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","ModRM:r/m (r)","","","Tuple1 Scalar","Move scalar single-precision floating-point values from m32 to xmm1 under writemask k1."
"VMOVSS xmm1 {k1}{z}, xmm2, xmm3","EVEX.LIG.F3.0F.W0 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Move scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register under writemask k1."
"VMOVSS m32 {k1}, xmm1","EVEX.LIG.F3.0F.W0 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Move scalar single-precision floating-point values from xmm1 to m32 under writemask k1."
"VMOVSS xmm1 {k1}{z}, xmm2, xmm3","EVEX.LIG.F3.0F.W0 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","EVEX.vvvv (r)","ModRM:reg (r)","","","Move scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register under writemask k1."
"VMOVUPD xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F.W1 10 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed double-precision floating-point from xmm2/m128 to xmm1 using writemask k1."
"VMOVUPD ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F.W1 10 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed double-precision floating-point from ymm2/m256 to ymm1 using writemask k1."
"VMOVUPD zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F.W1 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed double-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVUPD xmm2/m128 {k1}{z}, xmm1","EVEX.128.66.0F.W1 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed double-precision floating-point from xmm1 to xmm2/m128 using writemask k1."
"VMOVUPD ymm2/m256 {k1}{z}, ymm1","EVEX.256.66.0F.W1 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed double-precision floating-point from ymm1 to ymm2/m256 using writemask k1."
"VMOVUPD zmm2/m512 {k1}{z}, zmm1","EVEX.512.66.0F.W1 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed double-precision floating-point values from zmm1 to zmm2/m512 using writemask k1."
"VMOVUPS xmm1 {k1}{z}, xmm2/m128","EVEX.128.0F.W0 10 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed single-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VMOVUPS ymm1 {k1}{z}, ymm2/m256","EVEX.256.0F.W0 10 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed single-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VMOVUPS zmm1 {k1}{z}, zmm2/m512","EVEX.512.0F.W0 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed single-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVUPS xmm2/m128 {k1}{z}, xmm1","EVEX.128.0F.W0 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed single-precision floating-point values from xmm1 to xmm2/m128 using writemask k1."
"VMOVUPS ymm2/m256 {k1}{z}, ymm1","EVEX.256.0F.W0 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed single-precision floating-point values from ymm1 to ymm2/m256 using writemask k1."
"VMOVUPS zmm2/m512 {k1}{z}, zmm1","EVEX.512.0F.W0 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed single-precision floating-point values from zmm1 to zmm2/m512 using writemask k1."
"VMULPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1."
"VMULPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1."
"VMULPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F.W1 59 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed double-precision floating-point values in zmm3/m512/m64bcst with zmm2 and store result in zmm1."
"VMULPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.0F.W0 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from xmm3/m128/m32bcst to xmm2 and store result in xmm1."
"VMULPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.0F.W0 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values from ymm3/m256/m32bcst to ymm2 and store result in ymm1."
"VMULPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst {er}","EVEX.512.0F.W0 59 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed single-precision floating-point values in zmm3/m512/m32bcst with zmm2 and store result in zmm1."
"VMULSD xmm1 {k1}{z}, xmm2, xmm3/m64 {er}","EVEX.LIG.F2.0F.W1 59 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply the low double-precision floating-point value in xmm3/m64 by low double-precision floating-point value in xmm2."
"VMULSS xmm1 {k1}{z}, xmm2, xmm3/m32 {er}","EVEX.LIG.F3.0F.W0 59 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Multiply the low single-precision floating-point value in xmm3/m32 by the low single-precision floating-point value in xmm2."
"VPABSD xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F38.W0 1E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of 32-bit integers in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSD ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F38.W0 1E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of 32-bit integers in xmm2/m256 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSD zmm1 {k1}{z}, zmm2/m512/m32bcst","EVEX.512.66.0F38.W0 1E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of 32-bit integers in xmm2/m512 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 1F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of 64-bit integers in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSQ ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F38.W1 1F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of 64-bit integers in xmm2/m256 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSQ zmm1 {k1}{z}, zmm2/m512/m64bcst","EVEX.512.66.0F38.W1 1F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of 64-bit integers in xmm2/m512 and store UNSIGNED result in xmm1 using writemask k1."
"VPADDD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 FE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed doubleword integers from xmm2, and xmm3/m128/m32bcst and store in xmm1 using writemask k1."
"VPADDD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F.W0 FE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed doubleword integers from ymm2, and ymm3/m256/m32bcst and store in ymm1 using writemask k1."
"VPADDD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F.W0 FE /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed doubleword integers from zmm2, and zmm3/m512/m32bcst and store in zmm1 using writemask k1."
"VPADDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 D4 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed quadword integers from xmm2, and xmm3/m128/m64bcst and store in xmm1 using writemask k1."
"VPADDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 D4 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed quadword integers from ymm2, and ymm3/m256/m64bcst and store in ymm1 using writemask k1."
"VPADDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F.W1 D4 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed quadword integers from zmm2, and zmm3/m512/m64bcst and store in zmm1 using writemask k1."
"VPANDD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 DB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1."
"VPANDD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F.W0 DB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1."
"VPANDD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F.W0 DB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1."
"VPANDND xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 DF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND NOT of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1."
"VPANDND ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F.W0 DF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND NOT of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1."
"VPANDND zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F.W0 DF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND NOT of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1."
"VPANDNQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 DF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND NOT of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1."
"VPANDNQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 DF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND NOT of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1."
"VPANDNQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F.W1 DF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND NOT of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1."
"VPANDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 DB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1."
"VPANDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 DB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1."
"VPANDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F.W1 DB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1."
"VPBLENDMD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 64 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend doubleword integer vector xmm2 and doubleword vector xmm3/m128/m32bcst and store the result in xmm1, under control mask."
"VPBLENDMD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 64 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend doubleword integer vector ymm2 and doubleword vector ymm3/m256/m32bcst and store the result in ymm1, under control mask."
"VPBLENDMD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 64 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend doubleword integer vector zmm2 and doubleword vector zmm3/m512/m32bcst and store the result in zmm1, under control mask."
"VPBLENDMQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 64 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend quadword integer vector xmm2 and quadword vector xmm3/m128/m64bcst and store the result in xmm1, under control mask."
"VPBLENDMQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 64 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend quadword integer vector ymm2 and quadword vector ymm3/m256/m64bcst and store the result in ymm1, under control mask."
"VPBLENDMQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 64 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Blend quadword integer vector zmm2 and quadword vector zmm3/m512/m64bcst and store the result in zmm1, under control mask."
"VPBROADCASTD xmm1 {k1}{z}, xmm2/m32","EVEX.128.66.0F38.W0 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a dword integer in the source operand to locations in xmm1 subject to writemask k1."
"VPBROADCASTD ymm1 {k1}{z}, xmm2/m32","EVEX.256.66.0F38.W0 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a dword integer in the source operand to locations in ymm1 subject to writemask k1."
"VPBROADCASTD zmm1 {k1}{z}, xmm2/m32","EVEX.512.66.0F38.W0 58 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a dword integer in the source operand to locations in zmm1 subject to writemask k1."
"VPBROADCASTD xmm1 {k1}{z}, rw","EVEX.128.66.0F38.W0 7C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a 32-bit value from a GPR to all double-words in the 128-bit destination subject to writemask k1."
"VPBROADCASTD ymm1 {k1}{z}, rw","EVEX.256.66.0F38.W0 7C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a 32-bit value from a GPR to all double-words in the 256-bit destination subject to writemask k1."
"VPBROADCASTD zmm1 {k1}{z}, rw","EVEX.512.66.0F38.W0 7C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a 32-bit value from a GPR to all double-words in the 512-bit destination subject to writemask k1."
"VPBROADCASTQ xmm1 {k1}{z}, xmm2/m64","EVEX.128.66.0F38.W1 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a qword element in source operand to locations in xmm1 subject to writemask k1."
"VPBROADCASTQ ymm1 {k1}{z}, xmm2/m64","EVEX.256.66.0F38.W1 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a qword element in source operand to locations in ymm1 subject to writemask k1."
"VPBROADCASTQ zmm1 {k1}{z}, xmm2/m64","EVEX.512.66.0F38.W1 59 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a qword element in source operand to locations in zmm1 subject to writemask k1."
"VPBROADCASTQ xmm1 {k1}{z}, rw","EVEX.128.66.0F38.W1 7C /r","Valid","Invalid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a 64-bit value from a GPR to all quad-words in the 128-bit destination subject to writemask k1."
"VPBROADCASTQ ymm1 {k1}{z}, rw","EVEX.256.66.0F38.W1 7C /r","Valid","Invalid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a 64-bit value from a GPR to all quad-words in the 256-bit destination subject to writemask k1."
"VPBROADCASTQ zmm1 {k1}{z}, rw","EVEX.512.66.0F38.W1 7C /r","Valid","Invalid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a 64-bit value from a GPR to all quad-words in the 512-bit destination subject to writemask k1."
"VPCMPD k1 {k2}, xmm2, xmm3/m128/m32bcst, ib","EVEX.128.66.0F3A.W0 1F /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed signed doubleword integer values in xmm3/m128/m32bcst and xmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPD k1 {k2}, ymm2, ymm3/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 1F /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed signed doubleword integer values in ymm3/m256/m32bcst and ymm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPD k1 {k2}, zmm2, zmm3/m512/m32bcst, ib","EVEX.512.66.0F3A.W0 1F /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed signed doubleword integer values in zmm2 and zmm3/m512/m32bcst using bits 2:0 of ib as a comparison predicate. The comparison results are written to the destination k1 under writemask k2."
"VPCMPEQD k1 {k2}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 76 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Equal between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQD k1 {k2}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F.W0 76 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Equal between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQD k1 {k2}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F.W0 76 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Equal between int32 vectors in zmm2 and zmm3/m512/m32bcst, and set destination k1 according to the comparison results under writemask k2."
"VPCMPEQQ k1 {k2}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Equal between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQQ k1 {k2}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Equal between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQQ k1 {k2}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Equal between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTD k1 {k2}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 66 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Greater between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTD k1 {k2}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F.W0 66 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Greater between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTD k1 {k2}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F.W0 66 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Greater between int32 elements in zmm2 and zmm3/m512/m32bcst,and set destination k1 according to the comparison results under writemask. k2."
"VPCMPGTQ k1 {k2}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 37 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Greater between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTQ k1 {k2}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 37 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Greater between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTQ k1 {k2}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 37 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare Greater between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPQ k1 {k2}, xmm2, xmm3/m128/m64bcst, ib","EVEX.128.66.0F3A.W1 1F /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed signed quadword integer values in xmm3/m128/m64bcst and xmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPQ k1 {k2}, ymm2, ymm3/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 1F /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed signed quadword integer values in ymm3/m256/m64bcst and ymm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPQ k1 {k2}, zmm2, zmm3/m512/m64bcst, ib","EVEX.512.66.0F3A.W1 1F /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed signed quadword integer values in zmm3/m512/m64bcst and zmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUD k1 {k2}, xmm2, xmm3/m128/m32bcst, ib","EVEX.128.66.0F3A.W0 1E /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed unsigned doubleword integer values in xmm3/m128/m32bcst and xmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUD k1 {k2}, ymm2, ymm3/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 1E /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed unsigned doubleword integer values in ymm3/m256/m32bcst and ymm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUD k1 {k2}, zmm2, zmm3/m512/m32bcst, ib","EVEX.512.66.0F3A.W0 1E /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed unsigned doubleword integer values in zmm2 and zmm3/m512/m32bcst using bits 2:0 of ib as a comparison predicate. The comparison results are written to the destination k1 under writemask k2."
"VPCMPUQ k1 {k2}, xmm2, xmm3/m128/m64bcst, ib","EVEX.128.66.0F3A.W1 1E /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed unsigned quadword integer values in xmm3/m128/m64bcst and xmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUQ k1 {k2}, ymm2, ymm3/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 1E /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed unsigned quadword integer values in ymm3/m256/m64bcst and ymm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUQ k1 {k2}, zmm2, zmm3/m512/m64bcst, ib","EVEX.512.66.0F3A.W1 1E /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed unsigned quadword integer values in zmm3/m512/m64bcst and zmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCOMPRESSD xmm1/m128/i32x4 {k1}{z}, xmm2","EVEX.128.66.0F38.W0 8B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed doubleword integer values from xmm2 to xmm1/m128 using controlmask k1."
"VPCOMPRESSD ymm1/m256/i32x8 {k1}{z}, ymm2","EVEX.256.66.0F38.W0 8B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed doubleword integer values from ymm2 to ymm1/m256 using controlmask k1."
"VPCOMPRESSD zmm1/m512/i32x16 {k1}{z}, zmm2","EVEX.512.66.0F38.W0 8B /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed doubleword integer values from zmm2 to zmm1/m512 using controlmask k1."
"VPCOMPRESSQ xmm1/m128/i64x2 {k1}{z}, xmm2","EVEX.128.66.0F38.W1 8B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed quadword integer values from xmm2 to xmm1/m128 using controlmask k1."
"VPCOMPRESSQ ymm1/m256/i64x4 {k1}{z}, ymm2","EVEX.256.66.0F38.W1 8B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed quadword integer values from ymm2 to ymm1/m256 using controlmask k1."
"VPCOMPRESSQ zmm1/m512/i64x8 {k1}{z}, zmm2","EVEX.512.66.0F38.W1 8B /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Tuple1 Scalar","Compress packed quadword integer values from zmm2 to zmm1/m512 using controlmask k1."
"VPERMD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 36 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute doublewords in ymm3/m256/m32bcst using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 36 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute doublewords in zmm3/m512/m32bcst using indices in zmm2 and store the result in zmm1 using writemask k1."
"VPERMI2D xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 76 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-words from two tables in xmm3/m128/m32bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1."
"VPERMI2D ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 76 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-words from two tables in ymm3/m256/m32bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1."
"VPERMI2D zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 76 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-words from two tables in zmm3/m512/m32bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1."
"VPERMI2PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 77 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-precision FP values from two tables in xmm3/m128/m64bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1."
"VPERMI2PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 77 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-precision FP values from two tables in ymm3/m256/m64bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1."
"VPERMI2PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 77 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-precision FP values from two tables in zmm3/m512/m64bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1."
"VPERMI2PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 77 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute single-precision FP values from two tables in xmm3/m128/m32bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1."
"VPERMI2PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 77 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute single-precision FP values from two tables in ymm3/m256/m32bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1."
"VPERMI2PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 77 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute single-precision FP values from two tables in zmm3/m512/m32bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1."
"VPERMI2Q xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 76 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute quad-words from two tables in xmm3/m128/m64bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1."
"VPERMI2Q ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 76 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute quad-words from two tables in ymm3/m256/m64bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1."
"VPERMI2Q zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 76 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute quad-words from two tables in zmm3/m512/m64bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1."
"VPERMILPD xmm1 {k1}{z}, xmm2/m128/m64bcst, ib","EVEX.128.66.0F3A.W1 05 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute double-precision floating-point values in xmm2/m128/m64bcst using controls from ib and store the result in xmm1 using writemask k1."
"VPERMILPD ymm1 {k1}{z}, ymm2/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 05 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute double-precision floating-point values in ymm2/m256/m64bcst using controls from ib and store the result in ymm1 using writemask k1."
"VPERMILPD zmm1 {k1}{z}, zmm2/m512/m64bcst, ib","EVEX.512.66.0F3A.W1 05 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute double-precision floating-point values in zmm2/m512/m64bcst using controls from ib and store the result in zmm1 using writemask k1."
"VPERMILPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 0D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-precision floating-point values in xmm2 using control from xmm3/m128/m64bcst and store the result in xmm1 using writemask k1."
"VPERMILPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 0D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-precision floating-point values in ymm2 using control from ymm3/m256/m64bcst and store the result in ymm1 using writemask k1."
"VPERMILPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 0D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-precision floating-point values in zmm2 using control from zmm3/m512/m64bcst and store the result in zmm1 using writemask k1."
"VPERMILPS xmm1 {k1}{z}, xmm2/m128/m32bcst, ib","EVEX.128.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute single-precision floating-point values xmm2/m128/m32bcst using controls from ib and store the result in xmm1 using writemask k1."
"VPERMILPS ymm1 {k1}{z}, ymm2/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute single-precision floating-point values ymm2/m256/m32bcst using controls from ib and store the result in ymm1 using writemask k1."
"VPERMILPS zmm1 {k1}{z}, zmm2/m512/m32bcst, ib","EVEX.512.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute single-precision floating-point values zmm2/m512/m32bcst using controls from ib and store the result in zmm1 using writemask k1."
"VPERMILPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute single-precision floating-point values xmm2 using control from xmm3/m128/m32bcst and store the result in xmm1 using writemask k1."
"VPERMILPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute single-precision floating-point values ymm2 using control from ymm3/m256/m32bcst and store the result in ymm1 using writemask k1."
"VPERMILPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute single-precision floating-point values zmm2 using control from zmm3/m512/m32bcst and store the result in zmm1 using writemask k1."
"VPERMPD ymm1 {k1}{z}, ymm2/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 01 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute double-precision floating-point elements in ymm2/m256/m64bcst using indexes in ib and store the result in ymm1 subject to writemask k1."
"VPERMPD zmm1 {k1}{z}, zmm2/m512/m64bcst, ib","EVEX.512.66.0F3A.W1 01 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute double-precision floating-point elements in zmm2/m512/m64bcst using indices in ib and store the result in zmm1 subject to writemask k1."
"VPERMPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 16 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-precision floating-point elements in ymm3/m256/m64bcst using indexes in ymm2 and store the result in ymm1 subject to writemask k1."
"VPERMPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute double-precision floating-point elements in zmm3/m512/m64bcst using indices in zmm2 and store the result in zmm1 subject to writemask k1."
"VPERMPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 16 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute single-precision floating-point elements in ymm3/m256/m32bcst using indexes in ymm2 and store the result in ymm1 subject to write mask k1."
"VPERMPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute single-precision floating-point values in zmm3/m512/m32bcst using indices in zmm2 and store the result in zmm1 subject to write mask k1."
"VPERMQ ymm1 {k1}{z}, ymm2/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 00 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute qwords in ymm2/m256/m64bcst using indexes in ib and store the result in ymm1."
"VPERMQ zmm1 {k1}{z}, zmm2/m512/m64bcst, ib","EVEX.512.66.0F3A.W1 00 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Permute qwords in zmm2/m512/m64bcst using indices in ib and store the result in zmm1."
"VPERMQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 36 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute qwords in ymm3/m256/m64bcst using indexes in ymm2 and store the result in ymm1."
"VPERMQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 36 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Permute qwords in zmm3/m512/m64bcst using indices in zmm2 and store the result in zmm1."
"VPERMT2D xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 7E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute double-words from two tables in xmm3/m128/m32bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2D ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 7E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute double-words from two tables in ymm3/m256/m32bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2D zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 7E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute double-words from two tables in zmm3/m512/m32bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1."
"VPERMT2PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute double-precision FP values from two tables in xmm3/m128/m64bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute double-precision FP values from two tables in ymm3/m256/m64bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute double-precision FP values from two tables in zmm3/m512/m64bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1."
"VPERMT2PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute single-precision FP values from two tables in xmm3/m128/m32bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute single-precision FP values from two tables in ymm3/m256/m32bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute single-precision FP values from two tables in zmm3/m512/m32bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1."
"VPERMT2Q xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 7E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute quad-words from two tables in xmm3/m128/m64bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2Q ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 7E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute quad-words from two tables in ymm3/m256/m64bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2Q zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 7E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full","Permute quad-words from two tables in zmm3/m512/m64bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1."
"VPEXPANDD xmm1 {k1}{z}, xmm2/m128/i32x4","EVEX.128.66.0F38.W0 89 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed double-word integer values from xmm2/m128 to xmm1 using writemask k1."
"VPEXPANDD ymm1 {k1}{z}, ymm2/m256/i32x8","EVEX.256.66.0F38.W0 89 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed double-word integer values from ymm2/m256 to ymm1 using writemask k1."
"VPEXPANDD zmm1 {k1}{z}, zmm2/m512/i32x16","EVEX.512.66.0F38.W0 89 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed double-word integer values from zmm2/m512 to zmm1 using writemask k1."
"VPEXPANDQ xmm1 {k1}{z}, xmm2/m128/i64x2","EVEX.128.66.0F38.W1 89 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed quad-word integer values from xmm2/m128 to xmm1 using writemask k1."
"VPEXPANDQ ymm1 {k1}{z}, ymm2/m256/i64x4","EVEX.256.66.0F38.W1 89 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed quad-word integer values from ymm2/m256 to ymm1 using writemask k1."
"VPEXPANDQ zmm1 {k1}{z}, zmm2/m512/i64x8","EVEX.512.66.0F38.W1 89 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Expand packed quad-word integer values from zmm2/m512 to zmm1 using writemask k1."
"VPGATHERDD xmm1 {k1}, vm32x/i32x4","EVEX.128.66.0F38.W0 90 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERDD ymm1 {k1}, vm32y/i32x8","EVEX.256.66.0F38.W0 90 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERDD zmm1 {k1}, vm32z/i32x16","EVEX.512.66.0F38.W0 90 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERDQ xmm1 {k1}, vm32x/i64x2","EVEX.128.66.0F38.W1 90 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERDQ ymm1 {k1}, vm32x/i64x4","EVEX.256.66.0F38.W1 90 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERDQ zmm1 {k1}, vm32y/i64x8","EVEX.512.66.0F38.W1 90 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERQD xmm1 {k1}, vm64x/i32x2","EVEX.128.66.0F38.W0 91 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERQD xmm1 {k1}, vm64y/i32x4","EVEX.256.66.0F38.W0 91 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERQD ymm1 {k1}, vm64z/i32x8","EVEX.512.66.0F38.W0 91 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERQQ xmm1 {k1}, vm64x/i64x2","EVEX.128.66.0F38.W1 91 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERQQ ymm1 {k1}, vm64y/i64x4","EVEX.256.66.0F38.W1 91 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERQQ zmm1 {k1}, vm64z/i64x8","EVEX.512.66.0F38.W1 91 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","","","Tuple1 Scalar","Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPMAXSD xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 3D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed dword integers in xmm2 and xmm3/m128/m32bcst and store packed maximum values in xmm1 using writemask k1."
"VPMAXSD ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 3D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed dword integers in xmm2 and xmm3/m256/m32bcst and store packed maximum values in xmm1 using writemask k1."
"VPMAXSD zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 3D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed dword integers in xmm2 and xmm3/m512/m32bcst and store packed maximum values in xmm1 using writemask k1."
"VPMAXSQ xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 3D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed qword integers in xmm2 and xmm3/m128/m64bcst and store packed maximum values in xmm1 using writemask k1."
"VPMAXSQ ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 3D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed qword integers in xmm2 and xmm3/m256/m64bcst and store packed maximum values in xmm1 using writemask k1."
"VPMAXSQ zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 3D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed qword integers in xmm2 and xmm3/m512/m64bcst and store packed maximum values in xmm1 using writemask k1."
"VPMAXUD xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 3F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned dword integers in xmm2 and xmm3/m128/m32bcst and store packed maximum values in xmm1 under writemask k1."
"VPMAXUD ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 3F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned dword integers in ymm2 and ymm3/m256/m32bcst and store packed maximum values in ymm1 under writemask k1."
"VPMAXUD zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 3F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned dword integers in zmm2 and zmm3/m512/m32bcst and store packed maximum values in zmm1 under writemask k1."
"VPMAXUQ xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 3F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned qword integers in xmm2 and xmm3/m128/m64bcst and store packed maximum values in xmm1 under writemask k1."
"VPMAXUQ ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 3F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned qword integers in ymm2 and ymm3/m256/m64bcst and store packed maximum values in ymm1 under writemask k1."
"VPMAXUQ zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 3F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned qword integers in zmm2 and zmm3/m512/m64bcst and store packed maximum values in zmm1 under writemask k1."
"VPMINSD xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 39 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSD ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 39 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSD zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 39 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed dword integers in zmm2 and zmm3/m512/m32bcst and store packed minimum values in zmm1 under writemask k1."
"VPMINSQ xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 39 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed qword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSQ ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 39 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed qword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSQ zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 39 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed signed qword integers in zmm2 and zmm3/m512/m64bcst and store packed minimum values in zmm1 under writemask k1."
"VPMINUD xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 3B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned dword integers in xmm2 and xmm3/m128/m32bcst and store packed minimum values in xmm1 under writemask k1."
"VPMINUD ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 3B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned dword integers in ymm2 and ymm3/m256/m32bcst and store packed minimum values in ymm1 under writemask k1."
"VPMINUD zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 3B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned dword integers in zmm2 and zmm3/m512/m32bcst and store packed minimum values in zmm1 under writemask k1."
"VPMINUQ xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 3B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned qword integers in xmm2 and xmm3/m128/m64bcst and store packed minimum values in xmm1 under writemask k1."
"VPMINUQ ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 3B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned qword integers in ymm2 and ymm3/m256/m64bcst and store packed minimum values in ymm1 under writemask k1."
"VPMINUQ zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 3B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Compare packed unsigned qword integers in zmm2 and zmm3/m512/m64bcst and store packed minimum values in zmm1 under writemask k1."
"VPMOVDB xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 31 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 16 packed double-word integers from zmm2 into 16 packed byte integers in xmm1/m128 with truncation under writemask k1."
"VPMOVDB xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 31 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 4 packed double-word integers from xmm2 into 4 packed byte integers in xmm1/m32 with truncation under writemask k1."
"VPMOVDB xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 31 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 8 packed double-word integers from ymm2 into 8 packed byte integers in xmm1/m64 with truncation under writemask k1."
"VPMOVDW xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 33 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 8 packed double-word integers from ymm2 into 8 packed word integers in xmm1/m128 with truncation under writemask k1."
"VPMOVDW xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 33 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 4 packed double-word integers from xmm2 into 4 packed word integers in xmm1/m64 with truncation under writemask k1."
"VPMOVDW ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 33 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 16 packed double-word integers from zmm2 into 16 packed word integers in ymm1/m256 with truncation under writemask k1."
"VPMOVQB xmm1/m16 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 32 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Oct Vector Mem","Converts 2 packed quad-word integers from xmm2 into 2 packed byte integers in xmm1/m16 with truncation under writemask k1."
"VPMOVQB xmm1/m32 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 32 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Oct Vector Mem","Converts 4 packed quad-word integers from ymm2 into 4 packed byte integers in xmm1/m32 with truncation under writemask k1."
"VPMOVQB xmm1/m64 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 32 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Oct Vector Mem","Converts 8 packed quad-word integers from zmm2 into 8 packed byte integers in xmm1/m64 with truncation under writemask k1."
"VPMOVQD xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 35 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 2 packed quad-word integers from xmm2 into 2 packed double-word integers in xmm1/m128 with truncation subject to writemask k1."
"VPMOVQD xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 35 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 4 packed quad-word integers from ymm2 into 4 packed double-word integers in xmm1/m128 with truncation subject to writemask k1."
"VPMOVQD ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 35 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 8 packed quad-word integers from zmm2 into 8 packed double-word integers in ymm1/m256 with truncation subject to writemask k1."
"VPMOVQW xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 34 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 8 packed quad-word integers from zmm2 into 8 packed word integers in xmm1/m128 with truncation under writemask k1."
"VPMOVQW xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 34 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 2 packed quad-word integers from xmm2 into 2 packed word integers in xmm1/m32 with truncation under writemask k1."
"VPMOVQW xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 34 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 4 packed quad-word integers from ymm2 into 4 packed word integers in xmm1/m64 with truncation under writemask k1."
"VPMOVSDB xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 21 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 16 packed signed double-word integers from zmm2 into 16 packed signed byte integers in xmm1/m128 using signed saturation under writemask k1."
"VPMOVSDB xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 21 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 4 packed signed double-word integers from xmm2 into 4 packed signed byte integers in xmm1/m32 using signed saturation under writemask k1."
"VPMOVSDB xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 21 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 8 packed signed double-word integers from ymm2 into 8 packed signed byte integers in xmm1/m64 using signed saturation under writemask k1."
"VPMOVSDW xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 23 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 8 packed signed double-word integers from ymm2 into 8 packed signed word integers in xmm1/m128 using signed saturation under writemask k1."
"VPMOVSDW xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 23 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 4 packed signed double-word integers from xmm2 into 4 packed signed word integers in ymm1/m64 using signed saturation under writemask k1."
"VPMOVSDW ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 23 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 16 packed signed double-word integers from zmm2 into 16 packed signed word integers in ymm1/m256 using signed saturation under writemask k1."
"VPMOVSQB xmm1/m16 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 22 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Oct Vector Mem","Converts 2 packed signed quad-word integers from xmm2 into 2 packed signed byte integers in xmm1/m16 using signed saturation under writemask k1."
"VPMOVSQB xmm1/m32 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 22 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Oct Vector Mem","Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed byte integers in xmm1/m32 using signed saturation under writemask k1."
"VPMOVSQB xmm1/m64 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 22 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Oct Vector Mem","Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed byte integers in xmm1/m64 using signed saturation under writemask k1."
"VPMOVSQD xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed double-word integers in xmm1/m128 using signed saturation subject to writemask k1."
"VPMOVSQD xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 2 packed signed quad-word integers from xmm2 into 2 packed signed double-word integers in xmm1/m64 using signed saturation subject to writemask k1."
"VPMOVSQD ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed double-word integers in ymm1/m256 using signed saturation subject to writemask k1."
"VPMOVSQW xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 24 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed word integers in xmm1/m128 using signed saturation under writemask k1."
"VPMOVSQW xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 24 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed word integers in xmm1/m32 using signed saturation under writemask k1."
"VPMOVSQW xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 24 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed word integers in xmm1/m64 using signed saturation under writemask k1."
"VPMOVSXBD xmm1{k1}{z}, xmm2/m32","EVEX.128.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVSXBD ymm1{k1}{z}, xmm2/m64","EVEX.256.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1 subject to writemask k1."
"VPMOVSXBD zmm1{k1}{z}, xmm2/m128","EVEX.512.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Sign extend 16 packed 8-bit integers in the low 16 bytes of xmm2/m128 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVSXBQ xmm1{k1}{z}, xmm2/m16","EVEX.128.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Oct Vector Mem","Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVSXBQ ymm1{k1}{z}, xmm2/m32","EVEX.256.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Oct Vector Mem","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVSXBQ zmm1{k1}{z}, xmm2/m64","EVEX.512.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Oct Vector Mem","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMOVSXDQ xmm1{k1}{z}, xmm2/m64","EVEX.128.66.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVSXDQ ymm1{k1}{z}, xmm2/m128","EVEX.256.66.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Sign extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVSXDQ zmm1{k1}{z}, ymm2/m256","EVEX.512.66.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Sign extend 8 packed 32-bit integers in the low 32 bytes of ymm2/m256 to 8 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVSXWD xmm1{k1}{z}, xmm2/m64","EVEX.128.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Sign extend 4 packed 16-bit integers in the low 8 bytes of ymm2/mem to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVSXWD ymm1{k1}{z}, xmm2/m128","EVEX.256.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Sign extend 8 packed 16-bit integers in the low 16 bytes of ymm2/m128 to 8 packed 32-bit integers in ymm1 subject to writemask k1."
"VPMOVSXWD zmm1{k1}{z}, ymm2/m256","EVEX.512.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Sign extend 16 packed 16-bit integers in the low 32 bytes of ymm2/m256 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVSXWQ xmm1{k1}{z}, xmm2/m32","EVEX.128.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVSXWQ ymm1{k1}{z}, xmm2/m64","EVEX.256.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVSXWQ zmm1{k1}{z}, xmm2/m128","EVEX.512.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMOVUSDB xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 16 packed unsigned double-word integers from zmm2 into 16 packed unsigned byte integers in xmm1/m128 using unsigned saturation under writemask k1."
"VPMOVUSDB xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 4 packed unsigned double-word integers from xmm2 into 4 packed unsigned byte integers in xmm1/m32 using unsigned saturation under writemask k1."
"VPMOVUSDB xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 8 packed unsigned double-word integers from ymm2 into 8 packed unsigned byte integers in xmm1/m64 using unsigned saturation under writemask k1."
"VPMOVUSDW xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 8 packed unsigned double-word integers from ymm2 into 8 packed unsigned word integers in xmm1/m128 using unsigned saturation under writemask k1."
"VPMOVUSDW xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 4 packed unsigned double-word integers from xmm2 into 4 packed unsigned word integers in xmm1/m64 using unsigned saturation under writemask k1."
"VPMOVUSDW ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 16 packed unsigned double-word integers from zmm2 into 16 packed unsigned word integers in ymm1/m256 using unsigned saturation under writemask k1."
"VPMOVUSQB xmm1/m16 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Oct Vector Mem","Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned byte integers in xmm1/m16 using unsigned saturation under writemask k1."
"VPMOVUSQB xmm1/m32 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Oct Vector Mem","Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned byte integers in xmm1/m32 using unsigned saturation under writemask k1."
"VPMOVUSQB xmm1/m64 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Oct Vector Mem","Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned byte integers in xmm1/m64 using unsigned saturation under writemask k1."
"VPMOVUSQD xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned double-word integers in xmm1/m128 using unsigned saturation subject to writemask k1."
"VPMOVUSQD xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned double-word integers in xmm1/m64 using unsigned saturation subject to writemask k1."
"VPMOVUSQD ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned double-word integers in ymm1/m256 using unsigned saturation subject to writemask k1."
"VPMOVUSQW xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned word integers in xmm1/m128 using unsigned saturation under writemask k1."
"VPMOVUSQW xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned word integers in xmm1/m32 using unsigned saturation under writemask k1."
"VPMOVUSQW xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","","","Quarter Vector Mem","Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned word integers in xmm1/m64 using unsigned saturation under writemask k1."
"VPMOVZXBD xmm1{k1}{z}, xmm2/m32","EVEX.128.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVZXBD ymm1{k1}{z}, xmm2/m64","EVEX.256.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1 subject to writemask k1."
"VPMOVZXBD zmm1{k1}{z}, xmm2/m128","EVEX.512.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVZXBQ xmm1{k1}{z}, xmm2/m16","EVEX.128.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Oct Vector Mem","Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVZXBQ ymm1{k1}{z}, xmm2/m32","EVEX.256.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Oct Vector Mem","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVZXBQ zmm1{k1}{z}, xmm2/m64","EVEX.512.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Oct Vector Mem","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMOVZXWD xmm1{k1}{z}, xmm2/m64","EVEX.128.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVZXWD ymm1{k1}{z}, xmm2/m128","EVEX.256.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVZXWD zmm1{k1}{z}, ymm2/m256","EVEX.512.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Zero extend 16 packed 16-bit integers in ymm2/m256 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVZXWQ xmm1{k1}{z}, xmm2/m32","EVEX.128.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVZXWQ ymm1{k1}{z}, xmm2/m64","EVEX.256.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVZXWQ zmm1{k1}{z}, xmm2/m128","EVEX.512.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector Mem","Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMULDQ xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 using writemask k1."
"VPMULDQ ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 using writemask k1."
"VPMULDQ zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed signed doubleword integers in zmm2 by packed signed doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 using writemask k1."
"VPMULLD xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 40 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply the packed dword signed integers in xmm2 and xmm3/m128/m32bcst and store the low 32 bits of each product in xmm1 under writemask k1."
"VPMULLD ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 40 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply the packed dword signed integers in ymm2 and ymm3/m256/m32bcst and store the low 32 bits of each product in ymm1 under writemask k1."
"VPMULLD zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 40 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply the packed dword signed integers in zmm2 and zmm3/m512/m32bcst and store the low 32 bits of each product in zmm1 under writemask k1."
"VPMULUDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 F4 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 under writemask k1."
"VPMULUDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 F4 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 under writemask k1."
"VPMULUDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F.W1 F4 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed unsigned doubleword integers in zmm2 by packed unsigned doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 under writemask k1."
"VPORD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 EB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise OR of packed doubleword integers in xmm2 and xmm3/m128/m32bcst using writemask k1."
"VPORD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F.W0 EB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise OR of packed doubleword integers in ymm2 and ymm3/m256/m32bcst using writemask k1."
"VPORD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F.W0 EB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise OR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1."
"VPORQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 EB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise OR of packed quadword integers in xmm2 and xmm3/m128/m64bcst using writemask k1."
"VPORQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 EB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise OR of packed quadword integers in ymm2 and ymm3/m256/m64bcst using writemask k1."
"VPORQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F.W1 EB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise OR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1."
"VPROLD xmm1 {k1}{z}, xmm2/m128/m32bcst, ib","EVEX.128.66.0F.W0 72 /1 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate doublewords in xmm2/m128/m32bcst left by ib. Result written to xmm1 using writemask k1."
"VPROLD ymm1 {k1}{z}, ymm2/m256/m32bcst, ib","EVEX.256.66.0F.W0 72 /1 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate doublewords in ymm2/m256/m32bcst left by ib. Result written to ymm1 using writemask k1."
"VPROLD zmm1 {k1}{z}, zmm2/m512/m32bcst, ib","EVEX.512.66.0F.W0 72 /1 ib","Valid","Valid","Invalid","AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate left of doublewords in zmm3/m512/m32bcst by ib. Result written to zmm1 using writemask k1."
"VPROLQ xmm1 {k1}{z}, xmm2/m128/m64bcst, ib","EVEX.128.66.0F.W1 72 /1 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate quadwords in xmm2/m128/m64bcst left by ib. Result written to xmm1 using writemask k1."
"VPROLQ ymm1 {k1}{z}, ymm2/m256/m64bcst, ib","EVEX.256.66.0F.W1 72 /1 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate quadwords in ymm2/m256/m64bcst left by ib. Result written to ymm1 using writemask k1."
"VPROLQ zmm1 {k1}{z}, zmm2/m512/m64bcst, ib","EVEX.512.66.0F.W1 72 /1 ib","Valid","Valid","Invalid","AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate quadwords in zmm2/m512/m64bcst left by ib. Result written to zmm1 using writemask k1."
"VPROLVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate doublewords in xmm2 left by count in the corresponding element of xmm3/m128/m32bcst. Result written to xmm1 under writemask k1."
"VPROLVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate doublewords in ymm2 left by count in the corresponding element of ymm3/m256/m32bcst. Result written to ymm1 under writemask k1."
"VPROLVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate left of doublewords in zmm2 by count in the corresponding element of zmm3/m512/m32bcst. Result written to zmm1 using writemask k1."
"VPROLVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate quadwords in xmm2 left by count in the corresponding element of xmm3/m128/m64bcst. Result written to xmm1 under writemask k1."
"VPROLVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate quadwords in ymm2 left by count in the corresponding element of ymm3/m256/m64bcst. Result written to ymm1 under writemask k1."
"VPROLVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate quadwords in zmm2 left by count in the corresponding element of zmm3/m512/m64bcst. Result written to zmm1under writemask k1."
"VPRORD xmm1 {k1}{z}, xmm2/m128/m32bcst, ib","EVEX.128.66.0F.W0 72 /0 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate doublewords in xmm2/m128/m32bcst right by ib, store result using writemask k1."
"VPRORD ymm1 {k1}{z}, ymm2/m256/m32bcst, ib","EVEX.256.66.0F.W0 72 /0 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate doublewords in ymm2/m256/m32bcst right by ib, store result using writemask k1."
"VPRORD zmm1 {k1}{z}, zmm2/m512/m32bcst, ib","EVEX.512.66.0F.W0 72 /0 ib","Valid","Valid","Invalid","AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate doublewords in zmm2/m512/m32bcst right by ib, store result using writemask k1."
"VPRORQ xmm1 {k1}{z}, xmm2/m128/m64bcst, ib","EVEX.128.66.0F.W1 72 /0 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate quadwords in xmm2/m128/m64bcst right by ib, store result using writemask k1."
"VPRORQ ymm1 {k1}{z}, ymm2/m256/m64bcst, ib","EVEX.256.66.0F.W1 72 /0 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate quadwords in ymm2/m256/m64bcst right by ib, store result using writemask k1."
"VPRORQ zmm1 {k1}{z}, zmm2/m512/m64bcst, ib","EVEX.512.66.0F.W1 72 /0 ib","Valid","Valid","Invalid","AVX512F","VEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Rotate quadwords in zmm2/m512/m64bcst right by ib, store result using writemask k1."
"VPRORVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate doublewords in xmm2 right by count in the corresponding element of xmm3/m128/m32bcst, store result using writemask k1."
"VPRORVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate doublewords in ymm2 right by count in the corresponding element of ymm3/m256/m32bcst, store using result writemask k1."
"VPRORVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate doublewords in zmm2 right by count in the corresponding element of zmm3/m512/m32bcst, store result using writemask k1."
"VPRORVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate quadwords in xmm2 right by count in the corresponding element of xmm3/m128/m64bcst, store result using writemask k1."
"VPRORVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate quadwords in ymm2 right by count in the corresponding element of ymm3/m256/m64bcst, store result using writemask k1."
"VPRORVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Rotate quadwords in zmm2 right by count in the corresponding element of zmm3/m512/m64bcst, store result using writemask k1."
"VPSCATTERDD vm32x/i32x4 {k1}, xmm1","EVEX.128.66.0F38.W0 A0 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter dword values to memory using writemask k1."
"VPSCATTERDD vm32y/i32x8 {k1}, ymm1","EVEX.256.66.0F38.W0 A0 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter dword values to memory using writemask k1."
"VPSCATTERDD vm32z/i32x16 {k1}, zmm1","EVEX.512.66.0F38.W0 A0 /r","Valid","Valid","Invalid","AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter dword values to memory using writemask k1."
"VPSCATTERDQ vm32x/i64x2 {k1}, xmm1","EVEX.128.66.0F38.W1 A0 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter qword values to memory using writemask k1."
"VPSCATTERDQ vm32x/i64x4 {k1}, ymm1","EVEX.256.66.0F38.W1 A0 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter qword values to memory using writemask k1."
"VPSCATTERDQ vm32y/i64x8 {k1}, zmm1","EVEX.512.66.0F38.W1 A0 /r","Valid","Valid","Invalid","AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter qword values to memory using writemask k1."
"VPSCATTERQD vm64x/i32x2 {k1}, xmm1","EVEX.128.66.0F38.W0 A1 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter dword values to memory using writemask k1."
"VPSCATTERQD vm64y/i32x4 {k1}, xmm1","EVEX.256.66.0F38.W0 A1 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter dword values to memory using writemask k1."
"VPSCATTERQD vm64z/i32x8 {k1}, ymm1","EVEX.512.66.0F38.W0 A1 /r","Valid","Valid","Invalid","AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter dword values to memory using writemask k1."
"VPSCATTERQQ vm64x/i64x2 {k1}, xmm1","EVEX.128.66.0F38.W1 A1 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter qword values to memory using writemask k1."
"VPSCATTERQQ vm64y/i64x4 {k1}, ymm1","EVEX.256.66.0F38.W1 A1 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter qword values to memory using writemask k1."
"VPSCATTERQQ vm64z/i64x8 {k1}, zmm1","EVEX.512.66.0F38.W1 A1 /r","Valid","Valid","Invalid","AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter qword values to memory using writemask k1."
"VPSHUFD xmm1 {k1}{z}, xmm2/m128/m32bcst, ib","EVEX.128.66.0F.W0 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Shuffle the doublewords in xmm2/m128/m32bcst based on the encoding in ib and store the result in xmm1 using writemask k1."
"VPSHUFD ymm1 {k1}{z}, ymm2/m256/m32bcst, ib","EVEX.256.66.0F.W0 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Shuffle the doublewords in ymm2/m256/m32bcst based on the encoding in ib and store the result in ymm1 using writemask k1."
"VPSHUFD zmm1 {k1}{z}, zmm2/m512/m32bcst, ib","EVEX.512.66.0F.W0 70 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Shuffle the doublewords in zmm2/m512/m32bcst based on the encoding in ib and store the result in zmm1 using writemask k1."
"VPSLLD xmm1 {k1}{z}, xmm2/m128/m32bcst, ib","EVEX.128.66.0F.W0 72 /6 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift doublewords in xmm2/m128/m32bcst left by ib while shifting in 0s using writemask k1."
"VPSLLD ymm1 {k1}{z}, ymm2/m256/m32bcst, ib","EVEX.256.66.0F.W0 72 /6 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift doublewords in ymm2/m256/m32bcst left by ib while shifting in 0s using writemask k1."
"VPSLLD zmm1 {k1}{z}, zmm2/m512/m32bcst, ib","EVEX.512.66.0F.W0 72 /6 ib","Valid","Valid","Invalid","AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift doublewords in zmm2/m512/m32bcst left by ib while shifting in 0s using writemask k1."
"VPSLLD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.W0 F2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s under writemask k1."
"VPSLLD ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.256.66.0F.W0 F2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift doublewords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s under writemask k1."
"VPSLLD zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.512.66.0F.W0 F2 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift doublewords in zmm2 left by amount specified in xmm3/m128 while shifting in 0s under writemask k1."
"VPSLLQ xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.W1 F3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLQ ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.256.66.0F.W1 F3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift quadwords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLQ zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.512.66.0F.W1 F3 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift quadwords in zmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in 0s using writemask k1."
"VPSLLVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in 0s using writemask k1."
"VPSLLVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift doublewords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in 0s using writemask k1."
"VPSLLVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in 0s using writemask k1."
"VPSLLVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in 0s using writemask k1."
"VPSLLVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift quadwords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in 0s using writemask k1."
"VPSRAD xmm1 {k1}{z}, xmm2/m128/m32bcst, ib","EVEX.128.66.0F.W0 72 /4 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift doublewords in xmm2/m128/m32bcst right by ib while shifting in sign bits using writemask k1."
"VPSRAD ymm1 {k1}{z}, ymm2/m256/m32bcst, ib","EVEX.256.66.0F.W0 72 /4 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift doublewords in ymm2/m256/m32bcst right by ib while shifting in sign bits using writemask k1."
"VPSRAD zmm1 {k1}{z}, zmm2/m512/m32bcst, ib","EVEX.512.66.0F.W0 72 /4 ib","Valid","Valid","Invalid","AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift doublewords in zmm2/m512/m32bcst right by ib while shifting in sign bits using writemask k1."
"VPSRAD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.W0 E2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAD ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.256.66.0F.W0 E2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAD zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.512.66.0F.W0 E2 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift doublewords in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAQ xmm1 {k1}{z}, xmm2/m128/m64bcst, ib","EVEX.128.66.0F.W1 72 /4 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift quadwords in xmm2/m128/m64bcst right by ib while shifting in sign bits using writemask k1."
"VPSRAQ ymm1 {k1}{z}, ymm2/m256/m64bcst, ib","EVEX.256.66.0F.W1 72 /4 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift quadwords in ymm2/m256/m64bcst right by ib while shifting in sign bits using writemask k1."
"VPSRAQ zmm1 {k1}{z}, zmm2/m512/m64bcst, ib","EVEX.512.66.0F.W1 72 /4 ib","Valid","Valid","Invalid","AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift quadwords in zmm2/m512/m64bcst right by ib while shifting in sign bits using writemask k1."
"VPSRAQ xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.W1 E2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAQ ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.256.66.0F.W1 E2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAQ zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.512.66.0F.W1 E2 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift quadwords in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in sign bits using writemask k1."
"VPSRAVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in sign bits using writemask k1."
"VPSRAVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift doublewords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in sign bits using writemask k1."
"VPSRAVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 46 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in sign bits using writemask k1."
"VPSRAVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 46 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in sign bits using writemask k1."
"VPSRAVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 46 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift quadwords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in sign bits using writemask k1."
"VPSRLD xmm1 {k1}{z}, xmm2/m128/m32bcst, ib","EVEX.128.66.0F.W0 72 /2 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift doublewords in xmm2/m128/m32bcst right by ib while shifting in 0s using writemask k1."
"VPSRLD ymm1 {k1}{z}, ymm2/m256/m32bcst, ib","EVEX.256.66.0F.W0 72 /2 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift doublewords in ymm2/m256/m32bcst right by ib while shifting in 0s using writemask k1."
"VPSRLD zmm1 {k1}{z}, zmm2/m512/m32bcst, ib","EVEX.512.66.0F.W0 72 /2 ib","Valid","Valid","Invalid","AVX512F","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector","Shift doublewords in zmm2/m512/m32bcst right by ib while shifting in 0s using writemask k1."
"VPSRLD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.W0 D2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLD ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.256.66.0F.W0 D2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLD zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.512.66.0F.W0 D2 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift doublewords in zmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLQ xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.W1 D3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLQ ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.256.66.0F.W1 D3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLQ zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.512.66.0F.W1 D3 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift quadwords in zmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in 0s using writemask k1."
"VPSRLVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in 0s using writemask k1."
"VPSRLVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift doublewords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in 0s using writemask k1."
"VPSRLVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in 0s using writemask k1."
"VPSRLVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in 0s using writemask k1."
"VPSRLVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Shift quadwords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in 0s using writemask k1."
"VPSUBQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 FB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed quadword integers in xmm3/m128/m64bcst from xmm2 and store in xmm1 using writemask k1."
"VPSUBQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 FB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed quadword integers in ymm3/m256/m64bcst from ymm2 and store in ymm1 using writemask k1."
"VPSUBQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F.W1 FB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed quadword integers in zmm3/m512/m64bcst from zmm2 and store in zmm1 using writemask k1."
"VPTERNLOGD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst, ib","EVEX.128.66.0F3A.W0 25 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Bitwise ternary logic taking xmm1, xmm2 and xmm3/m128/m32bcst as source operands and writing the result to xmm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 25 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Bitwise ternary logic taking ymm1, ymm2 and ymm3/m256/m32bcst as source operands and writing the result to ymm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst, ib","EVEX.512.66.0F3A.W0 25 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Bitwise ternary logic taking zmm1, zmm2 and zmm3/m512/m32bcst as source operands and writing the result to zmm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst, ib","EVEX.128.66.0F3A.W1 25 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Bitwise ternary logic taking xmm1, xmm2 and xmm3/m128/m64bcst as source operands and writing the result to xmm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 25 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Bitwise ternary logic taking ymm1, ymm2 and ymm3/m256/m64bcst as source operands and writing the result to ymm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst, ib","EVEX.512.66.0F3A.W1 25 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Bitwise ternary logic taking zmm1, zmm2 and zmm3/m512/m64bcst as source operands and writing the result to zmm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented."
"VPTESTMD k2 {k1}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMD k2 {k1}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMD k2 {k1}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 27 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMQ k2 {k1}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMQ k2 {k1}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMQ k2 {k1}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F38.W1 27 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise AND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTNMD k2 {k1}, xmm2, xmm3/m128/m32bcst","EVEX.128.F3.0F38.W0 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise NAND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMD k2 {k1}, ymm2, ymm3/m256/m32bcst","EVEX.256.F3.0F38.W0 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise NAND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMD k2 {k1}, zmm2, zmm3/m512/m32bcst","EVEX.512.F3.0F38.W0 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise NAND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMQ k2 {k1}, xmm2, xmm3/m128/m64bcst","EVEX.128.F3.0F38.W1 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise NAND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMQ k2 {k1}, ymm2, ymm3/m256/m64bcst","EVEX.256.F3.0F38.W1 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise NAND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMQ k2 {k1}, zmm2, zmm3/m512/m64bcst","EVEX.512.F3.0F38.W1 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise NAND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPUNPCKHDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 6A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Interleave high-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register using k1 write mask."
"VPUNPCKHQDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 6D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Interleave high-order quadword from xmm2 and xmm3/m128/m64bcst into xmm1 register using k1 write mask."
"VPUNPCKLDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 62 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Interleave low-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register subject to write mask k1."
"VPUNPCKLQDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 6C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Interleave low-order quadword from zmm2 and zmm3/m512/m64bcst into zmm1 register subject to write mask k1."
"VPXORD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 EF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise XOR of packed doubleword integers in xmm2 and xmm3/m128 using writemask k1."
"VPXORD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F.W0 EF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise XOR of packed doubleword integers in ymm2 and ymm3/m256 using writemask k1."
"VPXORD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F.W0 EF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise XOR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1."
"VPXORQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 EF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise XOR of packed quadword integers in xmm2 and xmm3/m128 using writemask k1."
"VPXORQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 EF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise XOR of packed quadword integers in ymm2 and ymm3/m256 using writemask k1."
"VPXORQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F.W1 EF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Bitwise XOR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1."
"VRCP14PD xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 4C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocals of the packed double-precision floating-point values in xmm2/m128/m64bcst and stores the results in xmm1. Under writemask."
"VRCP14PD ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F38.W1 4C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocals of the packed double-precision floating-point values in ymm2/m256/m64bcst and stores the results in ymm1. Under writemask."
"VRCP14PD zmm1 {k1}{z}, zmm2/m512/m64bcst","EVEX.512.66.0F38.W1 4C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocals of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the results in zmm1. Under writemask."
"VRCP14PS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F38.W0 4C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocals of the packed single-precision floating-point values in xmm2/m128/m32bcst and stores the results in xmm1. Under writemask."
"VRCP14PS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F38.W0 4C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocals of the packed single-precision floating-point values in ymm2/m256/m32bcst and stores the results in ymm1. Under writemask."
"VRCP14PS zmm1 {k1}{z}, zmm2/m512/m32bcst","EVEX.512.66.0F38.W0 4C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocals of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask."
"VRCP14SD xmm1 {k1}{z}, xmm2, xmm3/m64","EVEX.LIG.66.0F38.W1 4D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Computes the approximate reciprocal of the scalar double-precision floating-point value in xmm3/m64 and stores the result in xmm1 using writemask k1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VRCP14SS xmm1 {k1}{z}, xmm2, xmm3/m32","EVEX.LIG.66.0F38.W0 4D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm3/m32 and stores the results in xmm1 using writemask k1. Also, upper double-precision floating-point value (bits[127:32]) from xmm2 is copied to xmm1[127:32]."
"VRNDSCALEPD xmm1 {k1}{z}, xmm2/m128/m64bcst, ib","EVEX.128.66.0F3A.W1 09 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Rounds packed double-precision floating point values in xmm2/m128/m64bcst to a number of fraction bits specified by the ib field. Stores the result in xmm1 register. Under writemask."
"VRNDSCALEPD ymm1 {k1}{z}, ymm2/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 09 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Rounds packed double-precision floating point values in ymm2/m256/m64bcst to a number of fraction bits specified by the ib field. Stores the result in ymm1 register. Under writemask."
"VRNDSCALEPD zmm1 {k1}{z}, zmm2/m512/m64bcst{sae}, ib","EVEX.512.66.0F3A.W1 09 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Rounds packed double-precision floating-point values in zmm2/m512/m64bcst to a number of fraction bits specified by the ib field. Stores the result in zmm1 register using writemask k1."
"VRNDSCALEPS xmm1 {k1}{z}, xmm2/m128/m32bcst, ib","EVEX.128.66.0F3A.W0 08 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Rounds packed single-precision floating point values in xmm2/m128/m32bcst to a number of fraction bits specified by the ib field. Stores the result in xmm1 register. Under writemask."
"VRNDSCALEPS ymm1 {k1}{z}, ymm2/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 08 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Rounds packed single-precision floating point values in ymm2/m256/m32bcst to a number of fraction bits specified by the ib field. Stores the result in ymm1 register. Under writemask."
"VRNDSCALEPS zmm1 {k1}{z}, zmm2/m512/m32bcst{sae}, ib","EVEX.512.66.0F3A.W0 08 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Rounds packed single-precision floating-point values in zmm2/m512/m32bcst to a number of fraction bits specified by the ib field. Stores the result in zmm1 register using writemask."
"VRNDSCALESD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}, ib","EVEX.LIG.66.0F3A.W1 0B /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Rounds scalar double-precision floating-point value in xmm3/m64 to a number of fraction bits specified by the ib field. Stores the result in xmm1 register."
"VRNDSCALESS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}, ib","EVEX.LIG.66.0F3A.W0 0A /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Rounds scalar single-precision floating-point value in xmm3/m32 to a number of fraction bits specified by the ib field. Stores the result in xmm1 register under writemask."
"VRSQRT14PD xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 4E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocal square roots of the packed double-precision floating-point values in xmm2/m128/m64bcst and stores the results in xmm1. Under writemask."
"VRSQRT14PD ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F38.W1 4E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocal square roots of the packed double-precision floating-point values in ymm2/m256/m64bcst and stores the results in ymm1. Under writemask."
"VRSQRT14PD zmm1 {k1}{z}, zmm2/m512/m64bcst","EVEX.512.66.0F38.W1 4E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocal square roots of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the results in zmm1 under writemask."
"VRSQRT14PS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F38.W0 4E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocal square roots of the packed single-precision floating-point values in xmm2/m128/m32bcst and stores the results in xmm1. Under writemask."
"VRSQRT14PS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F38.W0 4E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocal square roots of the packed single-precision floating-point values in ymm2/m256/m32bcst and stores the results in ymm1. Under writemask."
"VRSQRT14PS zmm1 {k1}{z}, zmm2/m512/m32bcst","EVEX.512.66.0F38.W0 4E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes the approximate reciprocal square roots of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask."
"VRSQRT14SD xmm1 {k1}{z}, xmm2, xmm3/m64","EVEX.LIG.66.0F38.W1 4F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Computes the approximate reciprocal square root of the scalar double-precision floating-point value in xmm3/m64 and stores the result in the low quadword element of xmm1 using writemask k1. Bits[127:64] of xmm2 is copied to xmm1[127:64]."
"VRSQRT14SS xmm1 {k1}{z}, xmm2, xmm3/m32","EVEX.LIG.66.0F38.W0 4F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Computes the approximate reciprocal square root of the scalar single-precision floating-point value in xmm3/m32 and stores the result in the low doubleword element of xmm1 using writemask k1. Bits[127:32] of xmm2 is copied to xmm1[127:32]."
"VSCALEFPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F38.W1 2C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Scale the packed double-precision floating-point values in xmm2 using values from xmm3/m128/m64bcst. Under writemask k1."
"VSCALEFPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F38.W1 2C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Scale the packed double-precision floating-point values in ymm2 using values from ymm3/m256/m64bcst. Under writemask k1."
"VSCALEFPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F38.W1 2C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Scale the packed double-precision floating-point values in zmm2 using values from zmm3/m512/m64bcst. Under writemask k1."
"VSCALEFPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Scale the packed single-precision floating-point values in xmm2 using values from xmm3/m128/m32bcst. Under writemask k1."
"VSCALEFPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Scale the packed single-precision values in ymm2 using floating point values from ymm3/m256/m32bcst. Under writemask k1."
"VSCALEFPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Scale the packed single-precision floating-point values in zmm2 using floating-point values from zmm3/m512/m32bcst. Under writemask k1."
"VSCALEFSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.66.0F38.W1 2D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Scale the scalar double-precision floating-point values in xmm2 using the value from xmm3/m64. Under writemask k1."
"VSCALEFSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.66.0F38.W0 2D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Scale the scalar single-precision floating-point value in xmm2 using floating-point value from xmm3/m32. Under writemask k1."
"VSCATTERDPD vm32x/f64x2 {k1}, xmm1","EVEX.128.66.0F38.W1 A2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERDPD vm32x/f64x4 {k1}, ymm1","EVEX.256.66.0F38.W1 A2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERDPD vm32y/f64x8 {k1}, zmm1","EVEX.512.66.0F38.W1 A2 /r","Valid","Valid","Invalid","AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERDPS vm32x/f32x4 {k1}, xmm1","EVEX.128.66.0F38.W0 A2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERDPS vm32y/f32x8 {k1}, ymm1","EVEX.256.66.0F38.W0 A2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERDPS vm32z/f32x16 {k1}, zmm1","EVEX.512.66.0F38.W0 A2 /r","Valid","Valid","Invalid","AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed dword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERQPD vm64x/f64x2 {k1}, xmm1","EVEX.128.66.0F38.W1 A3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERQPD vm64y/f64x4 {k1}, ymm1","EVEX.256.66.0F38.W1 A3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERQPD vm64z/f64x8 {k1}, zmm1","EVEX.512.66.0F38.W1 A3 /r","Valid","Valid","Invalid","AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERQPS vm64x/f32x2 {k1}, xmm1","EVEX.128.66.0F38.W0 A3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERQPS vm64y/f32x4 {k1}, xmm1","EVEX.256.66.0F38.W0 A3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERQPS vm64z/f32x8 {k1}, ymm1","EVEX.512.66.0F38.W0 A3 /r","Valid","Valid","Invalid","AVX512F","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","ModRM:reg (r)","","","","Using signed qword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSHUFF32X4 ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 23 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle 128-bit packed single-precision floating-point values selected by imm8 from ymm2 and ymm3/m256/m32bcst and place results in ymm1 subject to writemask k1."
"VSHUFF32X4 zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst, ib","EVEX.512.66.0F3A.W0 23 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle 128-bit packed single-precision floating-point values selected by ib from zmm2 and zmm3/m512/m32bcst and place results in zmm1 subject to writemask k1"
"VSHUFF64X2 ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 23 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle 128-bit packed double precision floating-point values selected by imm8 from ymm2 and ymm3/m256/m64bcst and place results in ymm1 subject to writemask k1."
"VSHUFF64X2 zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst, ib","EVEX.512.66.0F3A.W1 23 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle 128-bit packed double-precision floating-point values selected by ib from zmm2 and zmm3/m512/m64bcst and place results in zmm1 subject to writemask k1."
"VSHUFI32X4 ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst, ib","EVEX.256.66.0F3A.W0 43 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle 128-bit packed double-word values selected by imm8 from ymm2 and ymm3/m256/m32bcst and place results in ymm1 subject to writemask k1."
"VSHUFI32X4 zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst, ib","EVEX.512.66.0F3A.W0 43 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle 128-bit packed double-word values selected by ib from zmm2 and zmm3/m512/m32bcst and place results in zmm1 subject to writemask k1."
"VSHUFI64X2 ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst, ib","EVEX.256.66.0F3A.W1 43 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle 128-bit packed quad-word values selected by imm8 from ymm2 and ymm3/m256/m64bcst and place results in ymm1 subject to writemask k1."
"VSHUFI64X2 zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst, ib","EVEX.512.66.0F3A.W1 43 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle 128-bit packed quad-word values selected by ib from zmm2 and zmm3/m512/m64bcst and place results in zmm1 subject to writemask k1."
"VSHUFPD xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst, ib","EVEX.128.66.0F.W1 C6 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle two paris of double-precision floating-point values from xmm2 and xmm3/m128/m64bcst using ib to select from each pair. store interleaved results in xmm1 subject to writemask k1."
"VSHUFPD ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst, ib","EVEX.256.66.0F.W1 C6 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle four paris of double-precision floating-point values from ymm2 and ymm3/m256/m64bcst using ib to select from each pair. store interleaved results in ymm1 subject to writemask k1."
"VSHUFPD zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst, ib","EVEX.512.66.0F.W1 C6 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Shuffle eight paris of double-precision floating-point values from zmm2 and zmm3/m512/m64bcst using ib to select from each pair. store interleaved results in zmm1 subject to writemask k1."
"VSHUFPS xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst, ib","EVEX.128.0F.W0 C6 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Select from quadruplet of single-precision floating-point values in xmm1 and xmm2/m128 using ib, interleaved result pairs are stored in xmm1, subject to writemask k1."
"VSHUFPS ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst, ib","EVEX.256.0F.W0 C6 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Select from quadruplet of single-precision floating-point values in ymm2 and ymm3/m256 using ib, interleaved result pairs are stored in ymm1, subject to writemask k1."
"VSHUFPS zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst, ib","EVEX.512.0F.W0 C6 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Select from quadruplet of single-precision floating-point values in zmm2 and zmm3/m512 using ib, interleaved result pairs are stored in zmm1, subject to writemask k1."
"VSQRTPD xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F.W1 51 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes Square Roots of the packed double-precision floating-point values in xmm2/m128/m64bcst and stores the result in xmm1 subject to writemask k1."
"VSQRTPD ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F.W1 51 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes Square Roots of the packed double-precision floating-point values in ymm2/m256/m64bcst and stores the result in ymm1 subject to writemask k1."
"VSQRTPD zmm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.66.0F.W1 51 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes Square Roots of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the result in zmm1 subject to writemask k1."
"VSQRTPS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.0F.W0 51 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes Square Roots of the packed single-precision floating-point values in xmm2/m128/m32bcst and stores the result in xmm1 subject to writemask k1."
"VSQRTPS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.0F.W0 51 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes Square Roots of the packed single-precision floating-point values in ymm2/m256/m32bcst and stores the result in ymm1 subject to writemask k1."
"VSQRTPS zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.0F.W0 51 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Computes Square Roots of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the result in zmm1 subject to writemask k1."
"VSQRTSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.F2.0F.W1 51 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Computes square root of the low double-precision floating-point value in xmm3/m64 and stores the results in xmm1 under writemask k1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VSQRTSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.F3.0F.W0 51 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Computes square root of the low single-precision floating-point value in xmm3/m32 and stores the results in xmm1 under writemask k1. Also, upper single-precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VSUBPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 5C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed double-precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1."
"VSUBPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 5C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed double-precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1."
"VSUBPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.512.66.0F.W1 5C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed double-precision floating-point values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1."
"VSUBPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.0F.W0 5C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed single-precision floating-point values from xmm3/m128/m32bcst to xmm2 and stores result in xmm1 with writemask k1."
"VSUBPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.0F.W0 5C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed single-precision floating-point values from ymm3/m256/m32bcst to ymm2 and stores result in ymm1 with writemask k1."
"VSUBPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.512.0F.W0 5C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed single-precision floating-point values in zmm3/m512/m32bcst from zmm2 and stores result in zmm1 with writemask k1."
"VSUBSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.LIG.F2.0F.W1 5C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Subtract the low double-precision floating-point value in xmm3/m64 from xmm2 and store the result in xmm1 under writemask k1."
"VSUBSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.LIG.F3.0F.W0 5C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Tuple1 Scalar","Subtract the low single-precision floating-point value in xmm3/m32 from xmm2 and store the result in xmm1 under writemask k1."
"VUCOMISD xmm1, xmm2/m64{sae}","EVEX.LIG.66.0F.W1 2E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Compare low double-precision floating-point values in xmm1 and xmm2/m64 and set the EFLAGS flags accordingly."
"VUCOMISS xmm1, xmm2/m32{sae}","EVEX.LIG.0F.W0 2E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VUNPCKHPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves double precision floating-point values from high quadwords of xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VUNPCKHPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves double precision floating-point values from high quadwords of ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VUNPCKHPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F.W1 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves double-precision floating-point values from high quadwords of zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"VUNPCKHPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.0F.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm2 and xmm3/m128/m32bcst and write result to xmm1 subject to writemask k1."
"VUNPCKHPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.0F.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves single-precision floating-point values from high quadwords of ymm2 and ymm3/m256/m32bcst and write result to ymm1 subject to writemask k1."
"VUNPCKHPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.0F.W0 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves single-precision floating-point values from high quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to writemask k1."
"VUNPCKLPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.128.66.0F.W1 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves double precision floating-point values from low quadwords of xmm2 and xmm3/m128/m64bcst subject to write mask k1."
"VUNPCKLPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.256.66.0F.W1 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves double precision floating-point values from low quadwords of ymm2 and ymm3/m256/m64bcst subject to write mask k1."
"VUNPCKLPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.512.66.0F.W1 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves double-precision floating-point values from low quadwords of zmm2 and zmm3/m512/m64bcst subject to write mask k1."
"VUNPCKLPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.0F.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm2 and xmm3/mem and write result to xmm1 subject to write mask k1."
"VUNPCKLPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.0F.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves single-precision floating-point values from low quadwords of ymm2 and ymm3/mem and write result to ymm1 subject to write mask k1."
"VUNPCKLPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.0F.W0 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Unpacks and Interleaves single-precision floating-point values from low quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to write mask k1."
