"Instruction","Opcode","Valid 64-bit","Valid 32-bit","Valid 16-bit","Feature Flags","Operand 1","Operand 2","Operand 3","Operand 4","Tuple Type","Description"
"VADDPH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.map5.W0 58 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed FP16 value from xmm3/m128/m16bcst to xmm2, and store result in xmm1 subject to writemask k1."
"VADDPH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.map5.W0 58 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed FP16 value from ymm3/m256/m16bcst to ymm2, and store result in ymm1 subject to writemask k1."
"VADDPH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.map5.W0 58 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Add packed FP16 value from zmm3/m512/m16bcst to zmm2, and store result in zmm1 subject to writemask k1."
"VADDSH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.f3.map5.W0 58 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Add the low FP16 value from xmm3/m16 to xmm2, and store the result in xmm1 subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VCMPPH k{k},xmm,xmm/m128/m16bcst,ib","EVEX.128.0F3A.W0 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed FP16 values in xmm3/m128/m16bcst and xmm2 using bits 4:0 of imm8 as a comparison predicate subject to writemask k2, and store the result in mask register k1."
"VCMPPH k{k},ymm,ymm/m256/m16bcst,ib","EVEX.256.0F3A.W0 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed FP16 values in ymm3/m256/m16bcst and ymm2 using bits 4:0 of imm8 as a comparison predicate subject to writemask k2, and store the result in mask register k1."
"VCMPPH k{k},zmm,zmm/m512/m16bcst{sae},ib","EVEX.512.0F3A.W0 C2 /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector","Compare packed FP16 values in zmm3/m512/m16bcst and zmm2 using bits 4:0 of imm8 as a comparison predicate subject to writemask k2, and store the result in mask register k1."
"VCMPSH k{k},xmm,xmm/m16{sae},ib","EVEX.LIG.f3.0F3A.W0 C2 /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","Scalar","Compare low FP16 values in xmm3/m16 and xmm2 using bits 4:0 of imm8 as a comparison predicate subject to writemask k2, and store the result in mask register k1."
"VCOMISH xmm,xmm/m16{sae}","EVEX.LIG.map5.W0 2F /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r)","ModRM:r/m (r)","","","Scalar","Compare low FP16 values in xmm1 and xmm2/m16, and set the EFLAGS flags accordingly."
"VCVTDQ2PH xmm{k}{z},xmm/m128/m32bcst","EVEX.128.map5.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed signed doubleword integers from xmm2/m128/m32bcst to four packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTDQ2PH xmm{k}{z},ymm/m256/m32bcst","EVEX.256.map5.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTDQ2PH ymm{k}{z},zmm/m512/m32bcst{er}","EVEX.512.map5.W0 5B /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed signed doubleword integers from zmm2/m512/m32bcst to sixteen packed FP16 values, and store the result in ymm1 subject to writemask k1."
"VCVTPD2PH xmm{k}{z},xmm/m128/m64bcst","EVEX.128.66.map5.W1 5A /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert two packed double precision floating-point values in xmm2/m128/m64bcst to two packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTPD2PH xmm{k}{z},ymm/m256/m64bcst","EVEX.256.66.map5.W1 5A /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed double precision floating-point values in ymm2/m256/m64bcst to four packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTPD2PH xmm{k}{z},zmm/m512/m64bcst{er}","EVEX.512.66.map5.W1 5A /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed double precision floating-point values in zmm2/m512/m64bcst to eight packed FP16 values, and store the result in ymm1 subject to writemask k1."
"VCVTPH2DQ xmm{k}{z},xmm/m64/m16bcst","EVEX.128.66.map5.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert four packed FP16 values in xmm2/m64/m16bcst to four signed doubleword integers, and store the result in xmm1 subject to writemask k1."
"VCVTPH2DQ ymm{k}{z},xmm/m128/m16bcst","EVEX.256.66.map5.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight signed doubleword integers, and store the result in ymm1 subject to writemask k1."
"VCVTPH2DQ zmm{k}{z},ymm/m256/m16bcst{er}","EVEX.512.66.map5.W0 5B /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert sixteen packed FP16 values in ymm2/m256/m16bcst to sixteen signed doubleword integers, and store the result in zmm1 subject to writemask k1."
"VCVTPH2PD xmm{k}{z},xmm/m32/m16bcst","EVEX.128.map5.W0 5A /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert packed FP16 values in xmm2/m32/m16bcst to FP64 values, and store result in xmm1 subject to writemask k1."
"VCVTPH2PD ymm{k}{z},xmm/m64/m16bcst","EVEX.256.map5.W0 5A /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert packed FP16 values in xmm2/m64/m16bcst to FP64 values, and store result in ymm1 subject to writemask k1."
"VCVTPH2PD zmm{k}{z},xmm/m128/m16bcst{sae}","EVEX.512.map5.W0 5A /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert packed FP16 values in xmm2/m128/m16bcst to FP64 values, and store result in zmm1 subject to writemask k1."
"VCVTPH2PSX xmm{k}{z},xmm/m64/m16bcst","EVEX.128.66.map6.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert four packed FP16 values in xmm2/m64/m16bcst to four packed single precision floating-point values, and store result in xmm1 subject to writemask k1."
"VCVTPH2PSX ymm{k}{z},xmm/m128/m16bcst","EVEX.256.66.map6.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight packed single precision floating-point values, and store result in ymm1 subject to writemask k1."
"VCVTPH2PSX zmm{k}{z},ymm/m256/m16bcst{sae}","EVEX.512.66.map6.W0 13 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert sixteen packed FP16 values in ymm2/m256/m16bcst to sixteen packed single precision floating-point values, and store result in zmm1 subject to writemask k1."
"VCVTPH2QQ xmm{k}{z},xmm/m32/m16bcst","EVEX.128.66.map5.W0 7b /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert two packed FP16 values in xmm2/m32/m16bcst to two signed quadword integers, and store the result in xmm1 subject to writemask k1."
"VCVTPH2QQ ymm{k}{z},xmm/m64/m16bcst","EVEX.256.66.map5.W0 7b /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert four packed FP16 values in xmm2/m64/m16bcst to four signed quadword integers, and store the result in ymm1 subject to writemask k1."
"VCVTPH2QQ zmm{k}{z},xmm/m128/m16bcst{er}","EVEX.512.66.map5.W0 7b /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight signed quadword integers, and store the result in zmm1 subject to writemask k1."
"VCVTPH2UDQ xmm{k}{z},xmm/m64/m16bcst","EVEX.128.map5.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert four packed FP16 values in xmm2/m64/m16bcst to four unsigned doubleword integers, and store the result in xmm1 subject to writemask k1."
"VCVTPH2UDQ ymm{k}{z},xmm/m128/m16bcst","EVEX.256.map5.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight unsigned doubleword integers, and store the result in ymm1 subject to writemask k1."
"VCVTPH2UDQ zmm{k}{z},ymm/m256/m16bcst{er}","EVEX.512.map5.W0 79 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert sixteen packed FP16 values in ymm2/m256/m16bcst to sixteen unsigned doubleword integers, and store the result in zmm1 subject to writemask k1."
"VCVTPH2UQQ xmm{k}{z},xmm/m32/m16bcst","EVEX.128.66.map5.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert two packed FP16 values in xmm2/m32/m16bcst to two unsigned quadword integers, and store the result in xmm1 subject to writemask k1."
"VCVTPH2UQQ ymm{k}{z},xmm/m64/m16bcst","EVEX.256.66.map5.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert four packed FP16 values in xmm2/m64/m16bcst to four unsigned quadword integers, and store the result in ymm1 subject to writemask k1."
"VCVTPH2UQQ zmm{k}{z},xmm/m128/m16bcst{er}","EVEX.512.66.map5.W0 79 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight unsigned quadword integers, and store the result in zmm1 subject to writemask k1."
"VCVTPH2UW xmm{k}{z},xmm/m128/m16bcst","EVEX.128.map5.W0 7d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert packed FP16 values in xmm2/m128/m16bcst to unsigned word integers, and store the result in xmm1."
"VCVTPH2UW ymm{k}{z},ymm/m256/m16bcst","EVEX.256.map5.W0 7d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert packed FP16 values in ymm2/m256/m16bcst to unsigned word integers, and store the result in ymm1."
"VCVTPH2UW zmm{k}{z},zmm/m512/m16bcst{er}","EVEX.512.map5.W0 7d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert packed FP16 values in zmm2/m512/m16bcst to unsigned word integers, and store the result in zmm1."
"VCVTPH2W xmm{k}{z},xmm/m128/m16bcst","EVEX.128.66.map5.W0 7d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert packed FP16 values in xmm2/m128/m16bcst to signed word integers, and store the result in xmm1."
"VCVTPH2W ymm{k}{z},ymm/m256/m16bcst","EVEX.256.66.map5.W0 7d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert packed FP16 values in ymm2/m256/m16bcst to signed word integers, and store the result in ymm1."
"VCVTPH2W zmm{k}{z},zmm/m512/m16bcst{er}","EVEX.512.66.map5.W0 7d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert packed FP16 values in zmm2/m512/m16bcst to signed word integers, and store the result in zmm1."
"VCVTPS2PHX xmm{k}{z},xmm/m128/m32bcst","EVEX.128.66.map5.W0 1d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed single precision floating-point values in xmm2/m128/m32bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTPS2PHX xmm{k}{z},ymm/m256/m32bcst","EVEX.256.66.map5.W0 1d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed single precision floating-point values in ymm2/m256/m32bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTPS2PHX ymm{k}{z},zmm/m512/m32bcst{er}","EVEX.512.66.map5.W0 1d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed single precision floating-point values in zmm2 /m512/m32bcst to packed FP16 values, and store the result in ymm1 subject to writemask k1."
"VCVTQQ2PH xmm{k}{z},xmm/m128/m64bcst","EVEX.128.map5.W1 5b /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert two packed signed quadword integers in xmm2/m128/m64bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTQQ2PH xmm{k}{z},ymm/m256/m64bcst","EVEX.256.map5.W1 5b /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed signed quadword integers in ymm2/m256/m64bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTQQ2PH xmm{k}{z},zmm/m512/m64bcst{er}","EVEX.512.map5.W1 5b /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed signed quadword integers in zmm2/m512/m64bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTSD2SH xmm{k}{z},xmm,xmm/m64{er}","EVEX.LIG.f2.map5.W1 5a /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Convert the low FP64 value in xmm3/m64 to an FP16 value and store the result in the low element of xmm1 subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VCVTSH2SD xmm,xmm,xmm/m16{sae}","EVEX.LIG.f3.map5.W0 5a /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Convert the low FP16 value in xmm3/m16 to an FP64 value and store the result in the low element of xmm1 subject to writemask k1. Bits 127:64 of xmm2 are copied to xmm1[127:64]."
"VCVTSH2SI r32,xmm/m16{er}","EVEX.LIG.f3.map5.W0 2d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Convert the low FP16 element in xmm1/m16 to a signed integer and store the result in r32."
"VCVTSH2SI r64,xmm/m16{er}","EVEX.LIG.f3.map5.W1 2d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Convert the low FP16 element in xmm1/m16 to a signed integer and store the result in r64."
"VCVTSH2SS xmm{k}{z},xmm,xmm/m16{sae}","EVEX.LIG.map6.W0 13 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Convert the low FP16 element in xmm3/m16 to an FP32 value and store in the low element of xmm1 subject to writemask k1. Bits 127:32 of xmm2 are copied to xmm1[127:32]."
"VCVTSH2USI r32,xmm/m16{er}","EVEX.LIG.f3.map5.W0 79 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Convert the low FP16 element in xmm1/m16 to an unsigned integer and store the result in r32."
"VCVTSH2USI r64,xmm/m16{er}","EVEX.LIG.f3.map5.W1 79 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Convert the low FP16 element in xmm1/m16 to an unsigned integer and store the result in r64."
"VCVTSI2SH xmm,xmm,r32/m32{er}","EVEX.LIG.f3.map5.W0 2a /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Convert the signed doubleword integer in r32/m32 to an FP16 value and store the result in xmm1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VCVTSI2SH xmm,xmm,r64/m64{er}","EVEX.LIG.f3.map5.W1 2a /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Convert the signed quadword integer in r64/m64 to an FP16 value and store the result in xmm1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VCVTSS2SH xmm,xmm,xmm/m32{er}","EVEX.LIG.map5.W0 1d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Convert low FP32 value in xmm3/m32 to an FP16 value and store in the low element of xmm1 subject to writemask k1. Bits 127:16 from xmm2 are copied to xmm1[127:16]."
"VCVTTPH2DQ xmm{k}{z},xmm/m64/m16bcst","EVEX.128.f3.map5.W0 5b /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert four packed FP16 values in xmm2/m64/m16bcst to four signed doubleword integers, and store the result in xmm1 using truncation subject to writemask k1."
"VCVTTPH2DQ ymm{k}{z},xmm/m128/m16bcst","EVEX.256.f3.map5.W0 5b /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight signed doubleword integers, and store the result in ymm1 using truncation subject to writemask k1."
"VCVTTPH2DQ zmm{k}{z},ymm/m256/m16bcst{sae}","EVEX.512.f3.map5.W0 5b /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert sixteen packed FP16 values in ymm2/m256/m16bcst to sixteen signed doubleword integers, and store the result in zmm1 using truncation subject to writemask k1."
"VCVTTPH2QQ xmm{k}{z},xmm/m32/m16bcst","EVEX.128.66.map5.W0 7a /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert two packed FP16 values in xmm2/m32/m16bcst to two signed quadword integers, and store the result in xmm1 using truncation subject to writemask k1."
"VCVTTPH2QQ ymm{k}{z},xmm/m64/m16bcst","EVEX.256.66.map5.W0 7a /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert four packed FP16 values in xmm2/m64/m16bcst to four signed quadword integers, and store the result in ymm1 using truncation subject to writemask k1."
"VCVTTPH2QQ zmm{k}{z},xmm/m128/m16bcst{sae}","EVEX.512.66.map5.W0 7a /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight signed quadword integers, and store the result in zmm1 using truncation subject to writemask k1."
"VCVTTPH2UDQ xmm{k}{z},xmm/m64/m16bcst","EVEX.128.map5.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert four packed FP16 values in xmm2/m64/m16bcst to four unsigned doubleword integers, and store the result in xmm1 using truncation subject to writemask k1."
"VCVTTPH2UDQ ymm{k}{z},xmm/m128/m16bcst","EVEX.256.map5.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight unsigned doubleword integers, and store the result in ymm1 using truncation subject to writemask k1."
"VCVTTPH2UDQ zmm{k}{z},ymm/m256/m16bcst{sae}","EVEX.512.map5.W0 78 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector","Convert sixteen packed FP16 values in ymm2/m256/m16bcst to sixteen unsigned doubleword integers, and store the result in zmm1 using truncation subject to writemask k1."
"VCVTTPH2UQQ xmm{k}{z},xmm/m32/m16bcst","EVEX.128.66.map5.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert two packed FP16 values in xmm2/m32/m16bcst to two unsigned quadword integers, and store the result in xmm1 using truncation subject to writemask k1."
"VCVTTPH2UQQ ymm{k}{z},xmm/m64/m16bcst","EVEX.256.66.map5.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert four packed FP16 values in xmm2/m64/m16bcst to four unsigned quadword integers, and store the result in ymm1 using truncation subject to writemask k1."
"VCVTTPH2UQQ zmm{k}{z},xmm/m128/m16bcst{sae}","EVEX.512.66.map5.W0 78 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Quarter Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight unsigned quadword integers, and store the result in zmm1 using truncation subject to writemask k1."
"VCVTTPH2UW xmm{k}{z},xmm/m128/m16bcst","EVEX.128.map5.W0 7c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight unsigned word integers, and store the result in xmm1 using truncation subject to writemask k1."
"VCVTTPH2UW ymm{k}{z},ymm/m256/m16bcst","EVEX.256.map5.W0 7c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed FP16 values in ymm2/m256/m16bcst to sixteen unsigned word integers, and store the result in ymm1 using truncation subject to writemask k1."
"VCVTTPH2UW zmm{k}{z},zmm/m512/m16bcst{sae}","EVEX.512.map5.W0 7c /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert thirty-two packed FP16 values in zmm2/m512/m16bcst to thirty-two unsigned word integers, and store the result in zmm1 using truncation subject to writemask k1."
"VCVTTPH2W xmm{k}{z},xmm/m128/m16bcst","EVEX.128.66.map5.W0 7c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed FP16 values in xmm2/m128/m16bcst to eight signed word integers, and store the result in xmm1 using truncation subject to writemask k1."
"VCVTTPH2W ymm{k}{z},ymm/m256/m16bcst","EVEX.256.66.map5.W0 7c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed FP16 values in ymm2/m256/m16bcst to sixteen signed word integers, and store the result in ymm1 using truncation subject to writemask k1."
"VCVTTPH2W zmm{k}{z},zmm/m512/m16bcst{sae}","EVEX.512.66.map5.W0 7c /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert thirty-two packed FP16 values in zmm2/m512/m16bcst to thirty-two signed word integers, and store the result in zmm1 using truncation subject to writemask k1."
"VCVTTSH2SI r32,xmm/m16{sae}","EVEX.LIG.f3.map5.W0 2c /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Convert FP16 value in the low element of xmm1/m16 to a signed integer and store the result in r32 using truncation."
"VCVTTSH2SI r64,xmm/m16{sae}","EVEX.LIG.f3.map5.W1 2c /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Convert FP16 value in the low element of xmm1/m16 to a signed integer and store the result in r64 using truncation."
"VCVTTSH2USI r32,xmm/m16{sae}","EVEX.LIG.f3.map5.W0 78 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Convert FP16 value in the low element of xmm1/m16 to an unsigned integer and store the result in r32 using truncation."
"VCVTTSH2USI r64,xmm/m16{sae}","EVEX.LIG.f3.map5.W1 78 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Convert FP16 value in the low element of xmm1/m16 to an unsigned integer and store the result in r64 using truncation."
"VCVTUDQ2PH xmm{k}{z},xmm/m128/m32bcst","EVEX.128.f2.map5.W0 7a /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed unsigned doubleword integers from xmm2/m128/m32bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTUDQ2PH xmm{k}{z},ymm/m256/m32bcst","EVEX.256.f2.map5.W0 7a /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed unsigned doubleword integers from ymm2/m256/m32bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTUDQ2PH ymm{k}{z},zmm/m512/m32bcst","EVEX.512.f2.map5.W0 7a /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed unsigned doubleword integers from zmm2/m512/m32bcst to packed FP16 values, and store the result in ymm1 subject to writemask k1."
"VCVTUQQ2PH xmm{k}{z},xmm/m128/m64bcst","EVEX.128.f2.map5.W1 7a /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert two packed unsigned doubleword integers from xmm2/m128/m64bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTUQQ2PH xmm{k}{z},ymm/m256/m64bcst","EVEX.256.f2.map5.W1 7a /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert four packed unsigned doubleword integers from ymm2/m256/m64bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTUQQ2PH xmm{k}{z},zmm/m512/m64bcst","EVEX.512.f2.map5.W1 7a /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed unsigned doubleword integers from zmm2/m512/m64bcst to packed FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTUSI2SH xmm,xmm,r32/m32{er}","EVEX.LIG.f3.map5.W0 7b /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Convert an unsigned doubleword integer from r32/m32 to an FP16 value, and store the result in xmm1. Bits 127:16 from xmm2 are copied to xmm1[127:16]."
"VCVTUSI2SH xmm,xmm,r64/m64{er}","EVEX.LIG.f3.map5.W1 7b /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Convert an unsigned quadword integer from r64/m64 to an FP16 value, and store the result in xmm1. Bits 127:16 from xmm2 are copied to xmm1[127:16]."
"VCVTUW2PH xmm{k}{z},xmm/m128/m16bcst","EVEX.128.f2.map5.W0 7d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed unsigned word integers from xmm2/m128/m16bcst to FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTUW2PH ymm{k}{z},ymm/m256/m16bcst","EVEX.256.f2.map5.W0 7d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed unsigned word integers from ymm2/m256/m16bcst to FP16 values, and store the result in ymm1 subject to writemask k1."
"VCVTUW2PH zmm{k}{z},zmm/m512/m16bcst{er}","EVEX.512.f2.map5.W0 7d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert thirty-two packed unsigned word integers from zmm2/m512/m16bcst to FP16 values, and store the result in zmm1 subject to writemask k1."
"VCVTW2PH xmm{k}{z},xmm/m128/m16bcst","EVEX.128.f3.map5.W0 7d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert eight packed signed word integers from xmm2/m128/m16bcst to FP16 values, and store the result in xmm1 subject to writemask k1."
"VCVTW2PH ymm{k}{z},ymm/m256/m16bcst","EVEX.256.f3.map5.W0 7d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert sixteen packed signed word integers from ymm2/m256/m16bcst to FP16 values, and store the result in ymm1 subject to writemask k1."
"VCVTW2PH zmm{k}{z},zmm/m512/m16bcst{er}","EVEX.512.f3.map5.W0 7d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert thirty-two packed signed word integers from zmm2/m512/m16bcst to FP16 values, and store the result in zmm1 subject to writemask k1."
"VDIVPH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.map5.W0 5e /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Divide packed FP16 values in xmm2 by packed FP16 values in xmm3/m128/m16bcst, and store the result in xmm1 subject to writemask k1."
"VDIVPH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.map5.W0 5e /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Divide packed FP16 values in ymm2 by packed FP16 values in ymm3/m256/m16bcst, and store the result in ymm1 subject to writemask k1."
"VDIVPH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.map5.W0 5e /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Divide packed FP16 values in zmm2 by packed FP16 values in zmm3/m512/m16bcst, and store the result in zmm1 subject to writemask k1."
"VDIVSH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.f3.map5.W0 5e /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Divide low FP16 value in xmm2 by low FP16 value in xmm3/m16, and store the result in xmm1 subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VFCMADDCPH xmm{k}{z},xmm,xmm/m128/m32bcst","EVEX.128.f2.map6.W0 56 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from xmm2 and xmm3/m128/m32bcst, add to xmm1 and store the result in xmm1 subject to writemask k1."
"VFCMADDCPH ymm{k}{z},ymm,ymm/m256/m32bcst","EVEX.256.f2.map6.W0 56 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from ymm2 and ymm3/m256/m32bcst, add to ymm1 and store the result in ymm1 subject to writemask k1."
"VFCMADDCPH zmm{k}{z},zmm,zmm/m512/m32bcst{er}","EVEX.512.f2.map6.W0 56 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from zmm2 and zmm3/m512/m32bcst, add to zmm1 and store the result in zmm1 subject to writemask k1."
"VFCMADDCSH xmm{k}{z},xmm,xmm/m32{er}","EVEX.LIG.f2.map6.W0 57 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Complex multiply a pair of FP16 values from xmm2 and xmm3/m32, add to xmm1 and store the result in xmm1 subject to writemask k1. Bits 127:32 of xmm2 are copied to xmm1[127:32]."
"VFCMULCPH xmm{k}{z},xmm,xmm/m128/m32bcst","EVEX.128.f2.map6.W0 d6 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from xmm2 and xmm3/m128/m32bcst, and store the result in xmm1 subject to writemask k1."
"VFCMULCPH ymm{k}{z},ymm,ymm/m256/m32bcst","EVEX.256.f2.map6.W0 d6 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from ymm2 and ymm3/m256/m32bcst, and store the result in ymm1 subject to writemask k1."
"VFCMULCPH zmm{k}{z},zmm,zmm/m512/m32bcst{er}","EVEX.512.f2.map6.W0 d6 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from zmm2 and zmm3/m512/m32bcst, and store the result in zmm1 subject to writemask k1."
"VFCMULCSH xmm{k}{z},xmm,xmm/m32{er}","EVEX.LIG.f2.map6.W0 d7 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Complex multiply a pair of FP16 values from xmm2 and xmm3/m32, and store the result in xmm1 subject to writemask k1. Bits 127:32 of xmm2 are copied to xmm1[127:32]."
"VFMADD132PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 98 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm3/m128/m16bcst, add to xmm2, and store the result in xmm1."
"VFMADD132PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 98 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm3/m256/m16bcst, add to ymm2, and store the result in ymm1."
"VFMADD132PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 98 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm3/m512/m16bcst, add to zmm2, and store the result in zmm1."
"VFMADD132SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 99 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm1 and xmm3/m16, add to xmm2, and store the result in xmm1."
"VFMADD213PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 a8 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm2, add to xmm3/m128/m16bcst, and store the result in xmm1."
"VFMADD213PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 a8 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm2, add to ymm3/m256/m16bcst, and store the result in ymm1."
"VFMADD213PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 a8 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm2, add to zmm3/m512/m16bcst, and store the result in zmm1."
"VFMADD213SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 a9 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm1 and xmm2, add to xmm3/m16, and store the result in xmm1."
"VFMADD231PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 b8 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm2 and xmm3/m128/m16bcst, add to xmm1, and store the result in xmm1."
"VFMADD231PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 b8 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm2 and ymm3/m256/m16bcst, add to ymm1, and store the result in ymm1."
"VFMADD231PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 b8 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm2 and zmm3/m512/m16bcst, add to zmm1, and store the result in zmm1."
"VFMADD231SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 b9 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm2 and xmm3/m16, add to xmm1, and store the result in xmm1."
"VFMADDCPH xmm{k}{z},xmm,xmm/m128/m32bcst","EVEX.128.f3.map6.W0 56 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from xmm2 and the complex conjugate of xmm3/m128/m32bcst, add to xmm1 and store the result in xmm1 subject to writemask k1."
"VFMADDCPH ymm{k}{z},ymm,ymm/m256/m32bcst","EVEX.256.f3.map6.W0 56 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from ymm2 and the complex conjugate of ymm3/m256/m32bcst, add to ymm1 and store the result in ymm1 subject to writemask k1."
"VFMADDCPH zmm{k}{z},zmm,zmm/m512/m32bcst{er}","EVEX.512.f3.map6.W0 56 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from zmm2 and the complex conjugate of zmm3/m512/m32bcst, add to zmm1 and store the result in zmm1 subject to writemask k1."
"VFMADDCSH xmm{k}{z},xmm,xmm/m32{er}","EVEX.LIG.f3.map6.W0 57 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Complex multiply a pair of FP16 values from xmm2 and the complex conjugate of xmm3/m32, add to xmm1 and store the result in xmm1 subject to writemask k1. Bits 127:32 of xmm2 are copied to xmm1[127:32]."
"VFMADDSUB132PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 96 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm3/m128/m16bcst, add/subtract elements in xmm2, and store the result in xmm1 subject to writemask k1."
"VFMADDSUB132PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 96 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm3/m256/m16bcst, add/subtract elements in ymm2, and store the result in ymm1 subject to writemask k1."
"VFMADDSUB132PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 96 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm3/m512/m16bcst, add/subtract elements in zmm2, and store the result in zmm1 subject to writemask k1."
"VFMADDSUB213PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 a6 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm2, add/subtract elements in xmm3/m128/m16bcst, and store the result in xmm1 subject to writemask k1."
"VFMADDSUB213PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 a6 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm2, add/subtract elements in ymm3/m256/m16bcst, and store the result in ymm1 subject to writemask k1."
"VFMADDSUB213PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 a6 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm2, add/subtract elements in zmm3/m512/m16bcst, and store the result in zmm1 subject to writemask k1."
"VFMADDSUB231PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 b6 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm2 and xmm3/m128/m16bcst, add/subtract elements in xmm1, and store the result in xmm1 subject to writemask k1."
"VFMADDSUB231PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 b6 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm2 and ymm3/m256/m16bcst, add/subtract elements in ymm1, and store the result in ymm1 subject to writemask k1."
"VFMADDSUB231PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 b6 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm2 and zmm3/m512/m16bcst, add/subtract elements in zmm1, and store the result in zmm1 subject to writemask k1."
"VFMSUB132PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 9a /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm3/m128/m16bcst, subtract xmm2, and store the result in xmm1 subject to writemask k1."
"VFMSUB132PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 9a /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm3/m256/m16bcst, subtract ymm2, and store the result in ymm1 subject to writemask k1."
"VFMSUB132PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 9a /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm3/m512/m16bcst, subtract zmm2, and store the result in zmm1 subject to writemask k1."
"VFMSUB132SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 9b /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm1 and xmm3/m16, subtract xmm2, and store the result in xmm1 subject to writemask k1."
"VFMSUB213PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 aa /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm2, subtract xmm3/m128/m16bcst, and store the result in xmm1 subject to writemask k1."
"VFMSUB213PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 aa /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm2, subtract ymm3/m256/m16bcst, and store the result in ymm1 subject to writemask k1."
"VFMSUB213PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 aa /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm2, subtract zmm3/m512/m16bcst, and store the result in zmm1 subject to writemask k1."
"VFMSUB213SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 ab /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm1 and xmm2, subtract xmm3/m16, and store the result in xmm1 subject to writemask k1."
"VFMSUB231PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 ba /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm2 and xmm3/m128/m16bcst, subtract xmm1, and store the result in xmm1 subject to writemask k1."
"VFMSUB231PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 ba /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm2 and ymm3/m256/m16bcst, subtract ymm1, and store the result in ymm1 subject to writemask k1."
"VFMSUB231PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 ba /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm2 and zmm3/m512/m16bcst, subtract zmm1, and store the result in zmm1 subject to writemask k1."
"VFMSUB231SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 bb /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm2 and xmm3/m16, subtract xmm1, and store the result in xmm1 subject to writemask k1."
"VFMSUBADD132PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 97 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm3/m128/m16bcst, subtract/add elements in xmm2, and store the result in xmm1 subject to writemask k1."
"VFMSUBADD132PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 97 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm3/m256/m16bcst, subtract/add elements in ymm2, and store the result in ymm1 subject to writemask k1."
"VFMSUBADD132PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 97 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm3/m512/m16bcst, subtract/add elements in zmm2, and store the result in zmm1 subject to writemask k1."
"VFMSUBADD213PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 a7 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm2, subtract/add elements in xmm3/m128/m16bcst, and store the result in xmm1 subject to writemask k1."
"VFMSUBADD213PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 a7 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm2, subtract/add elements in ymm3/m256/m16bcst, and store the result in ymm1 subject to writemask k1."
"VFMSUBADD213PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 a7 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm2, subtract/add elements in zmm3/m512/m16bcst, and store the result in zmm1 subject to writemask k1."
"VFMSUBADD231PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 b7 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm2 and xmm3/m128/m16bcst, subtract/add elements in xmm1, and store the result in xmm1 subject to writemask k1."
"VFMSUBADD231PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 b7 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm2 and ymm3/m256/m16bcst, subtract/add elements in ymm1, and store the result in ymm1 subject to writemask k1."
"VFMSUBADD231PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 b7 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm2 and zmm3/m512/m16bcst, subtract/add elements in zmm1, and store the result in zmm1 subject to writemask k1."
"VFMULCPH xmm{k}{z},xmm,xmm/m128/m32bcst","EVEX.128.f3.map6.W0 d6 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from xmm2 and the complex conjugate of xmm3/ m128/m32bcst, and store the result in xmm1 subject to writemask k1."
"VFMULCPH ymm{k}{z},ymm,ymm/m256/m32bcst","EVEX.256.f3.map6.W0 d6 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from ymm2 and the complex conjugate of ymm3/m256/m32bcst, and store the result in ymm1 subject to writemask k1."
"VFMULCPH zmm{k}{z},zmm,zmm/m512/m32bcst{er}","EVEX.512.f3.map6.W0 d6 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Complex multiply a pair of FP16 values from zmm2 and the complex conjugate of zmm3/m512/m32bcst, and store the result in zmm1 subject to writemask k1."
"VFMULCSH xmm{k}{z},xmm,xmm/m32{er}","EVEX.LIG.f3.map6.W0 d7 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Complex multiply a pair of FP16 values from xmm2 and the complex conjugate of xmm3/m32, and store the result in xmm1 subject to writemask k1. Bits 127:32 of xmm2 are copied to xmm1[127:32]."
"VFNMADD132PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 9c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm3/m128/m16bcst, and negate the value. Add this value to xmm2, and store the result in xmm1."
"VFNMADD132PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 9c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm3/m256/m16bcst, and negate the value. Add this value to ymm2, and store the result in ymm1."
"VFNMADD132PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 9c /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm3/m512/m16bcst, and negate the value. Add this value to zmm2, and store the result in zmm1."
"VFNMADD132SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 9d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm1 and xmm3/m16, and negate the value. Add this value to xmm2, and store the result in xmm1."
"VFNMADD213PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 ac /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm2, and negate the value. Add this value to xmm3/m128/m16bcst, and store the result in xmm1."
"VFNMADD213PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 ac /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm2, and negate the value. Add this value to ymm3/m256/m16bcst, and store the result in ymm1."
"VFNMADD213PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 ac /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm2, and negate the value. Add this value to zmm3/m512/m16bcst, and store the result in zmm1."
"VFNMADD213SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 ad /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm1 and xmm2, and negate the value. Add this value to xmm3/m16, and store the result in xmm1."
"VFNMADD231PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 bc /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm2 and xmm3/m128/m16bcst, and negate the value. Add this value to xmm1, and store the result in xmm1."
"VFNMADD231PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 bc /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm2 and ymm3/m256/m16bcst, and negate the value. Add this value to ymm1, and store the result in ymm1."
"VFNMADD231PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 bc /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm2 and zmm3/m512/m16bcst, and negate the value. Add this value to zmm1, and store the result in zmm1."
"VFNMADD231SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 bd /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm2 and xmm3/m16, and negate the value. Add this value to xmm1, and store the result in xmm1."
"VFNMSUB132PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 9e /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm3/m128/m16bcst, and negate the value. Subtract xmm2 from this value, and store the result in xmm1 subject to writemask k1."
"VFNMSUB132PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 9e /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm3/m256/m16bcst, and negate the value. Subtract ymm2 from this value, and store the result in ymm1 subject to writemask k1."
"VFNMSUB132PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 9e /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm3/m512/m16bcst, and negate the value. Subtract zmm2 from this value, and store the result in zmm1 subject to writemask k1."
"VFNMSUB132SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 9f /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm1 and xmm3/m16, and negate the value. Subtract xmm2 from this value, and store the result in xmm1 subject to writemask k1."
"VFNMSUB213PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 ae /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm1 and xmm2, and negate the value. Subtract xmm3/m128/m16bcst from this value, and store the result in xmm1 subject to writemask k1."
"VFNMSUB213PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 ae /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm1 and ymm2, and negate the value. Subtract ymm3/m256/m16bcst from this value, and store the result in ymm1 subject to writemask k1."
"VFNMSUB213PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 ae /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm1 and zmm2, and negate the value. Subtract zmm3/m512/m16bcst from this value, and store the result in zmm1 subject to writemask k1."
"VFNMSUB213SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 af /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm1 and xmm2, and negate the value. Subtract xmm3/m16 from this value, and store the result in xmm1 subject to writemask k1."
"VFNMSUB231PH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 be /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm2 and xmm3/m128/m16bcst, and negate the value. Subtract xmm1 from this value, and store the result in xmm1 subject to writemask k1."
"VFNMSUB231PH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 be /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm2 and ymm3/m256/m16bcst, and negate the value. Subtract ymm1 from this value, and store the result in ymm1 subject to writemask k1."
"VFNMSUB231PH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 be /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from zmm2 and zmm3/m512/m16bcst, and negate the value. Subtract zmm1 from this value, and store the result in zmm1 subject to writemask k1."
"VFNMSUB231SH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 bf /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply FP16 values from xmm2 and xmm3/m16, and negate the value. Subtract xmm1 from this value, and store the result in xmm1 subject to writemask k1."
"VFPCLASSPH k{k},xmm/m128/m16bcst,ib","EVEX.128.0F3A.W0 66 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Test the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSPH k{k},ymm/m256/m16bcst,ib","EVEX.256.0F3A.W0 66 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Test the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSPH k{k},zmm/m512/m16bcst,ib","EVEX.512.0F3A.W0 66 /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Test the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSSH k{k},xmm/m16,ib","EVEX.LIG.0F3A.W0 67 /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Scalar","Test the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VGETEXPPH xmm{k}{z},xmm/m128/m16bcst","EVEX.128.66.map6.W0 42 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert the exponent of FP16 values in the source operand to FP16 results representing unbiased integer exponents and stores the results in the destination register subject to writemask k1."
"VGETEXPPH ymm{k}{z},ymm/m256/m16bcst","EVEX.256.66.map6.W0 42 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert the exponent of FP16 values in the source operand to FP16 results representing unbiased integer exponents and stores the results in the destination register subject to writemask k1."
"VGETEXPPH zmm{k}{z},zmm/m512/m16bcst{sae}","EVEX.512.66.map6.W0 42 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Convert the exponent of FP16 values in the source operand to FP16 results representing unbiased integer exponents and stores the results in the destination register subject to writemask k1."
"VGETEXPSH xmm{k}{z},xmm,xmm/m16 {sae}","EVEX.128.66.map6.W0 43 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Convert the exponent of FP16 values in the low word of the source operand to FP16 results representing unbiased integer exponents, and stores the results in the low word of the destination register subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VGETMANTPH xmm{k}{z},xmm/m128/m16bcst,ib","EVEX.128.0F3A.W0 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Get normalized mantissa from FP16 vector xmm2/m128/m16bcst and store the result in xmm1, using imm8 for sign control and mantissa interval normalization, subject to writemask k1."
"VGETMANTPH ymm{k}{z},ymm/m256/m16bcst,ib","EVEX.256.0F3A.W0 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Get normalized mantissa from FP16 vector ymm2/m256/m16bcst and store the result in ymm1, using imm8 for sign control and mantissa interval normalization, subject to writemask k1."
"VGETMANTPH zmm{k}{z},zmm/m512/m16bcst{sae},ib","EVEX.512.0F3A.W0 26 /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Get normalized mantissa from FP16 vector zmm2/m512/m16bcst and store the result in zmm1, using imm8 for sign control and mantissa interval normalization, subject to writemask k1."
"VGETMANTSH xmm{k}{z},xmm,xmm/m16{sae},ib","EVEX.128.0F3A.W0 27 /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","Scalar","Extract the normalized mantissa of the low FP16 element in xmm3/m16 using imm8 for sign control and mantissa interval normalization. Store the mantissa to xmm1 subject to writemask k1 and merge with the other elements of xmm2. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VMAXPH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.map5.W0 5f /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the maximum packed FP16 values between xmm2 and xmm3/m128/m16bcst and store the result in xmm1 subject to writemask k1."
"VMAXPH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.map5.W0 5f /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the maximum packed FP16 values between ymm2 and ymm3/m256/m16bcst and store the result in ymm1 subject to writemask k1."
"VMAXPH zmm{k}{z},zmm,zmm/m512/m16bcst{sae}","EVEX.512.map5.W0 5f /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the maximum packed FP16 values between zmm2 and zmm3/m512/m16bcst and store the result in zmm1 subject to writemask k1."
"VMAXSH xmm{k}{z},xmm,xmm/m16{sae}","EVEX.LIG.f3.map5.W0 5f /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Return the maximum low FP16 value between xmm3/m16 and xmm2 and store the result in xmm1 subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VMINPH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.map5.W0 5d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the minimum packed FP16 values between xmm2 and xmm3/m128/m16bcst and store the result in xmm1 subject to writemask k1."
"VMINPH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.map5.W0 5d /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the minimum packed FP16 values between ymm2 and ymm3/m256/m16bcst and store the result in ymm1 subject to writemask k1."
"VMINPH zmm{k}{z},zmm,zmm/m512/m16bcst{sae}","EVEX.512.map5.W0 5d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Return the minimum packed FP16 values between zmm2 and zmm3/m512/m16bcst and store the result in zmm1 subject to writemask k1."
"VMINSH xmm{k}{z},xmm,xmm/m16{sae}","EVEX.LIG.f3.map5.W0 5d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Return the minimum low FP16 value between xmm3/m16 and xmm2. Stores the result in xmm1 subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VMOVSH xmm{k}{z},m16","EVEX.LIG.f3.map5.W0 10 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (r, w)","ModRM:r/m (r)","","","Scalar","Move FP16 value from m16 to xmm1 subject to writemask k1."
"VMOVSH xmm{k}{z},xmm,xmm","EVEX.LIG.f3.map5.W0 10 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Move low FP16 values from xmm3 to xmm1 subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VMOVSH m16{k},xmm","EVEX.LIG.f3.map5.W0 11 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:r/m (w)","ModRM:reg (r)","","","Scalar","Move low FP16 value from xmm1 to m16 subject to writemask k1."
"VMOVSH xmm{k}{z},xmm,xmm","EVEX.LIG.f3.map5.W0 11 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Move low FP16 values from xmm3 to xmm1 subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VMOVW xmm,r32/m16","EVEX.128.66.map5.wig 6e /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Copy word from reg/m16 to xmm1."
"VMOVW r32/m16,xmm","EVEX.128.66.map5.wig 7e /r","Valid","Valid","Invalid","AVX512FP16","ModRM:r/m (w)","ModRM:reg (r)","","","Scalar","Copy word from xmm1 to reg/m16."
"VMULPH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.map5.W0 59 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from xmm3/m128/ m16bcst to xmm2 and store the result in xmm1 subject to writemask k1."
"VMULPH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.map5.W0 59 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values from ymm3/m256/ m16bcst to ymm2 and store the result in ymm1 subject to writemask k1."
"VMULPH zmm{k}{z},zmm,zmm/m512/m16bcst","EVEX.512.map5.W0 59 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Multiply packed FP16 values in zmm3/m512/m16bcst with zmm2 and store the result in zmm1 subject to writemask k1."
"VMULSH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.f3.map5.W0 59 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Multiply the low FP16 value in xmm3/m16 by low FP16 value in xmm2, and store the result in xmm1 subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VRCPPH xmm{k}{z},xmm/m128/m16bcst","EVEX.128.66.map6.W0 4c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Compute the approximate reciprocals of packed FP16 values in xmm2/m128/m16bcst and store the result in xmm1 subject to writemask k1."
"VRCPPH ymm{k}{z},ymm/m256/m16bcst","EVEX.256.66.map6.W0 4c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Compute the approximate reciprocals of packed FP16 values in ymm2/m256/m16bcst and store the result in ymm1 subject to writemask k1."
"VRCPPH zmm{k}{z},zmm/m512/m16bcst","EVEX.512.66.map6.W0 4c /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Compute the approximate reciprocals of packed FP16 values in zmm2/m512/m16bcst and store the result in zmm1 subject to writemask k1."
"VRCPSH xmm{k}{z},xmm,xmm/m16","EVEX.LIG.66.map6.W0 4d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Compute the approximate reciprocal of the low FP16 value in xmm3/m16 and store the result in xmm1 subject to writemask k1. Bits 127:16 from xmm2 are copied to xmm1[127:16]."
"VREDUCEPH xmm{k}{z},xmm/m128/m16bcst,ib","EVEX.128.0F3A.W0 56 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Perform reduction transformation on packed FP16 values in xmm2/m128/m16bcst by subtracting a number of fraction bits specified by the imm8 field. Store the result in xmm1 subject to writemask k1."
"VREDUCEPH ymm{k}{z},ymm/m256/m16bcst,ib","EVEX.256.0F3A.W0 56 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Perform reduction transformation on packed FP16 values in ymm2/m256/m16bcst by subtracting a number of fraction bits specified by the imm8 field. Store the result in ymm1 subject to writemask k1."
"VREDUCEPH zmm{k}{z},zmm/m512/m16bcst{sae},ib","EVEX.512.0F3A.W0 56 /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Perform reduction transformation on packed FP16 values in zmm2/m512/m16bcst by subtracting a number of fraction bits specified by the imm8 field. Store the result in zmm1 subject to writemask k1."
"VREDUCESH xmm{k}{z},xmm,xmm/m16{sae},ib","EVEX.LIG.0F3A.W0 57 /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","Scalar","Perform a reduction transformation on the low binary encoded FP16 value in xmm3/m16 by subtracting a number of fraction bits specified by the imm8 field. Store the result in xmm1 subject to writemask k1. Bits 127:16 from xmm2 are copied to xmm1[127:16]."
"VRNDSCALEPH xmm{k}{z},xmm/m128/m16bcst,ib","EVEX.128.0F3A.W0 08 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Round packed FP16 values in xmm2/m128/ m16bcst to a number of fraction bits specified by the imm8 field. Store the result in xmm1 subject to writemask k1."
"VRNDSCALEPH ymm{k}{z},ymm/m256/m16bcst,ib","EVEX.256.0F3A.W0 08 /r ib","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Round packed FP16 values in ymm2/m256/m16bcst to a number of fraction bits specified by the imm8 field. Store the result in ymm1 subject to writemask k1."
"VRNDSCALEPH zmm{k}{z},zmm/m512/m16bcst{sae},ib","EVEX.512.0F3A.W0 08 /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector","Round packed FP16 values in zmm2/m512/m16bcst to a number of fraction bits specified by the imm8 field. Store the result in zmm1 subject to writemask k1."
"VRNDSCALESH xmm{k}{z},xmm,xmm/m16{sae},ib","EVEX.LIG.0F3A.W0 0a /r ib","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","Scalar","Round the low FP16 value in xmm3/m16 to a number of fraction bits specified by the imm8 field. Store the result in xmm1 subject to writemask k1. Bits 127:16 from xmm2 are copied to xmm1[127:16]."
"VRSQRTPH xmm{k}{z},xmm/m128/m16bcst","EVEX.128.66.map6.W0 4e /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Compute the approximate reciprocals of the square roots of packed FP16 values in xmm2/m128/m16bcst and store the result in xmm1 subject to writemask k1."
"VRSQRTPH ymm{k}{z},ymm/m256/m16bcst","EVEX.256.66.map6.W0 4e /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Compute the approximate reciprocals of the square roots of packed FP16 values in ymm2/m256/m16bcst and store the result in ymm1 subject to writemask k1."
"VRSQRTPH zmm{k}{z},zmm/m512/m16bcst","EVEX.512.66.map6.W0 4e /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Compute the approximate reciprocals of the square roots of packed FP16 values in zmm2/m512/m16bcst and store the result in zmm1 subject to writemask k1."
"VRSQRTSH xmm{k}{z},xmm,xmm/m16","EVEX.LIG.66.map6.W0 4f /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Compute the approximate reciprocal square root of the FP16 value in xmm3/m16 and store the result in the low word element of xmm1 subject to writemask k1. Bits 127:16 of xmm2 are copied to xmm1[127:16]."
"VSCALEFPH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.66.map6.W0 2c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Scale the packed FP16 values in xmm2 using values from xmm3/m128/m16bcst, and store the result in xmm1 subject to writemask k1."
"VSCALEFPH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.66.map6.W0 2c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Scale the packed FP16 values in ymm2 using values from ymm3/m256/m16bcst, and store the result in ymm1 subject to writemask k1."
"VSCALEFPH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.66.map6.W0 2c /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Scale the packed FP16 values in zmm2 using values from zmm3/m512/m16bcst, and store the result in zmm1 subject to writemask k1."
"VSCALEFSH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.66.map6.W0 2d /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Scale the FP16 values in xmm2 using the value from xmm3/m16 and store the result in xmm1 subject to writemask k1. Bits 127:16 from xmm2 are copied to xmm1[127:16]."
"VSQRTPH xmm{k}{z},xmm/m128/m16bcst","EVEX.128.map5.W0 51 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Compute square roots of the packed FP16 values in xmm2/m128/m16bcst, and store the result in xmm1 subject to writemask k1."
"VSQRTPH ymm{k}{z},ymm/m256/m16bcst","EVEX.256.map5.W0 51 /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Compute square roots of the packed FP16 values in ymm2/m256/m16bcst, and store the result in ymm1 subject to writemask k1."
"VSQRTPH zmm{k}{z},zmm/m512/m16bcst{er}","EVEX.512.map5.W0 51 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector","Compute square roots of the packed FP16 values in zmm2/m512/m16bcst, and store the result in zmm1 subject to writemask k1."
"VSQRTSH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.f3.map5.W0 51 /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Compute square root of the low FP16 value in xmm3/m16 and store the result in xmm1 subject to writemask k1. Bits 127:16 from xmm2 are copied to xmm1[127:16]."
"VSUBPH xmm{k}{z},xmm,xmm/m128/m16bcst","EVEX.128.map5.W0 5c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed FP16 values from xmm3/m128/m16bcst to xmm2, and store the result in xmm1 subject to writemask k1."
"VSUBPH ymm{k}{z},ymm,ymm/m256/m16bcst","EVEX.256.map5.W0 5c /r","Valid","Valid","Invalid","AVX512VL AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed FP16 values from ymm3/m256/m16bcst to ymm2, and store the result in ymm1 subject to writemask k1."
"VSUBPH zmm{k}{z},zmm,zmm/m512/m16bcst{er}","EVEX.512.map5.W0 5c /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Subtract packed FP16 values from zmm3/m512/m16bcst to zmm2, and store the result in zmm1 subject to writemask k1."
"VSUBSH xmm{k}{z},xmm,xmm/m16{er}","EVEX.LIG.f3.map5.W0 5c /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Scalar","Subtract the low FP16 value in xmm3/m16 from xmm2 and store the result in xmm1 subject to writemask k1. Bits 127:16 from xmm2 are copied to xmm1[127:16]."
"VUCOMISH xmm,xmm/m16{sae}","EVEX.LIG.map5.W0 2e /r","Valid","Valid","Invalid","AVX512FP16","ModRM:reg (w)","ModRM:r/m (r)","","","Scalar","Compare low FP16 values in xmm1 and xmm2/m16 and set the EFLAGS flags accordingly."
