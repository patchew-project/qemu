"Instruction","Opcode","Valid 64-bit","Valid 32-bit","Valid 16-bit","Feature Flags","Operand 1","Operand 2","Operand 3","Operand 4","Tuple Type","Description"
"VBROADCASTI128 ymm1, m128","VEX.256.66.0F38.W0 5A /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast 128 bits of integer data in mem to low and high 128-bits in ymm1."
"VBROADCASTSD ymm1, xmm2","VEX.256.66.0F38.W0 19 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast low double-precision floating-point element in the source operand to four locations in ymm1."
"VBROADCASTSS xmm1, xmm2","VEX.128.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast the low single-precision floating-point element in the source operand to four locations in xmm1."
"VEXTRACTI128 xmm1/m128, ymm2, ib","VEX.256.66.0F3A.W0 39 /r ib","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","ModRM:reg (r)","ib","","","Extract 128 bits of integer data from ymm2 and store results in xmm1/m128."
"VGATHERDPD xmm1, vm32x/f64x2, xmm2","VEX.128.66.0F38.W1 92 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using dword indices specified in vm32x, gather double-pre-cision FP values from memory conditioned on mask speci-fied by xmm2. Conditionally gathered elements are merged into xmm1."
"VGATHERDPD ymm1, vm32x/f64x4, ymm2","VEX.256.66.0F38.W1 92 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using dword indices specified in vm32x, gather double-pre-cision FP values from memory conditioned on mask speci-fied by ymm2. Conditionally gathered elements are merged into ymm1."
"VGATHERDPS xmm1, vm32x/f32x4, xmm2","VEX.128.66.0F38.W0 92 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using dword indices specified in vm32x, gather single-preci-sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VGATHERDPS ymm1, vm32y/f32x8, ymm2","VEX.256.66.0F38.W0 92 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using dword indices specified in vm32y, gather single-preci-sion FP values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1."
"VGATHERQPD xmm1, vm64x/f64x2, xmm2","VEX.128.66.0F38.W1 93 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using qword indices specified in vm64x, gather double-pre-cision FP values from memory conditioned on mask speci-fied by xmm2. Conditionally gathered elements are merged into xmm1."
"VGATHERQPD ymm1, vm64y/f64x4, ymm2","VEX.256.66.0F38.W1 93 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using qword indices specified in vm64y, gather double-pre-cision FP values from memory conditioned on mask speci-fied by ymm2. Conditionally gathered elements are merged into ymm1."
"VGATHERQPS xmm1, vm64x/f32x2, xmm2","VEX.128.66.0F38.W0 93 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using qword indices specified in vm64x, gather single-preci-sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VGATHERQPS xmm1, vm64y/f32x4, xmm2","VEX.256.66.0F38.W0 93 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using qword indices specified in vm64y, gather single-preci-sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VINSERTI128 ymm1, ymm2, xmm3/m128, ib","VEX.256.66.0F3A.W0 38 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Insert 128 bits of integer data from xmm3/m128 and the remaining values from ymm2 into ymm1."
"VMOVNTDQA ymm1, m256","VEX.256.66.0F38.WIG 2A /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type."
"VMPSADBW ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F3A.WIG 42 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and ymm3/m128 and writes the results in ymm1. Starting offsets within ymm2 and xmm3/m128 are determined by ib."
"VPABSB ymm1, ymm2/m256","VEX.256.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1."
"VPABSD ymm1, ymm2/m256","VEX.256.66.0F38.WIG 1E /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Compute the absolute value of 32-bit integers in ymm2/m256 and store UNSIGNED result in ymm1."
"VPABSW ymm1, ymm2/m256","VEX.256.66.0F38.WIG 1D /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Compute the absolute value of 16-bit integers in ymm2/m256 and store UNSIGNED result in ymm1."
"VPACKSSDW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 6B /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Converts 8 packed signed doubleword integers from ymm2 and from ymm3/m256 into 16 packed signed word integers in ymm1using signed saturation."
"VPACKSSWB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Converts 16 packed signed word integers from ymm2 and from ymm3/m256 into 32 packed signed byte integers in ymm1 using signed saturation."
"VPACKUSDW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 2B /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Convert 8 packed signed doubleword integers from ymm2 and 8 packed signed doubleword integers from ymm3/m256 into 16 packed unsigned word integers in ymm1 using unsigned saturation."
"VPACKUSWB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Converts 16 signed word integers from ymm2 and 16signed word integers from ymm3/m256 into 32 unsigned byte integers in ymm1 using unsigned saturation."
"VPADDB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG FC /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed byte integers from ymm2, and ymm3/m256 and store in ymm1."
"VPADDD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG FE /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed doubleword integers from ymm2, ymm3/m256 and store in ymm1."
"VPADDQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG D4 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed quadword integers from ymm2, ymm3/m256 and store in ymm1."
"VPADDSB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1."
"VPADDSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1."
"VPADDUSB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG DC /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed unsigned byte integers from ymm2,and ymm3/m256 and store the saturated results in ymm1."
"VPADDUSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG DD /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed unsigned word integers from ymm2,and ymm3/m256 and store the saturated results in ymm1."
"VPADDW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG FD /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed word integers from ymm2, ymm3/m256 and store in ymm1."
"VPALIGNR ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in ib from each intermediate result, and two 16-byte results are stored in ymm1."
"VPAND ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG DB /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Bitwise AND of ymm2, and ymm3/m256 and store result in ymm1."
"VPANDN ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG DF /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Bitwise AND NOT of ymm2, and ymm3/m256 and store result in ymm1."
"VPAVGB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1."
"VPAVGW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1."
"VPBLENDD xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F3A.W0 02 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Select dwords from xmm2 and xmm3/m128 from mask specified in ib and store the values into xmm1."
"VPBLENDD ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F3A.W0 02 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Select dwords from ymm2 and ymm3/m256 from mask specified in ib and store the values into ymm1."
"VPBLENDVB ymm1, ymm2, ymm3/m256, ymm4","VEX.256.66.0F3A.W0 4C /r /is4","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib[7:4]","","Select byte values from ymm2 and ymm3/m256 from mask specified in the high bit of each byte in ymm4 and store the values into ymm1."
"VPBLENDW ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F3A.WIG 0E /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Select words from ymm2 and ymm3/m256 from mask specified in ib and store the values into ymm1."
"VPBROADCASTB xmm1, xmm2/m8","VEX.128.66.0F38.W0 78 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast a byte integer in the source operand to sixteen locations in xmm1."
"VPBROADCASTB ymm1, xmm2/m8","VEX.256.66.0F38.W0 78 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast a byte integer in the source operand to thirty-two locations in ymm1."
"VPBROADCASTD xmm1, xmm2/m32","VEX.128.66.0F38.W0 58 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast a dword integer in the source operand to four locations in xmm1."
"VPBROADCASTD ymm1, xmm2/m32","VEX.256.66.0F38.W0 58 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast a dword integer in the source operand to eight locations in ymm1."
"VPBROADCASTQ xmm1, xmm2/m64","VEX.128.66.0F38.W0 59 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast a qword element in source operand to two locations in xmm1."
"VPBROADCASTQ ymm1, xmm2/m64","VEX.256.66.0F38.W0 59 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast a qword element in source operand to four locations in ymm1."
"VPBROADCASTW xmm1, xmm2/m16","VEX.128.66.0F38.W0 79 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast a word integer in the source operand to eight locations in xmm1."
"VPBROADCASTW ymm1, xmm2/m16","VEX.256.66.0F38.W0 79 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast a word integer in the source operand to sixteen locations in ymm1."
"VPCMPEQB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 74 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed bytes in ymm3/m256 and ymm2 for equality."
"VPCMPEQD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 76 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed doublewords in ymm3/m256 and ymm2 for equality."
"VPCMPEQQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 29 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed quadwords in ymm3/m256 and ymm2 for equality."
"VPCMPEQW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 75 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed words in ymm3/m256 and ymm2 for equality."
"VPCMPGTB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 64 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than."
"VPCMPGTD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 66 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed doubleword integers in ymm2 and ymm3/m256 for greater than."
"VPCMPGTQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 37 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed qwords in ymm2 and ymm3/m256 for greater than."
"VPCMPGTW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 65 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed word integers in ymm2 and ymm3/m256 for greater than."
"VPERM2I128 ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F3A.W0 46 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Permute 128-bit integer data in ymm2 and ymm3/mem using controls from ib and store result in ymm1."
"VPERMD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W0 36 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Permute doublewords in ymm3/m256 using indices in ymm2 and store the result in ymm1."
"VPERMPD ymm1, ymm2/m256, ib","VEX.256.66.0F3A.W1 01 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Permute double-precision floating-point elements in ymm2/m256 using indices in ib and store the result in ymm1."
"VPERMPS ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W0 16 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Permute single-precision floating-point elements in ymm3/m256 using indices in ymm2 and store the result in ymm1."
"VPERMQ ymm1, ymm2/m256, ib","VEX.256.66.0F3A.W1 00 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Permute qwords in ymm2/m256 using indices in ib and store the result in ymm1."
"VPGATHERDD xmm1, vm32x/i32x4, xmm2","VEX.128.66.0F38.W0 90 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using dword indices specified in vm32x, gather dword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERDD ymm1, vm32y/i32x8, ymm2","VEX.256.66.0F38.W0 90 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using dword indices specified in vm32y, gather dword val-ues from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1."
"VPGATHERDQ xmm1, vm32x/i64x2, xmm2","VEX.128.66.0F38.W1 90 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using dword indices specified in vm32x, gather qword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERDQ ymm1, vm32x/i64x4, ymm2","VEX.256.66.0F38.W1 90 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using dword indices specified in vm32x, gather qword val-ues from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1."
"VPGATHERQD xmm1, vm64x/i32x2, xmm2","VEX.128.66.0F38.W0 91 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using qword indices specified in vm64x, gather dword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERQD xmm1, vm64y/i32x4, xmm2","VEX.256.66.0F38.W0 91 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using qword indices specified in vm64y, gather dword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERQQ xmm1, vm64x/i64x2, xmm2","VEX.128.66.0F38.W1 91 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using qword indices specified in vm64x, gather qword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERQQ ymm1, vm64y/i64x4, ymm2","VEX.256.66.0F38.W1 91 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","BaseReg (r): VSIB:base, VectorReg (r): VSIB:index","VEX.vvvv (r, w)","","","Using qword indices specified in vm64y, gather qword val-ues from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1."
"VPHADDD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 02 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add 32-bit signed integers horizontally, pack to ymm1."
"VPHADDSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 03 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add 16-bit signed integers horizontally, pack saturated integers to ymm1."
"VPHADDW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 01 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add 16-bit signed integers horizontally, pack to ymm1."
"VPHSUBD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 06 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract 32-bit signed integers horizontally, pack to ymm1."
"VPHSUBSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 07 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract 16-bit signed integer horizontally, pack saturated integers to ymm1."
"VPHSUBW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 05 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract 16-bit signed integers horizontally, pack to ymm1."
"VPMADDUBSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1."
"VPMADDWD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1."
"VPMASKMOVD xmm1, xmm2, m128","VEX.128.66.0F38.W0 8C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Conditionally load dword values from m128 using mask in xmm2 and store in xmm1."
"VPMASKMOVD ymm1, ymm2, m256","VEX.256.66.0F38.W0 8C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Conditionally load dword values from m256 using mask in ymm2 and store in ymm1."
"VPMASKMOVD m128, xmm1, xmm2","VEX.128.66.0F38.W0 8E /r","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Conditionally store dword values from xmm2 using mask in xmm1."
"VPMASKMOVD m256, ymm1, ymm2","VEX.256.66.0F38.W0 8E /r","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Conditionally store dword values from ymm2 using mask in ymm1."
"VPMASKMOVQ xmm1, xmm2, m128","VEX.128.66.0F38.W1 8C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Conditionally load qword values from m128 using mask in xmm2 and store in xmm1."
"VPMASKMOVQ ymm1, ymm2, m256","VEX.256.66.0F38.W1 8C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Conditionally load qword values from m256 using mask in ymm2 and store in ymm1."
"VPMASKMOVQ m128, xmm1, xmm2","VEX.128.66.0F38.W1 8E /r","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Conditionally store qword values from xmm2 using mask in xmm1."
"VPMASKMOVQ m256, ymm1, ymm2","VEX.256.66.0F38.W1 8E /r","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Conditionally store qword values from ymm2 using mask in ymm1."
"VPMAXSB ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXSD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 3D /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed word integers in ymm3/m256 and ymm2 and store packed maximum values in ymm1."
"VPMAXUB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG DE /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXUD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 3F /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXUW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 3E /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned word integers in ymm3/m256 and ymm2 and store maximum packed values in ymm1."
"VPMINSB ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 38 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1."
"VPMINSD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 39 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed dword integers in ymm2 and ymm3/m128 and store packed minimum values in ymm1."
"VPMINSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG EA /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1."
"VPMINUB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG DA /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1."
"VPMINUD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 3B /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1."
"VPMINUW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 3A /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1."
"VPMOVMSKB r32, ymm1","VEX.256.66.0F.WIG D7 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Move a 32-bit mask of ymm1 to reg. The upper bits of r are zeroed."
"VPMOVSXBD ymm1, xmm2/m64","VEX.256.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1."
"VPMOVSXBQ ymm1, xmm2/m32","VEX.256.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1."
"VPMOVSXBW ymm1, xmm2/m128","VEX.256.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVSXDQ ymm1, xmm2/m128","VEX.256.66.0F38.WIG 25 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in ymm1."
"VPMOVSXWD ymm1, xmm2/m128","VEX.256.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 32-bit integers in ymm1."
"VPMOVSXWQ ymm1, xmm2/m64","VEX.256.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1."
"VPMOVZXBD ymm1, xmm2/m64","VEX.256.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1."
"VPMOVZXBQ ymm1, xmm2/m32","VEX.256.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1."
"VPMOVZXBW ymm1, xmm2/m128","VEX.256.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVZXDQ ymm1, xmm2/m128","VEX.256.66.0F38.WIG 35 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 4 packed 32-bit integers in xmm2/m128 to 4 packed 64-bit integers in ymm1."
"VPMOVZXWD ymm1, xmm2/m128","VEX.256.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 8 packed 16-bit integers xmm2/m128 to 8 packed 32-bit integers in ymm1."
"VPMOVZXWQ ymm1, xmm2/m64","VEX.256.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in xmm1."
"VPMULDQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 28 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256, and store the quadword results in ymm1."
"VPMULHRSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1."
"VPMULHUW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1."
"VPMULHW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1."
"VPMULLD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 40 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed dword signed integers in ymm2 and ymm3/m256 and store the low 32 bits of each product in ymm1."
"VPMULLW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1."
"VPMULUDQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG F4 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256, and store the quadword results in ymm1."
"VPOR ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG EB /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Bitwise OR of ymm2/m256 and ymm3."
"VPSADBW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Computes the absolute differences of the packed unsigned byte integers from ymm3/m256 and ymm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results."
"VPSHUFB ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shuffle bytes in ymm2 according to contents of ymm3/m256."
"VPSHUFD ymm1, ymm2/m256, ib","VEX.256.66.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Shuffle the doublewords in ymm2/m256 based on the encoding in ib and store the result in ymm1."
"VPSHUFHW ymm1, ymm2/m256, ib","VEX.256.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Shuffle the high words in ymm2/m256 based on the encoding in ib and store the result in ymm1."
"VPSHUFLW ymm1, ymm2/m256, ib","VEX.256.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Shuffle the low words in ymm2/m256 based on the encoding in ib and store the result in ymm1."
"VPSIGNB ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 08 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Negate packed byte integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero."
"VPSIGND ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 0A /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Negate packed doubleword integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero."
"VPSIGNW ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.WIG 09 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Negate packed 16-bit integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero."
"VPSLLD ymm1, ymm2, ib","VEX.256.66.0F.WIG 72 /6 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift doublewords in ymm2 left by ib while shifting in 0s."
"VPSLLD ymm1, ymm2, xmm3/m128","VEX.256.66.0F.WIG F2 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLDQ ymm1, ymm2, ib","VEX.256.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift ymm2 left by ib bytes while shifting in 0s and store result in ymm1."
"VPSLLQ ymm1, ymm2, ib","VEX.256.66.0F.WIG 73 /6 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift quadwords in ymm2 left by ib while shifting in 0s."
"VPSLLQ ymm1, ymm2, xmm3/m128","VEX.256.66.0F.WIG F3 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift quadwords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLVD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s."
"VPSLLVD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s."
"VPSLLVQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s."
"VPSLLVQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s."
"VPSLLW ymm1, ymm2, ib","VEX.256.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift words in ymm2 left by ib while shifting in 0s."
"VPSLLW ymm1, ymm2, xmm3/m128","VEX.256.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift words in ymm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSRAD ymm1, ymm2, ib","VEX.256.66.0F.WIG 72 /4 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift doublewords in ymm2 right by ib while shifting in sign bits."
"VPSRAD ymm1, ymm2, xmm3/m128","VEX.256.66.0F.WIG E2 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits."
"VPSRAVD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in sign bits."
"VPSRAVD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in sign bits."
"VPSRAW ymm1, ymm2, ib","VEX.256.66.0F.WIG 71 /4 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift words in ymm2 right by ib while shifting in sign bits."
"VPSRAW ymm1, ymm2, xmm3/m128","VEX.256.66.0F.WIG E1 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits."
"VPSRLD ymm1, ymm2, ib","VEX.256.66.0F.WIG 72 /2 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift doublewords in ymm2 right by ib while shifting in 0s."
"VPSRLD ymm1, ymm2, xmm3/m128","VEX.256.66.0F.WIG D2 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLDQ ymm1, ymm2, ib","VEX.256.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift ymm1 right by ib bytes while shifting in 0s."
"VPSRLQ ymm1, ymm2, ib","VEX.256.66.0F.WIG 73 /2 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift quadwords in ymm2 right by ib while shifting in 0s."
"VPSRLQ ymm1, ymm2, xmm3/m128","VEX.256.66.0F.WIG D3 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLVD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s."
"VPSRLVD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s."
"VPSRLVQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s."
"VPSRLVQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s."
"VPSRLW ymm1, ymm2, ib","VEX.256.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift words in ymm2 right by ib while shifting in 0s."
"VPSRLW ymm1, ymm2, xmm3/m128","VEX.256.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSUBB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed byte integers in ymm3/m256 from ymm2."
"VPSUBD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG FA /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed doubleword integers in ymm3/m256 from ymm2."
"VPSUBQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG FB /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed quadword integers in ymm3/m256 from ymm2."
"VPSUBSB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results."
"VPSUBSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG E9 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results."
"VPSUBUSB ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2 and saturate result."
"VPSUBUSW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG D9 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2 and saturate result."
"VPSUBW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed word integers in ymm3/m256 from ymm2."
"VPUNPCKHBW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 68 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave high-order bytes from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHDQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 6A /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave high-order doublewords from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHQDQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 6D /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave high-order quadword from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHWD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 69 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave high-order words from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLBW ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 60 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave low-order bytes from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLDQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 62 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave low-order doublewords from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLQDQ ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 6C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave low-order quadword from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLWD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 61 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave low-order words from ymm2 and ymm3/m256 into ymm1 register."
"VPXOR ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG EF /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Bitwise XOR of ymm3/m256 and ymm2."
