"Instruction","Opcode","Valid 64-bit","Valid 32-bit","Valid 16-bit","Feature Flags","Operand 1","Operand 2","Operand 3","Operand 4","Tuple Type","Description"
"VADDPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed double-precision floating-point values from xmm3/mem to xmm2 and store result in xmm1."
"VADDPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed double-precision floating-point values from ymm3/mem to ymm2 and store result in ymm1."
"VADDPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed single-precision floating-point values from xmm3/m128 to xmm2 and store result in xmm1."
"VADDPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed single-precision floating-point values from ymm3/m256 to ymm2 and store result in ymm1."
"VADDSD xmm1, xmm2, xmm3/m64","VEX.LIG.F2.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add the low double-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1."
"VADDSS xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add the low single-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1."
"VADDSUBPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG D0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add/subtract packed double-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1."
"VADDSUBPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG D0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add / subtract packed double-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1."
"VADDSUBPS xmm1, xmm2, xmm3/m128","VEX.128.F2.0F.WIG D0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add/subtract single-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1."
"VADDSUBPS ymm1, ymm2, ymm3/m256","VEX.256.F2.0F.WIG D0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add / subtract single-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1."
"VANDNPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 55 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical AND NOT of packed double-precision floating-point values in xmm2 and xmm3/mem."
"VANDNPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 55 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical AND NOT of packed double-precision floating-point values in ymm2 and ymm3/mem."
"VANDNPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 55 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical AND NOT of packed single-precision floating-point values in xmm2 and xmm3/mem."
"VANDNPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 55 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical AND NOT of packed single-precision floating-point values in ymm2 and ymm3/mem."
"VANDPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 54 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical AND of packed double-precision floating-point values in xmm2 and xmm3/mem."
"VANDPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 54 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical AND of packed double-precision floating-point values in ymm2 and ymm3/mem."
"VANDPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 54 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical AND of packed single-precision floating-point values in xmm2 and xmm3/mem."
"VANDPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 54 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical AND of packed single-precision floating-point values in ymm2 and ymm3/mem."
"VBLENDPD xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F3A.WIG 0D /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib[3:0]","","Select packed double-precision floating-point Values from xmm2 and xmm3/m128 from mask in ib and store the values in xmm1."
"VBLENDPD ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F3A.WIG 0D /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib[3:0]","","Select packed double-precision floating-point Values from ymm2 and ymm3/m256 from mask in ib and store the values in ymm1."
"VBLENDPS xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F3A.WIG 0C /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Select packed single-precision floating-point values from xmm2 and xmm3/m128 from mask in ib and store the values in xmm1."
"VBLENDPS ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F3A.WIG 0C /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Select packed single-precision floating-point values from ymm2 and ymm3/m256 from mask in ib and store the values in ymm1."
"VBLENDVPD xmm1, xmm2, xmm3/m128, xmm4","VEX.128.66.0F3A.W0 4B /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib[7:4]","","Conditionally copy double-precision floating-point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the mask operand, xmm4."
"VBLENDVPD ymm1, ymm2, ymm3/m256, ymm4","VEX.256.66.0F3A.W0 4B /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib[7:4]","","Conditionally copy double-precision floating-point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the mask operand, ymm4."
"VBLENDVPS xmm1, xmm2, xmm3/m128, xmm4","VEX.128.66.0F3A.W0 4A /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib[7:4]","","Conditionally copy single-precision floating-point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the specified mask operand, xmm4."
"VBLENDVPS ymm1, ymm2, ymm3/m256, ymm4","VEX.256.66.0F3A.W0 4A /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib[7:4]","","Conditionally copy single-precision floating-point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the specified mask register, ymm4."
"VBROADCASTF128 ymm1, m128","VEX.256.66.0F38.W0 1A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast 128 bits of floating-point data in mem to low and high 128-bits in ymm1."
"VBROADCASTSD ymm1, m64","VEX.256.66.0F38.W0 19 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast double-precision floating-point element in mem to four locations in ymm1."
"VBROADCASTSS ymm1, m32","VEX.256.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Broadcast single-precision floating-point element in mem to eight locations in ymm1."
"VCMPPD xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Compare packed double-precision floating-point values in xmm3/m128 and xmm2 using bits 4:0 of ib as a comparison predicate."
"VCMPPD ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Compare packed double-precision floating-point values in ymm3/m256 and ymm2 using bits 4:0 of ib as a comparison predicate."
"VCMPPS xmm1, xmm2, xmm3/m128, ib","VEX.128.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Compare packed single-precision floating-point values in xmm3/m128 and xmm2 using bits 4:0 of ib as a comparison predicate."
"VCMPPS ymm1, ymm2, ymm3/m256, ib","VEX.256.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Compare packed single-precision floating-point values in ymm3/m256 and ymm2 using bits 4:0 of ib as a comparison predicate."
"VCMPSD xmm1, xmm2, xmm3/m64, ib","VEX.LIG.F2.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Compare low double-precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of ib as comparison predicate."
"VCMPSS xmm1, xmm2, xmm3/m32, ib","VEX.LIG.F3.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Compare low single-precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of ib as comparison predicate."
"VCOMISD xmm1, xmm2/m64","VEX.LIG.66.0F.WIG 2F /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VCOMISS xmm1, xmm2/m32","VEX.LIG.0F.WIG 2F /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VCVTDQ2PD xmm1, xmm2/m64","VEX.128.F3.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert two packed signed doubleword integers from xmm2/mem to two packed double-precision floating-point values in xmm1."
"VCVTDQ2PD ymm1, xmm2/m128","VEX.256.F3.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert four packed signed doubleword integers from xmm2/mem to four packed double-precision floating-point values in ymm1."
"VCVTDQ2PS xmm1, xmm2/m128","VEX.128.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert four packed signed doubleword integers from xmm2/mem to four packed single-precision floating-point values in xmm1."
"VCVTDQ2PS ymm1, ymm2/m256","VEX.256.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert eight packed signed doubleword integers from ymm2/mem to eight packed single-precision floating-point values in ymm1."
"VCVTPD2DQ xmm1, xmm2/m128","VEX.128.F2.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1."
"VCVTPD2DQ xmm1, ymm2/m256","VEX.256.F2.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert four packed double-precision floating-point values in ymm2/mem to four signed doubleword integers in xmm1."
"VCVTPD2PS xmm1, xmm2/m128","VEX.128.66.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert two packed double-precision floating-point values in xmm2/mem to two single-precision floating-point values in xmm1."
"VCVTPD2PS xmm1, ymm2/m256","VEX.256.66.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert four packed double-precision floating-point values in ymm2/mem to four single-precision floating-point values in xmm1."
"VCVTPS2DQ xmm1, xmm2/m128","VEX.128.66.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1."
"VCVTPS2DQ ymm1, ymm2/m256","VEX.256.66.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert eight packed single-precision floating-point values from ymm2/mem to eight packed signed doubleword values in ymm1."
"VCVTPS2PD xmm1, xmm2/m64","VEX.128.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert two packed single-precision floating-point values in xmm2/m64 to two packed double-precision floating-point values in xmm1."
"VCVTPS2PD ymm1, xmm2/m128","VEX.256.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert four packed single-precision floating-point values in xmm2/m128 to four packed double-precision floating-point values in ymm1."
"VCVTSD2SI rw, xmm1/m64","VEX.LIG.F2.0F.W0 2D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer r."
"VCVTSD2SI rw, xmm1/m64","VEX.LIG.F2.0F.W1 2D /r","Valid","Invalid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer signextended into r."
"VCVTSD2SS xmm1, xmm2, xmm3/m64","VEX.LIG.F2.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Convert one double-precision floating-point value in xmm3/m64 to one single-precision floating-point value and merge with high bits in xmm2."
"VCVTSI2SD xmm1, xmm2, rw/mw","VEX.LIG.F2.0F.W0 2A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Convert one signed doubleword integer from r/m to one double-precision floating-point value in xmm1."
"VCVTSI2SD xmm1, xmm2, rw/mw","VEX.LIG.F2.0F.W1 2A /r","Valid","Invalid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Convert one signed quadword integer from r/m to one double-precision floating-point value in xmm1."
"VCVTSI2SS xmm1, xmm2, rw/mw","VEX.LIG.F3.0F.W0 2A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Convert one signed doubleword integer from r/m to one single-precision floating-point value in xmm1."
"VCVTSI2SS xmm1, xmm2, rw/mw","VEX.LIG.F3.0F.W1 2A /r","Valid","Invalid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Convert one signed quadword integer from r/m to one single-precision floating-point value in xmm1."
"VCVTSS2SD xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Convert one single-precision floating-point value in xmm3/m32 to one double-precision floating-point value and merge with high bits of xmm2."
"VCVTSS2SI rw, xmm1/m32","VEX.LIG.F3.0F.W0 2D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r."
"VCVTSS2SI rw, xmm1/m32","VEX.LIG.F3.0F.W1 2D /r","Valid","Invalid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r."
"VCVTTPD2DQ xmm1, xmm2/m128","VEX.128.66.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1 using truncation."
"VCVTTPD2DQ xmm1, ymm2/m256","VEX.256.66.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert four packed double-precision floating-point values in ymm2/mem to four signed doubleword integers in xmm1 using truncation."
"VCVTTPS2DQ xmm1, xmm2/m128","VEX.128.F3.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation."
"VCVTTPS2DQ ymm1, ymm2/m256","VEX.256.F3.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert eight packed single-precision floating-point values from ymm2/mem to eight packed signed doubleword values in ymm1 using truncation."
"VCVTTSD2SI rw, xmm1/m64","VEX.LIG.F2.0F.W0 2C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r using truncation."
"VCVTTSD2SI rw, xmm1/m64","VEX.LIG.F2.0F.W1 2C /r","Valid","Invalid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Fixed","Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer in r using truncation."
"VCVTTSS2SI rw, xmm1/m32","VEX.LIG.F3.0F.W0 2C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r using truncation."
"VCVTTSS2SI rw, xmm1/m32","VEX.LIG.F3.0F.W1 2C /r","Valid","Invalid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r using truncation."
"VDIVPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Divide packed double-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/mem."
"VDIVPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Divide packed double-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/mem."
"VDIVPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Divide packed single-precision floating-point values in xmm2 by packed single-precision floating-point values in xmm3/mem."
"VDIVPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Divide packed single-precision floating-point values in ymm2 by packed single-precision floating-point values in ymm3/mem."
"VDIVSD xmm1, xmm2, xmm3/m64","VEX.LIG.F2.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Divide low double-precision floating-point value in xmm2 by low double-precision floating-point value in xmm3/m64."
"VDIVSS xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Divide low single-precision floating-point value in xmm2 by low single-precision floating-point value in xmm3/m32."
"VDPPD xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F3A.WIG 41 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Selectively multiply packed DP floating-point values from xmm2 with packed DP floating-point values from xmm3, add and selectively store the packed DP floating-point values to xmm1."
"VDPPS xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F3A.WIG 40 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Multiply packed SP floating point values from xmm1 with packed SP floating point values from xmm2/mem selectively add and store to xmm1."
"VDPPS ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F3A.WIG 40 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Multiply packed single-precision floating-point values from ymm2 with packed SP floating point values from ymm3/mem, selectively add pairs of elements and store to ymm1."
"VEXTRACTF128 xmm1/m128, ymm2, ib","VEX.256.66.0F3A.W0 19 /r ib","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","ib","","","Extract 128 bits of packed floating-point values from ymm2 and store results in xmm1/m128."
"VEXTRACTPS r32/m32, xmm1, ib","VEX.128.66.0F3A.WIG 17 /r ib","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","ib","","","Extract one single-precision floating-point value from xmm1 at the offset specified by ib and store the result in reg or m32. Zero extend the results in 64-bit register if applicable."
"VHADDPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 7C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Horizontal add packed double-precision floating-point values from xmm2 and xmm3/mem."
"VHADDPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 7C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Horizontal add packed double-precision floating-point values from ymm2 and ymm3/mem."
"VHADDPS xmm1, xmm2, xmm3/m128","VEX.128.F2.0F.WIG 7C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Horizontal add packed single-precision floating-point values from xmm2 and xmm3/mem."
"VHADDPS ymm1, ymm2, ymm3/m256","VEX.256.F2.0F.WIG 7C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Horizontal add packed single-precision floating-point values from ymm2 and ymm3/mem."
"VHSUBPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 7D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Horizontal subtract packed double-precision floating-point values from xmm2 and xmm3/mem."
"VHSUBPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 7D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Horizontal subtract packed double-precision floating-point values from ymm2 and ymm3/mem."
"VHSUBPS xmm1, xmm2, xmm3/m128","VEX.128.F2.0F.WIG 7D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Horizontal subtract packed single-precision floating-point values from xmm2 and xmm3/mem."
"VHSUBPS ymm1, ymm2, ymm3/m256","VEX.256.F2.0F.WIG 7D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Horizontal subtract packed single-precision floating-point values from ymm2 and ymm3/mem."
"VINSERTF128 ymm1, ymm2, xmm3/m128, ib","VEX.256.66.0F3A.W0 18 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Insert 128 bits of packed floating-point values from xmm3/m128 and the remaining values from ymm2 into ymm1."
"VINSERTPS xmm1, xmm2, xmm3/m32, ib","VEX.128.66.0F3A.WIG 21 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Insert a single-precision floating-point value selected by ib from xmm3/m32 and merge with values in xmm2 at the specified destination element specified by ib and write out the result and zero out destination elements in xmm1 as indicated in ib."
"VLDDQU xmm1, m128","VEX.128.F2.0F.WIG F0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Load unaligned packed integer values from mem to xmm1."
"VLDDQU ymm1, m256","VEX.256.F2.0F.WIG F0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Load unaligned packed integer values from mem to ymm1."
"VLDMXCSR m32","VEX.LZ.0F.WIG AE /2","Valid","Valid","Invalid","AVX","ModRM:r/m (r, ModRM:[7:6] must not be 11b)","","","","","Load MXCSR register from m32."
"VMASKMOVDQU xmm1, xmm2","VEX.128.66.0F.WIG F7 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","RDI (r)","","","Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI."
"VMASKMOVPD xmm1, xmm2, m128","VEX.128.66.0F38.W0 2D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Conditionally load packed double-precision values from m128 using mask in xmm2 and store in xmm1."
"VMASKMOVPD ymm1, ymm2, m256","VEX.256.66.0F38.W0 2D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Conditionally load packed double-precision values from m256 using mask in ymm2 and store in ymm1."
"VMASKMOVPD m128, xmm1, xmm2","VEX.128.66.0F38.W0 2F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Conditionally store packed double-precision values from xmm2 using mask in xmm1."
"VMASKMOVPD m256, ymm1, ymm2","VEX.256.66.0F38.W0 2F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Conditionally store packed double-precision values from ymm2 using mask in ymm1."
"VMASKMOVPS xmm1, xmm2, m128","VEX.128.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Conditionally load packed single-precision values from m128 using mask in xmm2 and store in xmm1."
"VMASKMOVPS ymm1, ymm2, m256","VEX.256.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Conditionally load packed single-precision values from m256 using mask in ymm2 and store in ymm1."
"VMASKMOVPS m128, xmm1, xmm2","VEX.128.66.0F38.W0 2E /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Conditionally store packed single-precision values from xmm2 using mask in xmm1."
"VMASKMOVPS m256, ymm1, ymm2","VEX.256.66.0F38.W0 2E /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Conditionally store packed single-precision values from ymm2 using mask in ymm1."
"VMAXPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the maximum double-precision floating-point values between xmm2 and xmm3/m128."
"VMAXPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the maximum packed double-precision floating-point values between ymm2 and ymm3/m256."
"VMAXPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the maximum single-precision floating-point values between xmm2 and xmm3/mem."
"VMAXPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the maximum single-precision floating-point values between ymm2 and ymm3/mem."
"VMAXSD xmm1, xmm2, xmm3/m64","VEX.LIG.F2.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the maximum scalar double-precision floating-point value between xmm3/m64 and xmm2."
"VMAXSS xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the maximum scalar single-precision floating-point value between xmm3/m32 and xmm2."
"VMINPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the minimum double-precision floating-point values between xmm2 and xmm3/mem."
"VMINPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the minimum packed double-precision floating-point values between ymm2 and ymm3/mem."
"VMINPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the minimum single-precision floating-point values between xmm2 and xmm3/mem."
"VMINPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the minimum single double-precision floating-point values between ymm2 and ymm3/mem."
"VMINSD xmm1, xmm2, xmm3/m64","VEX.LIG.F2.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the minimum scalar double-precision floating-point value between xmm3/m64 and xmm2."
"VMINSS xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the minimum scalar single-precision floating-point value between xmm3/m32 and xmm2."
"VMOVAPD xmm1, xmm2/m128","VEX.128.66.0F.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move aligned packed double-precision floating-point values from xmm2/mem to xmm1."
"VMOVAPD ymm1, ymm2/m256","VEX.256.66.0F.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move aligned packed double-precision floating-point values from ymm2/mem to ymm1."
"VMOVAPD xmm2/m128, xmm1","VEX.128.66.0F.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move aligned packed double-precision floating-point values from xmm1 to xmm2/mem."
"VMOVAPD ymm2/m256, ymm1","VEX.256.66.0F.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move aligned packed double-precision floating-point values from ymm1 to ymm2/mem."
"VMOVAPS xmm1, xmm2/m128","VEX.128.0F.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move aligned packed single-precision floating-point values from xmm2/mem to xmm1."
"VMOVAPS ymm1, ymm2/m256","VEX.256.0F.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move aligned packed single-precision floating-point values from ymm2/mem to ymm1."
"VMOVAPS xmm2/m128, xmm1","VEX.128.0F.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move aligned packed single-precision floating-point values from xmm1 to xmm2/mem."
"VMOVAPS ymm2/m256, ymm1","VEX.256.0F.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move aligned packed single-precision floating-point values from ymm1 to ymm2/mem."
"VMOVD xmm1, rw/mw","VEX.128.66.0F.W0 6E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move doubleword from r/m to xmm1."
"VMOVD rw/mw, xmm1","VEX.128.66.0F.W0 7E /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move doubleword from xmm1 register to r/m."
"VMOVDDUP xmm1, xmm2/m64","VEX.128.F2.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move double-precision floating-point value from xmm2/m64 and duplicate into xmm1."
"VMOVDDUP ymm1, ymm2/m256","VEX.256.F2.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move even index double-precision floating-point values from ymm2/mem and duplicate each element into ymm1."
"VMOVDQA xmm1, xmm2/m128","VEX.128.66.0F.WIG 6F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move aligned packed integer values from xmm2/mem to xmm1."
"VMOVDQA ymm1, ymm2/m256","VEX.256.66.0F.WIG 6F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move aligned packed integer values from ymm2/mem to ymm1."
"VMOVDQA xmm2/m128, xmm1","VEX.128.66.0F.WIG 7F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move aligned packed integer values from xmm1 to xmm2/mem."
"VMOVDQA ymm2/m256, ymm1","VEX.256.66.0F.WIG 7F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move aligned packed integer values from ymm1 to ymm2/mem."
"VMOVDQU xmm1, xmm2/m128","VEX.128.F3.0F.WIG 6F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move unaligned packed integer values from xmm2/m128 to xmm1."
"VMOVDQU ymm1, ymm2/m256","VEX.256.F3.0F.WIG 6F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move unaligned packed integer values from ymm2/m256 to ymm1."
"VMOVDQU xmm2/m128, xmm1","VEX.128.F3.0F.WIG 7F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (r)","ModRM:reg (w)","","","","Move unaligned packed integer values from xmm1 to xmm2/m128."
"VMOVDQU ymm2/m256, ymm1","VEX.256.F3.0F.WIG 7F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (r)","ModRM:reg (w)","","","","Move unaligned packed integer values from ymm1 to ymm2/m256."
"VMOVHPD xmm2, xmm1, m64","VEX.128.66.0F.WIG 16 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Merge double-precision floating-point value from m64 and the low quadword of xmm1."
"VMOVHPD m64, xmm1","VEX.128.66.0F.WIG 17 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move double-precision floating-point value from high quadword of xmm1 to m64."
"VMOVHPS xmm2, xmm1, m64","VEX.128.0F.WIG 16 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Merge two packed single-precision floating-point values from m64 and the low quadword of xmm1."
"VMOVHPS m64, xmm1","VEX.128.0F.WIG 17 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move two packed single-precision floating-point values from high quadword of xmm1 to m64."
"VMOVLPD xmm2, xmm1, m64","VEX.128.66.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Merge double-precision floating-point value from m64 and the high quadword of xmm1."
"VMOVLPD m64, xmm1","VEX.128.66.0F.WIG 13 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move double-precision floating-point value from low quadword of xmm1 to m64."
"VMOVLPS xmm2, xmm1, m64","VEX.128.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Merge two packed single-precision floating-point values from m64 and the high quadword of xmm1."
"VMOVLPS m64, xmm1","VEX.128.0F.WIG 13 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move two packed single-precision floating-point values from low quadword of xmm1 to m64."
"VMOVMSKPD rw, xmm2","VEX.128.66.0F.WIG 50 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Extract 2-bit sign mask from xmm2 and store in reg. The upper bits of r are zeroed."
"VMOVMSKPD rw, ymm2","VEX.256.66.0F.WIG 50 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Extract 4-bit sign mask from ymm2 and store in reg. The upper bits of r are zeroed."
"VMOVMSKPS rw, xmm2","VEX.128.0F.WIG 50 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Extract 4-bit sign mask from xmm2 and store in reg. The upper bits of r are zeroed."
"VMOVMSKPS rw, ymm2","VEX.256.0F.WIG 50 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Extract 8-bit sign mask from ymm2 and store in reg. The upper bits of r are zeroed."
"VMOVNTDQ m128, xmm1","VEX.128.66.0F.WIG E7 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move packed integer values in xmm1 to m128 using non-temporal hint."
"VMOVNTDQ m256, ymm1","VEX.256.66.0F.WIG E7 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move packed integer values in ymm1 to m256 using non-temporal hint."
"VMOVNTDQA xmm1, m128","VEX.128.66.0F38.WIG 2A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move double quadword from m128 to xmm using non-temporal hint if WC memory type."
"VMOVNTPD m128, xmm1","VEX.128.66.0F.WIG 2B /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move packed double-precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPD m256, ymm1","VEX.256.66.0F.WIG 2B /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move packed double-precision values in ymm1 to m256 using non-temporal hint."
"VMOVNTPS m128, xmm1","VEX.128.0F.WIG 2B /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move packed single-precision values xmm1 to mem using non-temporal hint."
"VMOVNTPS m256, ymm1","VEX.256.0F.WIG 2B /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move packed single-precision values ymm1 to mem using non-temporal hint."
"VMOVQ xmm1, rw/mw","VEX.128.66.0F.W1 6E /r","Valid","Invalid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move quadword from r/m to xmm1."
"VMOVQ rw/mw, xmm1","VEX.128.66.0F.W1 7E /r","Valid","Invalid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move quadword from xmm1 register to r/m."
"VMOVQ xmm1, xmm2/m64","VEX.128.F3.0F.WIG 7E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move quadword from xmm2 to xmm1."
"VMOVQ xmm1/m64, xmm2","VEX.128.66.0F.WIG D6 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move quadword from xmm2 register to xmm1/m64."
"VMOVSD xmm1, m64","VEX.LIG.F2.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Load scalar double-precision floating-point value from m64 to xmm1 register."
"VMOVSD xmm1, xmm2, xmm3","VEX.LIG.F2.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Merge scalar double-precision floating-point value from xmm2 and xmm3 to xmm1 register."
"VMOVSD m64, xmm1","VEX.LIG.F2.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Store scalar double-precision floating-point value from xmm1 register to m64."
"VMOVSD xmm1, xmm2, xmm3","VEX.LIG.F2.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Merge scalar double-precision floating-point value from xmm2 and xmm3 registers to xmm1."
"VMOVSHDUP xmm1, xmm2/m128","VEX.128.F3.0F.WIG 16 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move odd index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1."
"VMOVSHDUP ymm1, ymm2/m256","VEX.256.F3.0F.WIG 16 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move odd index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1."
"VMOVSLDUP xmm1, xmm2/m128","VEX.128.F3.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move even index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1."
"VMOVSLDUP ymm1, ymm2/m256","VEX.256.F3.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move even index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1."
"VMOVSS xmm1, m32","VEX.LIG.F3.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Load scalar single-precision floating-point value from m32 to xmm1 register."
"VMOVSS xmm1, xmm2, xmm3","VEX.LIG.F3.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Merge scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register"
"VMOVSS m32, xmm1","VEX.LIG.F3.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move scalar single-precision floating-point value from xmm1 register to m32."
"VMOVSS xmm1, xmm2, xmm3","VEX.LIG.F3.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","","","Move scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register."
"VMOVUPD xmm1, xmm2/m128","VEX.128.66.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move unaligned packed double-precision floating-point from xmm2/mem to xmm1."
"VMOVUPD ymm1, ymm2/m256","VEX.256.66.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move unaligned packed double-precision floating-point from ymm2/mem to ymm1."
"VMOVUPD xmm2/m128, xmm1","VEX.128.66.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move unaligned packed double-precision floating-point from xmm1 to xmm2/mem."
"VMOVUPD ymm2/m256, ymm1","VEX.256.66.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move unaligned packed double-precision floating-point from ymm1 to ymm2/mem."
"VMOVUPS xmm1, xmm2/m128","VEX.128.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move unaligned packed single-precision floating-point from xmm2/mem to xmm1."
"VMOVUPS ymm1, ymm2/m256","VEX.256.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move unaligned packed single-precision floating-point from ymm2/mem to ymm1."
"VMOVUPS xmm2/m128, xmm1","VEX.128.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move unaligned packed single-precision floating-point from xmm1 to xmm2/mem."
"VMOVUPS ymm2/m256, ymm1","VEX.256.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","","","","Move unaligned packed single-precision floating-point from ymm1 to ymm2/mem."
"VMPSADBW xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F3A.WIG 42 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and xmm3/m128 and writes the results in xmm1. Starting offsets within xmm2 and xmm3/m128 are determined by ib."
"VMULPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply packed double-precision floating-point values in xmm3/m128 with xmm2 and store result in xmm1."
"VMULPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply packed double-precision floating-point values in ymm3/m256 with ymm2 and store result in ymm1."
"VMULPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply packed single-precision floating-point values in xmm3/m128 with xmm2 and store result in xmm1."
"VMULPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply packed single-precision floating-point values in ymm3/m256 with ymm2 and store result in ymm1."
"VMULSD xmm1, xmm2, xmm3/m64","VEX.LIG.F2.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the low double-precision floating-point value in xmm3/m64 by low double-precision floating-point value in xmm2."
"VMULSS xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the low single-precision floating-point value in xmm3/m32 by the low single-precision floating-point value in xmm2."
"VORPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 56 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical OR of packed double-precision floating-point values in xmm2 and xmm3/mem."
"VORPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 56 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical OR of packed double-precision floating-point values in ymm2 and ymm3/mem."
"VORPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 56 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical OR of packed single-precision floating-point values in xmm2 and xmm3/mem."
"VORPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 56 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical OR of packed single-precision floating-point values in ymm2 and ymm3/mem."
"VPABSB xmm1, xmm2/m128","VEX.128.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1."
"VPABSD xmm1, xmm2/m128","VEX.128.66.0F38.WIG 1E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Compute the absolute value of 32-bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"VPABSW xmm1, xmm2/m128","VEX.128.66.0F38.WIG 1D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"VPACKSSDW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 6B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Converts 4 packed signed doubleword integers from xmm2 and from xmm3/m128 into 8 packed signed word integers in xmm1 using signed saturation."
"VPACKSSWB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Converts 8 packed signed word integers from xmm2 and from xmm3/m128 into 16 packed signed byte integers in xmm1 using signed saturation."
"VPACKUSDW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 2B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Convert 4 packed signed doubleword integers from xmm2 and 4 packed signed doubleword integers from xmm3/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation."
"VPACKUSWB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Converts 8 signed word integers from xmm2 and 8 signed word integers from xmm3/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation."
"VPADDB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG FC /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed byte integers from xmm2, and xmm3/m128 and store in xmm1."
"VPADDD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG FE /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed doubleword integers from xmm2, xmm3/m128 and store in xmm1."
"VPADDQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG D4 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed quadword integers from xmm2, xmm3/m128 and store in xmm1."
"VPADDSB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed signed byte integers from xmm3/m128 and xmm2 saturate the results."
"VPADDSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed signed word integers from xmm3/m128 and xmm2 and saturate the results."
"VPADDUSB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG DC /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed unsigned byte integers from xmm3/m128 to xmm2 and saturate the results."
"VPADDUSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG DD /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed unsigned word integers from xmm3/m128 to xmm2 and saturate the results."
"VPADDW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG FD /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add packed word integers from xmm2, xmm3/m128 and store in xmm1."
"VPALIGNR xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Concatenate xmm2 and xmm3/m128, extract byte aligned result shifted to the right by constant value in ib and result is stored in xmm1."
"VPAND xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG DB /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Bitwise AND of xmm3/m128 and xmm."
"VPANDN xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG DF /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Bitwise AND NOT of xmm3/m128 and xmm2."
"VPAVGB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Average packed unsigned byte integers from xmm3/m128 and xmm2 with rounding."
"VPAVGW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Average packed unsigned word integers from xmm3/m128 and xmm2 with rounding."
"VPBLENDVB xmm1, xmm2, xmm3/m128, xmm4","VEX.128.66.0F3A.W0 4C /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib[7:4]","","Select byte values from xmm2 and xmm3/m128 using mask bits in the specified mask register, xmm4, and store the values into xmm1."
"VPBLENDW xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F3A.WIG 0E /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Select words from xmm2 and xmm3/m128 from mask specified in ib and store the values into xmm1."
"VPCMPEQB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 74 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed bytes in xmm3/m128 and xmm2 for equality."
"VPCMPEQD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 76 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed doublewords in xmm3/m128 and xmm2 for equality."
"VPCMPEQQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed quadwords in xmm3/m128 and xmm2 for equality."
"VPCMPEQW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 75 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed words in xmm3/m128 and xmm2 for equality."
"VPCMPESTRI xmm1, xmm2/m128, ib","VEX.128.66.0F3A.WIG 61 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","ib","","","Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX."
"VPCMPESTRM xmm1, xmm2/m128, ib","VEX.128.66.0F3A.WIG 60 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","ib","","","Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0."
"VPCMPGTB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 64 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than."
"VPCMPGTD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 66 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed doubleword integers in xmm2 and xmm3/m128 for greater than."
"VPCMPGTQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 37 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed qwords in xmm2 and xmm3/m128 for greater than."
"VPCMPGTW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 65 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed word integers in xmm2 and xmm3/m128 for greater than."
"VPCMPISTRI xmm1, xmm2/m128, ib","VEX.128.66.0F3A.WIG 63 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","ib","","","Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX."
"VPCMPISTRM xmm1, xmm2/m128, ib","VEX.128.66.0F3A.WIG 62 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","ib","","","Perform a packed comparison of string data with implicit lengths, generating a Mask, and storing the result in XMM0."
"VPERM2F128 ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F3A.W0 06 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Permute 128-bit floating-point fields in ymm2 and ymm3/mem using controls from ib and store result in ymm1."
"VPERMILPD xmm1, xmm2/m128, ib","VEX.128.66.0F3A.W0 05 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Permute double-precision floating-point values in xmm2/m128 using controls from ib."
"VPERMILPD ymm1, ymm2/m256, ib","VEX.256.66.0F3A.W0 05 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Permute double-precision floating-point values in ymm2/m256 using controls from ib."
"VPERMILPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.W0 0D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Permute double-precision floating-point values in xmm2 using controls from xmm3/m128 and store result in xmm1."
"VPERMILPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W0 0D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Permute double-precision floating-point values in ymm2 using controls from ymm3/m256 and store result in ymm1."
"VPERMILPS xmm1, xmm2/m128, ib","VEX.128.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Permute single-precision floating-point values in xmm2/m128 using controls from ib and store result in xmm1."
"VPERMILPS ymm1, ymm2/m256, ib","VEX.256.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Permute single-precision floating-point values in ymm2/m256 using controls from ib and store result in ymm1."
"VPERMILPS xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Permute single-precision floating-point values in xmm2 using controls from xmm3/m128 and store result in xmm1."
"VPERMILPS ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Permute single-precision floating-point values in ymm2 using controls from ymm3/m256 and store result in ymm1."
"VPEXTRB r32/m8, xmm2, ib","VEX.128.66.0F3A.W0 14 /r ib","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","ib","","","Extract a byte integer value from xmm2 at the source byte offset specified by ib into reg or m8. The upper bits of r are zeroed."
"VPEXTRD r32/m32, xmm2, ib","VEX.128.66.0F3A.W0 16 /r ib","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","ib","","","Extract a dword integer value from xmm2 at the source dword offset specified by ib into r/m."
"VPEXTRQ r64/m64, xmm2, ib","VEX.128.66.0F3A.W1 16 /r ib","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","ib","","","Extract a qword integer value from xmm2 at the source dword offset specified by ib into r/m."
"VPEXTRW r32/m16, xmm2, ib","VEX.128.66.0F3A.W0 15 /r ib","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","ib","","","Extract a word integer value from xmm2 at the source word offset specified by ib into reg or m16. The upper bits of r are zeroed."
"VPEXTRW r32, xmm1, ib","VEX.128.66.0F.W0 C5 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Extract the word specified by ib from xmm1 and move it to reg, bits 15:0. Zero-extend the result. The upper bits of r are zeroed."
"VPHADDD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 02 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add 32-bit integers horizontally, pack to xmm1."
"VPHADDSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 03 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add 16-bit signed integers horizontally, pack saturated integers to xmm1."
"VPHADDW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 01 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Add 16-bit integers horizontally, pack to xmm1."
"VPHMINPOSUW xmm1, xmm2/m128","VEX.128.66.0F38.WIG 41 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second-lowest word of xmm1."
"VPHSUBD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 06 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract 32-bit signed integers horizontally, pack to xmm1."
"VPHSUBSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 07 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1."
"VPHSUBW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 05 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract 16-bit signed integers horizontally, pack to xmm1."
"VPINSRB xmm1, xmm2, r32/m8, ib","VEX.128.66.0F3A.W0 20 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Merge a byte integer value from r/m8 and rest from xmm2 into xmm1 at the byte offset in ib."
"VPINSRD xmm1, xmm2, r32/m32, ib","VEX.128.66.0F3A.W0 22 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Insert a dword integer value from r/m32 and rest from xmm2 into xmm1 at the dword offset in ib."
"VPINSRQ xmm1, xmm2, r64/m64, ib","VEX.128.66.0F3A.W1 22 /r ib","Valid","Invalid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Insert a qword integer value from r/m64 and rest from xmm2 into xmm1 at the qword offset in ib."
"VPINSRW xmm1, xmm2, r32/m16, ib","VEX.128.66.0F.W0 C4 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Insert a word integer value from r/m and rest from xmm2 into xmm1 at the word offset in ib."
"VPMADDUBSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1."
"VPMADDWD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1."
"VPMAXSB xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXSD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 3D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed word integers in xmm3/m128 and xmm2 and store packed maximum values in xmm1."
"VPMAXUB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG DE /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXUD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 3F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXUW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 3E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned word integers in xmm3/m128 and xmm2 and store maximum packed values in xmm1."
"VPMINSB xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 38 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINSD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 39 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG EA /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed signed word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1."
"VPMINUB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG DA /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINUD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 3B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINUW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 3A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1."
"VPMOVMSKB r32, xmm1","VEX.128.66.0F.WIG D7 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Move a byte mask of xmm1 to reg. The upper bits of r are zeroed."
"VPMOVSXBD xmm1, xmm2/m32","VEX.128.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"VPMOVSXBQ xmm1, xmm2/m16","VEX.128.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"VPMOVSXBW xmm1, xmm2/m64","VEX.128.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"VPMOVSXDQ xmm1, xmm2/m64","VEX.128.66.0F38.WIG 25 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"VPMOVSXWD xmm1, xmm2/m64","VEX.128.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"VPMOVSXWQ xmm1, xmm2/m32","VEX.128.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"VPMOVZXBD xmm1, xmm2/m32","VEX.128.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"VPMOVZXBQ xmm1, xmm2/m16","VEX.128.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"VPMOVZXBW xmm1, xmm2/m64","VEX.128.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"VPMOVZXDQ xmm1, xmm2/m64","VEX.128.66.0F38.WIG 35 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"VPMOVZXWD xmm1, xmm2/m64","VEX.128.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"VPMOVZXWQ xmm1, xmm2/m32","VEX.128.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"VPMULDQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128, and store the quadword results in xmm1."
"VPMULHRSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1."
"VPMULHUW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1."
"VPMULHW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1."
"VPMULLD xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 40 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1."
"VPMULLW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1."
"VPMULUDQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG F4 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128, and store the quadword results in xmm1."
"VPOR xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG EB /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Bitwise OR of xmm2/m128 and xmm3."
"VPSADBW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Computes the absolute differences of the packed unsigned byte integers from xmm3/m128 and xmm2; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results."
"VPSHUFB xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shuffle bytes in xmm2 according to contents of xmm3/m128."
"VPSHUFD xmm1, xmm2/m128, ib","VEX.128.66.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Shuffle the doublewords in xmm2/m128 based on the encoding in ib and store the result in xmm1."
"VPSHUFHW xmm1, xmm2/m128, ib","VEX.128.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Shuffle the high words in xmm2/m128 based on the encoding in ib and store the result in xmm1."
"VPSHUFLW xmm1, xmm2/m128, ib","VEX.128.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Shuffle the low words in xmm2/m128 based on the encoding in ib and store the result in xmm1."
"VPSIGNB xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 08 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Negate/zero/preserve packed byte integers in xmm2 depending on the corresponding sign in xmm3/m128."
"VPSIGND xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 0A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Negate/zero/preserve packed doubleword integers in xmm2 depending on the corresponding sign in xmm3/m128."
"VPSIGNW xmm1, xmm2, xmm3/m128","VEX.128.66.0F38.WIG 09 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Negate/zero/preserve packed word integers in xmm2 depending on the corresponding sign in xmm3/m128."
"VPSLLD xmm1, xmm2, ib","VEX.128.66.0F.WIG 72 /6 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift doublewords in xmm2 left by ib while shifting in 0s."
"VPSLLD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG F2 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLDQ xmm1, xmm2, ib","VEX.128.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift xmm2 left by ib bytes while shifting in 0s and store result in xmm1."
"VPSLLQ xmm1, xmm2, ib","VEX.128.66.0F.WIG 73 /6 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift quadwords in xmm2 left by ib while shifting in 0s."
"VPSLLQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG F3 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLW xmm1, xmm2, ib","VEX.128.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift words in xmm2 left by ib while shifting in 0s."
"VPSLLW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSRAD xmm1, xmm2, ib","VEX.128.66.0F.WIG 72 /4 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift doublewords in xmm2 right by ib while shifting in sign bits."
"VPSRAD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG E2 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits."
"VPSRAW xmm1, xmm2, ib","VEX.128.66.0F.WIG 71 /4 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift words in xmm2 right by ib while shifting in sign bits."
"VPSRAW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG E1 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits."
"VPSRLD xmm1, xmm2, ib","VEX.128.66.0F.WIG 72 /2 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift doublewords in xmm2 right by ib while shifting in 0s."
"VPSRLD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG D2 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLDQ xmm1, xmm2, ib","VEX.128.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift xmm2 right by ib bytes while shifting in 0s."
"VPSRLQ xmm1, xmm2, ib","VEX.128.66.0F.WIG 73 /2 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift quadwords in xmm2 right by ib while shifting in 0s."
"VPSRLQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG D3 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLW xmm1, xmm2, ib","VEX.128.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","ib","","","Shift words in xmm2 right by ib while shifting in 0s."
"VPSRLW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSUBB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed byte integers in xmm3/m128 from xmm2."
"VPSUBD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG FA /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed doubleword integers in xmm3/m128 from xmm2."
"VPSUBQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG FB /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed quadword integers in xmm3/m128 from xmm2."
"VPSUBSB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results."
"VPSUBSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG E9 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results."
"VPSUBUSB xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2 and saturate result."
"VPSUBUSW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG D9 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate result."
"VPSUBW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed word integers in xmm3/m128 from xmm2."
"VPTEST xmm1, xmm2/m128","VEX.128.66.0F38.WIG 17 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Set ZF and CF depending on bitwise AND and ANDN of sources."
"VPTEST ymm1, ymm2/m256","VEX.256.66.0F38.WIG 17 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Set ZF and CF depending on bitwise AND and ANDN of sources."
"VPUNPCKHBW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 68 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKHDQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 6A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave high-order doublewords from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKHQDQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 6D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave high-order quadword from xmm2 and xmm3/m128 into xmm1 register."
"VPUNPCKHWD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 69 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave high-order words from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKLBW xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 60 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKLDQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 62 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave low-order doublewords from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKLQDQ xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 6C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave low-order quadword from xmm2 and xmm3/m128 into xmm1 register."
"VPUNPCKLWD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 61 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Interleave low-order words from xmm2 and xmm3/m128 into xmm1."
"VPXOR xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG EF /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Bitwise XOR of xmm3/m128 and xmm2."
"VRCPPS xmm1, xmm2/m128","VEX.128.0F.WIG 53 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Computes the approximate reciprocals of packed single-precision values in xmm2/mem and stores the results in xmm1."
"VRCPPS ymm1, ymm2/m256","VEX.256.0F.WIG 53 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Computes the approximate reciprocals of packed single-precision values in ymm2/mem and stores the results in ymm1."
"VRCPSS xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 53 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm3/m32 and stores the result in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VROUNDPD xmm1, xmm2/m128, ib","VEX.128.66.0F3A.WIG 09 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Round packed double-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by ib."
"VROUNDPD ymm1, ymm2/m256, ib","VEX.256.66.0F3A.WIG 09 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Round packed double-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by ib."
"VROUNDPS xmm1, xmm2/m128, ib","VEX.128.66.0F3A.WIG 08 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Round packed single-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by ib."
"VROUNDPS ymm1, ymm2/m256, ib","VEX.256.66.0F3A.WIG 08 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","ib","","","Round packed single-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by ib."
"VROUNDSD xmm1, xmm2, xmm3/m64, ib","VEX.LIG.66.0F3A.WIG 0B /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Round the low packed double precision floating-point value in xmm3/m64 and place the result in xmm1. The rounding mode is determined by ib. Upper packed double precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VROUNDSS xmm1, xmm2, xmm3/m32, ib","VEX.LIG.66.0F3A.WIG 0A /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Round the low packed single precision floating-point value in xmm3/m32 and place the result in xmm1. The rounding mode is determined by ib. Also, upper packed single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VRSQRTPS xmm1, xmm2/m128","VEX.128.0F.WIG 52 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Computes the approximate reciprocals of the square roots of packed single-precision values in xmm2/mem and stores the results in xmm1."
"VRSQRTPS ymm1, ymm2/m256","VEX.256.0F.WIG 52 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Computes the approximate reciprocals of the square roots of packed single-precision values in ymm2/mem and stores the results in ymm1."
"VRSQRTSS xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 52 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Computes the approximate reciprocal of the square root of the low single precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VSHUFPD xmm1, xmm2, xmm3/m128, ib","VEX.128.66.0F.WIG C6 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Shuffle two pairs of double-precision floating-point values from xmm2 and xmm3/m128 using ib to select from each pair, interleaved result is stored in xmm1."
"VSHUFPD ymm1, ymm2, ymm3/m256, ib","VEX.256.66.0F.WIG C6 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Shuffle four pairs of double-precision floating-point values from ymm2 and ymm3/m256 using ib to select from each pair, interleaved result is stored in xmm1."
"VSHUFPS xmm1, xmm2, xmm3/m128, ib","VEX.128.0F.WIG C6 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Select from quadruplet of single-precision floating-point values in xmm1 and xmm2/m128 using ib, interleaved result pairs are stored in xmm1."
"VSHUFPS ymm1, ymm2, ymm3/m256, ib","VEX.256.0F.WIG C6 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","ib","","Select from quadruplet of single-precision floating-point values in ymm2 and ymm3/m256 using ib, interleaved result pairs are stored in ymm1."
"VSQRTPD xmm1, xmm2/m128","VEX.128.66.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Computes Square Roots of the packed double-precision floating-point values in xmm2/m128 and stores the result in xmm1."
"VSQRTPD ymm1, ymm2/m256","VEX.256.66.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Computes Square Roots of the packed double-precision floating-point values in ymm2/m256 and stores the result in ymm1."
"VSQRTPS xmm1, xmm2/m128","VEX.128.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Computes Square Roots of the packed single-precision floating-point values in xmm2/m128 and stores the result in xmm1."
"VSQRTPS ymm1, ymm2/m256","VEX.256.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","","","","Computes Square Roots of the packed single-precision floating-point values in ymm2/m256 and stores the result in ymm1."
"VSQRTSD xmm1, xmm2, xmm3/m64","VEX.LIG.F2.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Computes square root of the low double-precision floating-point value in xmm3/m64 and stores the results in xmm1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VSQRTSS xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Computes square root of the low single-precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single-precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VSTMXCSR m32","VEX.LZ.0F.WIG AE /3","Valid","Valid","Invalid","AVX","ModRM:r/m (w, ModRM:[7:6] must not be 11b)","","","","","Store contents of MXCSR register to m32."
"VSUBPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed double-precision floating-point values in xmm3/mem from xmm2 and store result in xmm1."
"VSUBPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed double-precision floating-point values in ymm3/mem from ymm2 and store result in ymm1."
"VSUBPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed single-precision floating-point values in xmm3/mem from xmm2 and stores result in xmm1."
"VSUBPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract packed single-precision floating-point values in ymm3/mem from ymm2 and stores result in ymm1."
"VSUBSD xmm1, xmm2, xmm3/m64","VEX.LIG.F2.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract the low double-precision floating-point value in xmm3/m64 from xmm2 and store the result in xmm1."
"VSUBSS xmm1, xmm2, xmm3/m32","VEX.LIG.F3.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Subtract the low single-precision floating-point value in xmm3/m32 from xmm2 and store the result in xmm1."
"VTESTPD xmm1, xmm2/m128","VEX.128.66.0F38.W0 0F /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating-point sources."
"VTESTPD ymm1, ymm2/m256","VEX.256.66.0F38.W0 0F /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating-point sources."
"VTESTPS xmm1, xmm2/m128","VEX.128.66.0F38.W0 0E /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating-point sources."
"VTESTPS ymm1, ymm2/m256","VEX.256.66.0F38.W0 0E /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating-point sources."
"VUCOMISD xmm1, xmm2/m64","VEX.LIG.66.0F.WIG 2E /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VUCOMISS xmm1, xmm2/m32","VEX.LIG.0F.WIG 2E /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","","","","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VUNPCKHPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 15 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Unpacks and Interleaves double-precision floating-point values from high quadwords of xmm2 and xmm3/m128."
"VUNPCKHPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 15 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Unpacks and Interleaves double-precision floating-point values from high quadwords of ymm2 and ymm3/m256."
"VUNPCKHPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 15 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm2 and xmm3/m128."
"VUNPCKHPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 15 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Unpacks and Interleaves single-precision floating-point values from high quadwords of ymm2 and ymm3/m256."
"VUNPCKLPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 14 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Unpacks and Interleaves double-precision floating-point values from low quadwords of xmm2 and xmm3/m128."
"VUNPCKLPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 14 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Unpacks and Interleaves double-precision floating-point values from low quadwords of ymm2 and ymm3/m256."
"VUNPCKLPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 14 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm2 and xmm3/m128."
"VUNPCKLPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 14 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Unpacks and Interleaves single-precision floating-point values from low quadwords of ymm2 and ymm3/m256."
"VXORPD xmm1, xmm2, xmm3/m128","VEX.128.66.0F.WIG 57 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical XOR of packed double-precision floating-point values in xmm2 and xmm3/mem."
"VXORPD ymm1, ymm2, ymm3/m256","VEX.256.66.0F.WIG 57 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical XOR of packed double-precision floating-point values in ymm2 and ymm3/mem."
"VXORPS xmm1, xmm2, xmm3/m128","VEX.128.0F.WIG 57 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical XOR of packed single-precision floating-point values in xmm2 and xmm3/mem."
"VXORPS ymm1, ymm2, ymm3/m256","VEX.256.0F.WIG 57 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","","Return the bitwise logical XOR of packed single-precision floating-point values in ymm2 and ymm3/mem."
"VZEROALL","VEX.256.0F.WIG 77","Valid","Valid","Invalid","AVX","","","","","","Zero all YMM registers."
"VZEROUPPER","VEX.128.0F.WIG 77","Valid","Valid","Invalid","AVX","","","","","","Zero upper 128 bits of all YMM registers."
