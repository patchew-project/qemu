"Instruction","Opcode","Valid 64-bit","Valid 32-bit","Valid 16-bit","Feature Flags","Operand 1","Operand 2","Operand 3","Operand 4","Tuple Type","Description"
"KADDD k1, k2, k3","VEX.L1.66.0F.W1 4A /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Add 32 bits masks in k2 and k3 and place result in k1."
"KADDQ k1, k2, k3","VEX.L1.0F.W1 4A /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Add 64 bits masks in k2 and k3 and place result in k1."
"KANDD k1, k2, k3","VEX.L1.66.0F.W1 41 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND 32 bits masks k2 and k3 and place result in k1."
"KANDND k1, k2, k3","VEX.L1.66.0F.W1 42 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND NOT 32 bits masks k2 and k3 and place result in k1."
"KANDNQ k1, k2, k3","VEX.L1.0F.W1 42 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND NOT 64 bits masks k2 and k3 and place result in k1."
"KANDQ k1, k2, k3","VEX.L1.0F.W1 41 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND 64 bits masks k2 and k3 and place result in k1."
"KMOVD k1, k2/m32","VEX.L0.66.0F.W1 90 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Move 32 bits mask from k2/m32 and store the result in k1."
"KMOVD m32, k1","VEX.L0.66.0F.W1 91 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w, ModRM:[7:6] must not be 11b)","ModRM:reg (r)","","","","Move 32 bits mask from k1 and store the result in m32."
"KMOVD k1, rw","VEX.L0.F2.0F.W0 92 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 32 bits mask from r to k1."
"KMOVD rw, k1","VEX.L0.F2.0F.W0 93 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 32 bits mask from k1 to r."
"KMOVQ k1, k2/m64","VEX.L0.0F.W1 90 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Move 64 bits mask from k2/m64 and store the result in k1."
"KMOVQ m64, k1","VEX.L0.0F.W1 91 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w, ModRM:[7:6] must not be 11b)","ModRM:reg (r)","","","","Move 64 bits mask from k1 and store the result in m64."
"KMOVQ k1, rw","VEX.L0.F2.0F.W1 92 /r","Valid","Invalid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 64 bits mask from r to k1."
"KMOVQ rw, k1","VEX.L0.F2.0F.W1 93 /r","Valid","Invalid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 64 bits mask from k1 to r."
"KNOTD k1, k2","VEX.L0.66.0F.W1 44 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise NOT of 32 bits mask k2."
"KNOTQ k1, k2","VEX.L0.0F.W1 44 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise NOT of 64 bits mask k2."
"KORD k1, k2, k3","VEX.L1.66.0F.W1 45 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise OR 32 bits masks k2 and k3 and place result in k1."
"KORQ k1, k2, k3","VEX.L1.0F.W1 45 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise OR 64 bits masks k2 and k3 and place result in k1."
"KORTESTD k1, k2","VEX.L0.66.0F.W1 98 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise OR 32 bits masks k1 and k2 and update ZF and CF accordingly."
"KORTESTQ k1, k2","VEX.L0.0F.W1 98 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise OR 64 bits masks k1 and k2 and update ZF and CF accordingly."
"KSHIFTLD k1, k2, ib","VEX.L0.66.0F3A.W0 33 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","ib","","","Shift left 32 bits in k2 by immediate and write result in k1."
"KSHIFTLQ k1, k2, ib","VEX.L0.66.0F3A.W1 33 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","ib","","","Shift left 64 bits in k2 by immediate and write result in k1."
"KSHIFTRD k1, k2, ib","VEX.L0.66.0F3A.W0 31 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","ib","","","Shift right 32 bits in k2 by immediate and write result in k1."
"KSHIFTRQ k1, k2, ib","VEX.L0.66.0F3A.W1 31 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","ib","","","Shift right 64 bits in k2 by immediate and write result in k1."
"KTESTD k1, k2","VEX.L0.66.0F.W1 99 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Set ZF and CF depending on sign bit AND and ANDN of 32 bits mask register sources."
"KTESTQ k1, k2","VEX.L0.0F.W1 99 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Set ZF and CF depending on sign bit AND and ANDN of 64 bits mask register sources."
"KUNPCKDQ k1, k2, k3","VEX.L1.0F.W1 4B /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Unpack and interleave 32 bits masks in k2 and k3 and write quadword result in k1."
"KUNPCKWD k1, k2, k3","VEX.L1.0F.W0 4B /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Unpack and interleave 16 bits in k2 and k3 and write double-word result in k1."
"KXNORD k1, k2, k3","VEX.L1.66.0F.W1 46 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XNOR 32 bits masks k2 and k3 and place result in k1."
"KXNORQ k1, k2, k3","VEX.L1.0F.W1 46 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XNOR 64 bits masks k2 and k3 and place result in k1."
"KXORD k1, k2, k3","VEX.L1.66.0F.W1 47 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XOR 32 bits masks k2 and k3 and place result in k1."
"KXORQ k1, k2, k3","VEX.L1.0F.W1 47 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XOR 64 bits masks k2 and k3 and place result in k1."
"VDBPSADBW xmm1 {k1}{z}, xmm2, xmm3/m128, ib","EVEX.128.66.0F3A.W0 42 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","","Compute packed SAD word results of unsigned bytes in dword block from xmm2 with unsigned bytes of dword blocks transformed from xmm3/m128 using the shuffle controls in ib. Results are written to xmm1 under the writemask k1."
"VDBPSADBW ymm1 {k1}{z}, ymm2, ymm3/m256, ib","EVEX.256.66.0F3A.W0 42 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","","Compute packed SAD word results of unsigned bytes in dword block from ymm2 with unsigned bytes of dword blocks transformed from ymm3/m256 using the shuffle controls in ib. Results are written to ymm1 under the writemask k1."
"VDBPSADBW zmm1 {k1}{z}, zmm2, zmm3/m512, ib","EVEX.512.66.0F3A.W0 42 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","","Compute packed SAD word results of unsigned bytes in dword block from zmm2 with unsigned bytes of dword blocks transformed from zmm3/m512 using the shuffle controls in ib. Results are written to zmm1 under the writemask k1."
"VMOVDQU16 xmm1 {k1}{z}, xmm2/m128","EVEX.128.F2.0F.W1 6F /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed word integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQU16 ymm1 {k1}{z}, ymm2/m256","EVEX.256.F2.0F.W1 6F /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed word integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQU16 zmm1 {k1}{z}, zmm2/m512","EVEX.512.F2.0F.W1 6F /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed word integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQU16 xmm2/m128 {k1}{z}, xmm1","EVEX.128.F2.0F.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed word integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQU16 ymm2/m256 {k1}{z}, ymm1","EVEX.256.F2.0F.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed word integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQU16 zmm2/m512 {k1}{z}, zmm1","EVEX.512.F2.0F.W1 7F /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed word integer values from zmm1 to zmm2/m512 using writemask k1."
"VMOVDQU8 xmm1 {k1}{z}, xmm2/m128","EVEX.128.F2.0F.W0 6F /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed byte integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQU8 ymm1 {k1}{z}, ymm2/m256","EVEX.256.F2.0F.W0 6F /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed byte integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQU8 zmm1 {k1}{z}, zmm2/m512","EVEX.512.F2.0F.W0 6F /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Move unaligned packed byte integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQU8 xmm2/m128 {k1}{z}, xmm1","EVEX.128.F2.0F.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed byte integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQU8 ymm2/m256 {k1}{z}, ymm1","EVEX.256.F2.0F.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed byte integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQU8 zmm2/m512 {k1}{z}, zmm1","EVEX.512.F2.0F.W0 7F /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Full Vector Mem","Move unaligned packed byte integer values from zmm1 to zmm2/m512 using writemask k1."
"VPABSB xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSB ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1 using writemask k1."
"VPABSB zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of bytes in zmm2/m512 and store UNSIGNED result in zmm1 using writemask k1."
"VPABSW xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F38.WIG 1D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSW ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F38.WIG 1D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of 16-bit integers in xmm2/m256 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSW zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F38.WIG 1D /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Full Vector Mem","Compute the absolute value of 16-bit integers in xmm2/m512 and store UNSIGNED result in xmm1 using writemask k1."
"VPACKSSDW xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F.W0 6B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Converts packed signed doubleword integers from xmm2 and from xmm3/m128/m32bcst into packed signed word integers in xmm1 using signed saturation under writemask k1."
"VPACKSSWB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Converts packed signed word integers from xmm2 and from xmm3/m128 into packed signed byte integers in xmm1 using signed saturation under writemask k1."
"VPACKSSWB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Converts packed signed word integers from ymm2 and from ymm3/m256 into packed signed byte integers in ymm1 using signed saturation under writemask k1."
"VPACKSSWB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Converts packed signed word integers from zmm2 and from zmm3/m512 into packed signed byte integers in zmm1 using signed saturation under writemask k1."
"VPACKUSDW xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.128.66.0F38.W0 2B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Convert packed signed doubleword integers from xmm2 and packed signed doubleword integers from xmm3/m128/m32bcst into packed unsigned word integers in xmm1 using unsigned saturation under writemask k1."
"VPACKUSDW ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.256.66.0F38.W0 2B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Convert packed signed doubleword integers from ymm2 and packed signed doubleword integers from ymm3/m256/m32bcst into packed unsigned word integers in ymm1 using unsigned saturation under writemask k1."
"VPACKUSDW zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.512.66.0F38.W0 2B /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector","Convert packed signed doubleword integers from zmm2 and packed signed doubleword integers from zmm3/m512/m32bcst into packed unsigned word integers in zmm1 using unsigned saturation under writemask k1."
"VPACKUSWB xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Converts signed word integers from xmm2 and signed word integers from xmm3/m128 into unsigned byte integers in xmm1 using unsigned saturation under writemask k1."
"VPACKUSWB ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Converts signed word integers from ymm2 and signed word integers from ymm3/m256 into unsigned byte integers in ymm1 using unsigned saturation under writemask k1."
"VPACKUSWB zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Converts signed word integers from zmm2 and signed word integers from zmm3/m512 into unsigned byte integers in zmm1 using unsigned saturation under writemask k1."
"VPADDB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG FC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed byte integers from xmm2, and xmm3/m128 and store in xmm1 using writemask k1."
"VPADDB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG FC /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed byte integers from zmm2, and zmm3/m512 and store in xmm1 using writemask k1."
"VPADDB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG FC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed byte integers from ymm2, and ymm3/m256 and store in ymm1 using writemask k1."
"VPADDSB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed signed byte integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDSB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDSB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed signed byte integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPADDSW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed signed word integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDSW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDSW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed signed word integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPADDUSB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG DC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed unsigned byte integers from xmm2,and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDUSB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG DC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed unsigned byte integers from ymm2,and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDUSB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG DC /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed unsigned byte integers from zmm2,and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPADDUSW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG DD /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed unsigned word integers from xmm2,and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDUSW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG DD /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed unsigned word integers from ymm2,and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDUSW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG DD /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed unsigned word integers from zmm2,and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPADDW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG FD /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed word integers from xmm2, and xmm3/m128 and store in xmm1 using writemask k1."
"VPADDW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG FD /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed word integers from ymm2, and ymm3/m256 and store in ymm1 using writemask k1."
"VPADDW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG FD /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Add packed word integers from zmm2, and zmm3/m512 and store in xmm1 using writemask k1."
"VPALIGNR xmm1 {k1}{z}, xmm2, xmm3/m128, ib","EVEX.128.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","","Concatenate xmm2 and xmm3/m128 into a 32-byte intermediate result, extract byte aligned result shifted to the right by constant value in ib and result is stored in xmm1."
"VPALIGNR ymm1 {k1}{z}, ymm2, ymm3/m256, ib","EVEX.256.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","","Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in ib from each intermediate result, and two 16-byte results are stored in ymm1."
"VPALIGNR zmm1 {k1}{z}, zmm2, zmm3/m512, ib","EVEX.512.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","","Concatenate pairs of 16 bytes in zmm2 and zmm3/m512 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in ib from each intermediate result, and four 16-byte results are stored in zmm1."
"VPAVGB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Average packed unsigned byte integers from xmm2, and xmm3/m128 with rounding and store to xmm1 under writemask k1."
"VPAVGB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1 under writemask k1."
"VPAVGB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Average packed unsigned byte integers from zmm2, and zmm3/m512 with rounding and store to zmm1 under writemask k1."
"VPAVGW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Average packed unsigned word integers from xmm2, xmm3/m128 with rounding to xmm1 under writemask k1."
"VPAVGW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1 under writemask k1."
"VPAVGW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Average packed unsigned word integers from zmm2, zmm3/m512 with rounding to zmm1 under writemask k1."
"VPBLENDMB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.W0 66 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Blend byte integer vector xmm2 and byte vector xmm3/m128 and store the result in xmm1, under control mask."
"VPBLENDMB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.W0 66 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Blend byte integer vector ymm2 and byte vector ymm3/m256 and store the result in ymm1, under control mask."
"VPBLENDMB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.W0 66 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Blend byte integer vector zmm2 and byte vector zmm3/m512 and store the result in zmm1, under control mask."
"VPBLENDMW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.W1 66 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Blend word integer vector xmm2 and word vector xmm3/m128 and store the result in xmm1, under control mask."
"VPBLENDMW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.W1 66 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Blend word integer vector ymm2 and word vector ymm3/m256 and store the result in ymm1, under control mask."
"VPBLENDMW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.W1 66 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Blend word integer vector zmm2 and word vector zmm3/m512 and store the result in zmm1, under control mask."
"VPBROADCASTB xmm1{k1}{z}, xmm2/m8","EVEX.128.66.0F38.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a byte integer in the source operand to locations in xmm1 subject to writemask k1."
"VPBROADCASTB ymm1{k1}{z}, xmm2/m8","EVEX.256.66.0F38.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a byte integer in the source operand to locations in ymm1 subject to writemask k1."
"VPBROADCASTB zmm1{k1}{z}, xmm2/m8","EVEX.512.66.0F38.W0 78 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a byte integer in the source operand to 64 locations in zmm1 subject to writemask k1."
"VPBROADCASTB xmm1 {k1}{z}, r32","EVEX.128.66.0F38.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast an 8-bit value from a GPR to all bytes in the 128-bit destination subject to writemask k1."
"VPBROADCASTB ymm1 {k1}{z}, r32","EVEX.256.66.0F38.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast an 8-bit value from a GPR to all bytes in the 256-bit destination subject to writemask k1."
"VPBROADCASTB zmm1 {k1}{z}, r32","EVEX.512.66.0F38.W0 7A /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast an 8-bit value from a GPR to all bytes in the 512-bit destination subject to writemask k1."
"VPBROADCASTW xmm1{k1}{z}, xmm2/m16","EVEX.128.66.0F38.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a word integer in the source operand to locations in xmm1 subject to writemask k1."
"VPBROADCASTW ymm1{k1}{z}, xmm2/m16","EVEX.256.66.0F38.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a word integer in the source operand to locations in ymm1 subject to writemask k1."
"VPBROADCASTW zmm1{k1}{z}, xmm2/m16","EVEX.512.66.0F38.W0 79 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a word integer in the source operand to 32 locations in zmm1 subject to writemask k1."
"VPBROADCASTW xmm1 {k1}{z}, r32","EVEX.128.66.0F38.W0 7B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a 16-bit value from a GPR to all words in the 128-bit destination subject to writemask k1."
"VPBROADCASTW ymm1 {k1}{z}, r32","EVEX.256.66.0F38.W0 7B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a 16-bit value from a GPR to all words in the 256-bit destination subject to writemask k1."
"VPBROADCASTW zmm1 {k1}{z}, r32","EVEX.512.66.0F38.W0 7B /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Tuple1 Scalar","Broadcast a 16-bit value from a GPR to all words in the 512-bit destination subject to writemask k1."
"VPCMPB k1 {k2}, xmm2, xmm3/m128, ib","EVEX.128.66.0F3A.W0 3F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed signed byte values in xmm3/m128 and xmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPB k1 {k2}, ymm2, ymm3/m256, ib","EVEX.256.66.0F3A.W0 3F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed signed byte values in ymm3/m256 and ymm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPB k1 {k2}, zmm2, zmm3/m512, ib","EVEX.512.66.0F3A.W0 3F /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed signed byte values in zmm3/m512 and zmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPEQB k1 {k2}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 74 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed bytes in xmm3/m128 and xmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQB k1 {k2}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG 74 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed bytes in ymm3/m256 and ymm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQB k1 {k2}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG 74 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed bytes in zmm3/m512 and zmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQW k1 {k2}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 75 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed words in xmm3/m128 and xmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQW k1 {k2}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG 75 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed words in ymm3/m256 and ymm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQW k1 {k2}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG 75 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed words in zmm3/m512 and zmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTB k1 {k2}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 64 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTB k1 {k2}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG 64 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTB k1 {k2}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG 64 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed byte integers in zmm2 and zmm3/m512 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTW k1 {k2}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 65 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed word integers in xmm2 and xmm3/m128 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTW k1 {k2}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG 65 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed word integers in ymm2 and ymm3/m256 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTW k1 {k2}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG 65 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed word integers in zmm2 and zmm3/m512 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPUB k1 {k2}, xmm2, xmm3/m128, ib","EVEX.128.66.0F3A.W0 3E /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed unsigned byte values in xmm3/m128 and xmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUB k1 {k2}, ymm2, ymm3/m256, ib","EVEX.256.66.0F3A.W0 3E /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed unsigned byte values in ymm3/m256 and ymm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUB k1 {k2}, zmm2, zmm3/m512, ib","EVEX.512.66.0F3A.W0 3E /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed unsigned byte values in zmm3/m512 and zmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUW k1 {k2}, xmm2, xmm3/m128, ib","EVEX.128.66.0F3A.W1 3E /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed unsigned word integers in xmm3/m128 and xmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUW k1 {k2}, ymm2, ymm3/m256, ib","EVEX.256.66.0F3A.W1 3E /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed unsigned word integers in ymm3/m256 and ymm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUW k1 {k2}, zmm2, zmm3/m512, ib","EVEX.512.66.0F3A.W1 3E /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed unsigned word integers in zmm3/m512 and zmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPW k1 {k2}, xmm2, xmm3/m128, ib","EVEX.128.66.0F3A.W1 3F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed signed word integers in xmm3/m128 and xmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPW k1 {k2}, ymm2, ymm3/m256, ib","EVEX.256.66.0F3A.W1 3F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed signed word integers in ymm3/m256 and ymm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPW k1 {k2}, zmm2, zmm3/m512, ib","EVEX.512.66.0F3A.W1 3F /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Full Vector Mem","Compare packed signed word integers in zmm3/m512 and zmm2 using bits 2:0 of ib as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPERMI2W xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.W1 75 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Permute word integers from two tables in xmm3/m128 and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1."
"VPERMI2W ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.W1 75 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Permute word integers from two tables in ymm3/m256 and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1."
"VPERMI2W zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.W1 75 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Permute word integers from two tables in zmm3/m512 and zmm2 using indexes in zmm1 and store the result in zmm1 using writemask k1."
"VPERMT2W xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.W1 7D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Mem","Permute word integers from two tables in xmm3/m128 and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2W ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.W1 7D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Mem","Permute word integers from two tables in ymm3/m256 and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2W zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.W1 7D /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Mem","Permute word integers from two tables in zmm3/m512 and zmm1 using indexes in zmm2 and store the result in zmm1 using writemask k1."
"VPERMW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.W1 8D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Permute word integers in xmm3/m128 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.W1 8D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Permute word integers in ymm3/m256 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.W1 8D /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Permute word integers in zmm3/m512 using indexes in zmm2 and store the result in zmm1 using writemask k1."
"VPEXTRB r32/m8, xmm2, ib","EVEX.128.66.0F3A.WIG 14 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","ib","","Tuple1 Scalar","Extract a byte integer value from xmm2 at the source byte offset specified by ib into reg or m8. The upper bits of r are zeroed."
"VPINSRB xmm1, xmm2, r32/m8, ib","EVEX.128.66.0F3A.WIG 20 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Merge a byte integer value from r/m8 and rest from xmm2 into xmm1 at the byte offset in ib."
"VPINSRW xmm1, xmm2, r32/m16, ib","EVEX.128.66.0F.WIG C4 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","ib","Tuple1 Scalar","Insert a word integer value from r/m and rest from xmm2 into xmm1 at the word offset in ib."
"VPMADDUBSW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1 under writemask k1."
"VPMADDUBSW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1 under writemask k1."
"VPMADDUBSW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to zmm1 under writemask k1."
"VPMADDWD xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1 under writemask k1."
"VPMADDWD ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1 under writemask k1."
"VPMADDWD zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed word integers in zmm2 by the packed word integers in zmm3/m512, add adjacent doubleword results, and store in zmm1 under writemask k1."
"VPMAXSB xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXSB ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXSB zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMAXSW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXSW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXSW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed word integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMAXUB xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG DE /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXUB ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG DE /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXUB zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG DE /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMAXUW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.WIG 3E /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXUW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.WIG 3E /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXUW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.WIG 3E /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned word integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMINSB xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.WIG 38 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSB ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.WIG 38 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSB zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.WIG 38 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1."
"VPMINSW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG EA /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed word integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG EA /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed word integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG EA /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed signed word integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1."
"VPMINUB xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG DA /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINUB ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG DA /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINUB zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG DA /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1."
"VPMINUW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.WIG 3A /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1 under writemask k1."
"VPMINUW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.WIG 3A /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1 under writemask k1."
"VPMINUW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.WIG 3A /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Compare packed unsigned word integers in zmm3/m512 and zmm2 and return packed minimum values in zmm1 under writemask k1."
"VPMOVB2M k1, xmm1","EVEX.128.F3.0F38.W0 29 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in XMM1."
"VPMOVB2M k1, ymm1","EVEX.256.F3.0F38.W0 29 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in YMM1."
"VPMOVB2M k1, zmm1","EVEX.512.F3.0F38.W0 29 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in ZMM1."
"VPMOVM2B xmm1, k1","EVEX.128.F3.0F38.W0 28 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each byte in XMM1 to all 1's or all 0's based on the value of the corresponding bit in k1."
"VPMOVM2B ymm1, k1","EVEX.256.F3.0F38.W0 28 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each byte in YMM1 to all 1's or all 0's based on the value of the corresponding bit in k1."
"VPMOVM2B zmm1, k1","EVEX.512.F3.0F38.W0 28 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each byte in ZMM1 to all 1's or all 0's based on the value of the corresponding bit in k1."
"VPMOVM2W xmm1, k1","EVEX.128.F3.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each word in XMM1 to all 1's or all 0's based on the value of the corresponding bit in k1."
"VPMOVM2W ymm1, k1","EVEX.256.F3.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each word in YMM1 to all 1's or all 0's based on the value of the corresponding bit in k1."
"VPMOVM2W zmm1, k1","EVEX.512.F3.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each word in ZMM1 to all 1's or all 0's based on the value of the corresponding bit in k1."
"VPMOVSWB xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 20 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 16 packed signed word integers from ymm2 into 16 packed signed bytes in xmm1/m128 using signed saturation under writemask k1."
"VPMOVSWB xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 20 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 8 packed signed word integers from xmm2 into 8 packed signed bytes in xmm1/m64 using signed saturation under writemask k1."
"VPMOVSWB ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 20 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 32 packed signed word integers from zmm2 into 32 packed signed bytes in ymm1/m256 using signed saturation under writemask k1."
"VPMOVSXBW xmm1{k1}{z}, xmm2/m64","EVEX.128.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Sign extend 8 packed 8-bit integers in xmm2/m64 to 8 packed 16-bit integers in zmm1."
"VPMOVSXBW ymm1{k1}{z}, xmm2/m128","EVEX.256.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVSXBW zmm1{k1}{z}, ymm2/m256","EVEX.512.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Sign extend 32 packed 8-bit integers in ymm2/m256 to 32 packed 16-bit integers in zmm1."
"VPMOVUSWB xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 10 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 16 packed unsigned word integers from ymm2 into 16 packed unsigned bytes in xmm1/m128 using unsigned saturation under writemask k1."
"VPMOVUSWB xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 10 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 8 packed unsigned word integers from xmm2 into 8 packed unsigned bytes in 8mm1/m64 using unsigned saturation under writemask k1."
"VPMOVUSWB ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 10 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 32 packed unsigned word integers from zmm2 into 32 packed unsigned bytes in ymm1/m256 using unsigned saturation under writemask k1."
"VPMOVW2M k1, xmm1","EVEX.128.F3.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in XMM1."
"VPMOVW2M k1, ymm1","EVEX.256.F3.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in YMM1."
"VPMOVW2M k1, zmm1","EVEX.512.F3.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in ZMM1."
"VPMOVWB xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 30 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 16 packed word integers from ymm2 into 16 packed bytes in xmm1/m128 with truncation under writemask k1."
"VPMOVWB xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 30 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 8 packed word integers from xmm2 into 8 packed bytes in xmm1/m64 with truncation under writemask k1."
"VPMOVWB ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 30 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","","","Half Vector Mem","Converts 32 packed word integers from zmm2 into 32 packed bytes in ymm1/m256 with truncation under writemask k1."
"VPMOVZXBW xmm1{k1}{z}, xmm2/m64","EVEX.128.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"VPMOVZXBW ymm1{k1}{z}, xmm2/m128","EVEX.256.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVZXBW zmm1{k1}{z}, ymm2/m256","EVEX.512.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","Half Vector Mem","Zero extend 32 packed 8-bit integers in ymm2/m256 to 32 packed 16-bit integers in zmm1."
"VPMULHRSW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1 under writemask k1."
"VPMULHRSW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1 under writemask k1."
"VPMULHRSW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to zmm1 under writemask k1."
"VPMULHUW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1."
"VPMULHUW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1."
"VPMULHUW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed unsigned word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1."
"VPMULHW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1."
"VPMULHW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1."
"VPMULHW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed signed word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1."
"VPMULLW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the low 16 bits of the results in xmm1 under writemask k1."
"VPMULLW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1 under writemask k1."
"VPMULLW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Multiply the packed signed word integers in zmm2 and zmm3/m512, and store the low 16 bits of the results in zmm1 under writemask k1."
"VPSADBW xmm1, xmm2, xmm3/m128","EVEX.128.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Computes the absolute differences of the packed unsigned byte integers from xmm3/m128 and xmm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results."
"VPSADBW ymm1, ymm2, ymm3/m256","EVEX.256.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Computes the absolute differences of the packed unsigned byte integers from ymm3/m256 and ymm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results."
"VPSADBW zmm1, zmm2, zmm3/m512","EVEX.512.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Computes the absolute differences of the packed unsigned byte integers from zmm3/m512 and zmm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results."
"VPSHUFB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shuffle bytes in xmm2 according to contents of xmm3/m128 under write mask k1."
"VPSHUFB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shuffle bytes in ymm2 according to contents of ymm3/m256 under write mask k1."
"VPSHUFB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shuffle bytes in zmm2 according to contents of zmm3/m512 under write mask k1."
"VPSHUFHW xmm1 {k1}{z}, xmm2/m128, ib","EVEX.128.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shuffle the high words in xmm2/m128 based on the encoding in ib and store the result in xmm1 under write mask k1."
"VPSHUFHW ymm1 {k1}{z}, ymm2/m256, ib","EVEX.256.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shuffle the high words in ymm2/m256 based on the encoding in ib and store the result in ymm1 under write mask k1."
"VPSHUFHW zmm1 {k1}{z}, zmm2/m512, ib","EVEX.512.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shuffle the high words in zmm2/m512 based on the encoding in ib and store the result in zmm1 under write mask k1."
"VPSHUFLW xmm1 {k1}{z}, xmm2/m128, ib","EVEX.128.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shuffle the low words in xmm2/m128 based on the encoding in ib and store the result in xmm1 under write mask k1."
"VPSHUFLW ymm1 {k1}{z}, ymm2/m256, ib","EVEX.256.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shuffle the low words in ymm2/m256 based on the encoding in ib and store the result in ymm1 under write mask k1."
"VPSHUFLW zmm1 {k1}{z}, zmm2/m512, ib","EVEX.512.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shuffle the low words in zmm2/m512 based on the encoding in ib and store the result in zmm1 under write mask k1."
"VPSLLDQ xmm1, xmm2/m128, ib","EVEX.128.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift xmm2/m128 left by ib bytes while shifting in 0s and store result in xmm1."
"VPSLLDQ ymm1, ymm2/m256, ib","EVEX.256.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift ymm2/m256 left by ib bytes while shifting in 0s and store result in ymm1."
"VPSLLDQ zmm1, zmm2/m512, ib","EVEX.512.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift zmm2/m512 left by ib bytes while shifting in 0s and store result in zmm1."
"VPSLLVW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.W1 12 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shift words in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLVW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.W1 12 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shift words in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s using writemask k1."
"VPSLLVW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.W1 12 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shift words in zmm2 left by amount specified in the corresponding element of zmm3/m512 while shifting in 0s using writemask k1."
"VPSLLW xmm1 {k1}{z}, xmm2/m128, ib","EVEX.128.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift words in xmm2/m128 left by ib while shifting in 0s using writemask k1."
"VPSLLW ymm1 {k1}{z}, ymm2/m256, ib","EVEX.256.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift words in ymm2/m256 left by ib while shifting in 0s using writemask k1."
"VPSLLW zmm1 {k1}{z}, zmm2/m512, ib","EVEX.512.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift words in zmm2/m512 left by ib while shifting in 0 using writemask k1."
"VPSLLW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLW ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.256.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift words in ymm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLW zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.512.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift words in zmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRAVW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.W1 11 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shift words in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAVW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.W1 11 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shift words in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in sign bits using writemask k1."
"VPSRAVW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.W1 11 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shift words in zmm2 right by amount specified in the corresponding element of zmm3/m512 while shifting in sign bits using writemask k1."
"VPSRAW xmm1 {k1}{z}, xmm2/m128, ib","EVEX.128.66.0F.WIG 71 /4 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift words in xmm2/m128 right by ib while shifting in sign bits using writemask k1."
"VPSRAW ymm1 {k1}{z}, ymm2/m256, ib","EVEX.256.66.0F.WIG 71 /4 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift words in ymm2/m256 right by ib while shifting in sign bits using writemask k1."
"VPSRAW zmm1 {k1}{z}, zmm2/m512, ib","EVEX.512.66.0F.WIG 71 /4 ib","Valid","Valid","Invalid","AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift words in zmm2/m512 right by ib while shifting in sign bits using writemask k1."
"VPSRAW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG E1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAW ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.256.66.0F.WIG E1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAW zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.512.66.0F.WIG E1 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift words in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRLDQ xmm1, xmm2/m128, ib","EVEX.128.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift xmm2/m128 right by ib bytes while shifting in 0s and store result in xmm1."
"VPSRLDQ ymm1, ymm2/m256, ib","EVEX.256.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift ymm2/m256 right by ib bytes while shifting in 0s and store result in ymm1."
"VPSRLDQ zmm1, zmm2/m512, ib","EVEX.512.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift zmm2/m512 right by ib bytes while shifting in 0s and store result in zmm1."
"VPSRLVW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F38.W1 10 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shift words in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLVW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F38.W1 10 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shift words in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s using writemask k1."
"VPSRLVW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F38.W1 10 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Shift words in zmm2 right by amount specified in the corresponding element of zmm3/m512 while shifting in 0s using writemask k1."
"VPSRLW xmm1 {k1}{z}, xmm2/m128, ib","EVEX.128.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift words in xmm2/m128 right by ib while shifting in 0s using writemask k1."
"VPSRLW ymm1 {k1}{z}, ymm2/m256, ib","EVEX.256.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift words in ymm2/m256 right by ib while shifting in 0s using writemask k1."
"VPSRLW zmm1 {k1}{z}, zmm2/m512, ib","EVEX.512.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX512BW","EVEX.vvvv (w)","ModRM:r/m (r)","ib","","Full Vector Mem","Shift words in zmm2/m512 right by ib while shifting in 0s using writemask k1."
"VPSRLW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLW ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.256.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLW zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.512.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","","Mem128","Shift words in zmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSUBB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed byte integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1."
"VPSUBB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed byte integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1."
"VPSUBB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed byte integers in zmm3/m512 from zmm2 and store in zmm1 using writemask k1."
"VPSUBSB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results and store in xmm1 using writemask k1."
"VPSUBSB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results and store in ymm1 using writemask k1."
"VPSUBSB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed signed byte integers in zmm3/m512 from packed signed byte integers in zmm2 and saturate results and store in zmm1 using writemask k1."
"VPSUBSW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG E9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results and store in xmm1 using writemask k1."
"VPSUBSW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG E9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results and store in ymm1 using writemask k1."
"VPSUBUSB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2, saturate results and store in xmm1 using writemask k1."
"VPSUBUSB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2, saturate results and store in ymm1 using writemask k1."
"VPSUBUSB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed unsigned byte integers in zmm3/m512 from packed unsigned byte integers in zmm2, saturate results and store in zmm1 using writemask k1."
"VPSUBUSW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG D9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate results and store in xmm1 using writemask k1."
"VPSUBUSW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG D9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2, saturate results and store in ymm1 using writemask k1."
"VPSUBW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed word integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1."
"VPSUBW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.256.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed word integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1."
"VPSUBW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.512.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Subtract packed word integers in zmm3/m512 from zmm2 and store in zmm1 using writemask k1."
"VPTESTMB k2 {k1}, xmm2, xmm3/m128","EVEX.128.66.0F38.W0 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise AND of packed byte integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMB k2 {k1}, ymm2, ymm3/m256","EVEX.256.66.0F38.W0 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise AND of packed byte integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMB k2 {k1}, zmm2, zmm3/m512","EVEX.512.66.0F38.W0 26 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise AND of packed byte integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMW k2 {k1}, xmm2, xmm3/m128","EVEX.128.66.0F38.W1 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise AND of packed word integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMW k2 {k1}, ymm2, ymm3/m256","EVEX.256.66.0F38.W1 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise AND of packed word integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMW k2 {k1}, zmm2, zmm3/m512","EVEX.512.66.0F38.W1 26 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise AND of packed word integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTNMB k2 {k1}, xmm2, xmm3/m128","EVEX.128.F3.0F38.W0 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise NAND of packed byte integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMB k2 {k1}, ymm2, ymm3/m256","EVEX.256.F3.0F38.W0 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise NAND of packed byte integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMB k2 {k1}, zmm2, zmm3/m512","EVEX.512.F3.0F38.W0 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise NAND of packed byte integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMW k2 {k1}, xmm2, xmm3/m128","EVEX.128.F3.0F38.W1 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise NAND of packed word integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMW k2 {k1}, ymm2, ymm3/m256","EVEX.256.F3.0F38.W1 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise NAND of packed word integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMW k2 {k1}, zmm2, zmm3/m512","EVEX.512.F3.0F38.W1 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Bitwise NAND of packed word integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPUNPCKHBW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 68 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1 register using k1 write mask."
"VPUNPCKHWD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 69 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Interleave high-order words from xmm2 and xmm3/m128 into xmm1 register using k1 write mask."
"VPUNPCKLBW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 60 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1."
"VPUNPCKLWD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.128.66.0F.WIG 61 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","","Full Vector Mem","Interleave low-order words from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1."
